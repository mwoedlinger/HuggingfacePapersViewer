{
    "2312.17653": {
        "url": "https://arxiv.org/abs/2312.17653",
        "title": "LARP: Language-Agent Role Play for Open-World Games",
        "authors": [
            "Ming Yan",
            "Ruihao Li",
            "Hao Zhang",
            "Hao Wang",
            "Zhilan Yang",
            "Ji Yan"
        ],
        "abstract": "Language agents have shown impressive problem-solving skills within defined\nsettings and brief timelines. Yet, with the ever-evolving complexities of\nopen-world simulations, there's a pressing need for agents that can flexibly\nadapt to complex environments and consistently maintain a long-term memory to\nensure coherent actions. To bridge the gap between language agents and\nopen-world games, we introduce Language Agent for Role-Playing (LARP), which\nincludes a cognitive architecture that encompasses memory processing and a\ndecision-making assistant, an environment interaction module with a\nfeedback-driven learnable action space, and a postprocessing method that\npromotes the alignment of various personalities. The LARP framework refines\ninteractions between users and agents, predefined with unique backgrounds and\npersonalities, ultimately enhancing the gaming experience in open-world\ncontexts. Furthermore, it highlights the diverse uses of language models in a\nrange of areas such as entertainment, education, and various simulation\nscenarios. The project page is released at https://miao-ai-lab.github.io/LARP/.",
        "publication_date": "2023-12-24T10:08:59Z",
        "upvotes": 29
    },
    "2312.17681": {
        "url": "https://arxiv.org/abs/2312.17681",
        "title": "FlowVid: Taming Imperfect Optical Flows for Consistent Video-to-Video\n  Synthesis",
        "authors": [
            "Feng Liang",
            "Bichen Wu",
            "Jialiang Wang",
            "Licheng Yu",
            "Kunpeng Li",
            "Yinan Zhao",
            "Ishan Misra",
            "Jia-Bin Huang",
            "Peizhao Zhang",
            "Peter Vajda",
            "Diana Marculescu"
        ],
        "abstract": "Diffusion models have transformed the image-to-image (I2I) synthesis and are\nnow permeating into videos. However, the advancement of video-to-video (V2V)\nsynthesis has been hampered by the challenge of maintaining temporal\nconsistency across video frames. This paper proposes a consistent V2V synthesis\nframework by jointly leveraging spatial conditions and temporal optical flow\nclues within the source video. Contrary to prior methods that strictly adhere\nto optical flow, our approach harnesses its benefits while handling the\nimperfection in flow estimation. We encode the optical flow via warping from\nthe first frame and serve it as a supplementary reference in the diffusion\nmodel. This enables our model for video synthesis by editing the first frame\nwith any prevalent I2I models and then propagating edits to successive frames.\nOur V2V model, FlowVid, demonstrates remarkable properties: (1) Flexibility:\nFlowVid works seamlessly with existing I2I models, facilitating various\nmodifications, including stylization, object swaps, and local edits. (2)\nEfficiency: Generation of a 4-second video with 30 FPS and 512x512 resolution\ntakes only 1.5 minutes, which is 3.1x, 7.2x, and 10.5x faster than CoDeF,\nRerender, and TokenFlow, respectively. (3) High-quality: In user studies, our\nFlowVid is preferred 45.7% of the time, outperforming CoDeF (3.5%), Rerender\n(10.2%), and TokenFlow (40.4%).",
        "publication_date": "2023-12-29T16:57:12Z",
        "upvotes": 15
    },
    "2312.17276": {
        "url": "https://arxiv.org/abs/2312.17276",
        "title": "PanGu-$\u03c0$: Enhancing Language Model Architectures via Nonlinearity\n  Compensation",
        "authors": [
            "Yunhe Wang",
            "Hanting Chen",
            "Yehui Tang",
            "Tianyu Guo",
            "Kai Han",
            "Ying Nie",
            "Xutao Wang",
            "Hailin Hu",
            "Zheyuan Bai",
            "Yun Wang",
            "Fangcheng Liu",
            "Zhicheng Liu",
            "Jianyuan Guo",
            "Sinan Zeng",
            "Yinchen Zhang",
            "Qinghua Xu",
            "Qun Liu",
            "Jun Yao",
            "Chao Xu",
            "Dacheng Tao"
        ],
        "abstract": "The recent trend of large language models (LLMs) is to increase the scale of\nboth model size (\\aka the number of parameters) and dataset to achieve better\ngenerative ability, which is definitely proved by a lot of work such as the\nfamous GPT and Llama. However, large models often involve massive computational\ncosts, and practical applications cannot afford such high prices. However, the\nmethod of constructing a strong model architecture for LLMs is rarely\ndiscussed. We first analyze the state-of-the-art language model architectures\nand observe the feature collapse problem. Based on the theoretical analysis, we\npropose that the nonlinearity is also very important for language models, which\nis usually studied in convolutional neural networks for vision tasks. The\nseries informed activation function is then introduced with tiny calculations\nthat can be ignored, and an augmented shortcut is further used to enhance the\nmodel nonlinearity. We then demonstrate that the proposed approach is\nsignificantly effective for enhancing the model nonlinearity through carefully\ndesigned ablations; thus, we present a new efficient model architecture for\nestablishing modern, namely, PanGu-$\\pi$. Experiments are then conducted using\nthe same dataset and training strategy to compare PanGu-$\\pi$ with\nstate-of-the-art LLMs. The results show that PanGu-$\\pi$-7B can achieve a\ncomparable performance to that of benchmarks with about 10\\% inference\nspeed-up, and PanGu-$\\pi$-1B can achieve state-of-the-art performance in terms\nof accuracy and efficiency. In addition, we have deployed PanGu-$\\pi$-7B in the\nhigh-value domains of finance and law, developing an LLM named YunShan for\npractical application. The results show that YunShan can surpass other models\nwith similar scales on benchmarks.",
        "publication_date": "2023-12-27T11:49:24Z",
        "upvotes": 14
    },
    "2312.17742": {
        "url": "https://arxiv.org/abs/2312.17742",
        "title": "Learning Vision from Models Rivals Learning Vision from Data",
        "authors": [
            "Yonglong Tian",
            "Lijie Fan",
            "Kaifeng Chen",
            "Dina Katabi",
            "Dilip Krishnan",
            "Phillip Isola"
        ],
        "abstract": "We introduce SynCLR, a novel approach for learning visual representations\nexclusively from synthetic images and synthetic captions, without any real\ndata. We synthesize a large dataset of image captions using LLMs, then use an\noff-the-shelf text-to-image model to generate multiple images corresponding to\neach synthetic caption. We perform visual representation learning on these\nsynthetic images via contrastive learning, treating images sharing the same\ncaption as positive pairs. The resulting representations transfer well to many\ndownstream tasks, competing favorably with other general-purpose visual\nrepresentation learners such as CLIP and DINO v2 in image classification tasks.\nFurthermore, in dense prediction tasks such as semantic segmentation, SynCLR\noutperforms previous self-supervised methods by a significant margin, e.g.,\nimproving over MAE and iBOT by 6.2 and 4.3 mIoU on ADE20k for ViT-B/16.",
        "publication_date": "2023-12-28T18:59:55Z",
        "upvotes": 12
    },
    "2312.17661": {
        "url": "https://arxiv.org/abs/2312.17661",
        "title": "Gemini in Reasoning: Unveiling Commonsense in Multimodal Large Language\n  Models",
        "authors": [
            "Yuqing Wang",
            "Yun Zhao"
        ],
        "abstract": "The burgeoning interest in Multimodal Large Language Models (MLLMs), such as\nOpenAI's GPT-4V(ision), has significantly impacted both academic and industrial\nrealms. These models enhance Large Language Models (LLMs) with advanced visual\nunderstanding capabilities, facilitating their application in a variety of\nmultimodal tasks. Recently, Google introduced Gemini, a cutting-edge MLLM\ndesigned specifically for multimodal integration. Despite its advancements,\npreliminary benchmarks indicate that Gemini lags behind GPT models in\ncommonsense reasoning tasks. However, this assessment, based on a limited\ndataset (i.e., HellaSWAG), does not fully capture Gemini's authentic\ncommonsense reasoning potential. To address this gap, our study undertakes a\nthorough evaluation of Gemini's performance in complex reasoning tasks that\nnecessitate the integration of commonsense knowledge across modalities. We\ncarry out a comprehensive analysis of 12 commonsense reasoning datasets,\nranging from general to domain-specific tasks. This includes 11 datasets\nfocused solely on language, as well as one that incorporates multimodal\nelements. Our experiments across four LLMs and two MLLMs demonstrate Gemini's\ncompetitive commonsense reasoning capabilities. Additionally, we identify\ncommon challenges faced by current LLMs and MLLMs in addressing commonsense\nproblems, underscoring the need for further advancements in enhancing the\ncommonsense reasoning abilities of these models.",
        "publication_date": "2023-12-29T15:57:49Z",
        "upvotes": 10
    },
    "2401.00368": {
        "url": "https://arxiv.org/abs/2401.00368",
        "title": "Improving Text Embeddings with Large Language Models",
        "authors": [
            "Liang Wang",
            "Nan Yang",
            "Xiaolong Huang",
            "Linjun Yang",
            "Rangan Majumder",
            "Furu Wei"
        ],
        "abstract": "In this paper, we introduce a novel and simple method for obtaining\nhigh-quality text embeddings using only synthetic data and less than 1k\ntraining steps. Unlike existing methods that often depend on multi-stage\nintermediate pre-training with billions of weakly-supervised text pairs,\nfollowed by fine-tuning with a few labeled datasets, our method does not\nrequire building complex training pipelines or relying on manually collected\ndatasets that are often constrained by task diversity and language coverage. We\nleverage proprietary LLMs to generate diverse synthetic data for hundreds of\nthousands of text embedding tasks across nearly 100 languages. We then\nfine-tune open-source decoder-only LLMs on the synthetic data using standard\ncontrastive loss. Experiments demonstrate that our method achieves strong\nperformance on highly competitive text embedding benchmarks without using any\nlabeled data. Furthermore, when fine-tuned with a mixture of synthetic and\nlabeled data, our model sets new state-of-the-art results on the BEIR and MTEB\nbenchmarks.",
        "publication_date": "2023-12-31T02:13:18Z",
        "upvotes": 72
    },
    "2401.00448": {
        "url": "https://arxiv.org/abs/2401.00448",
        "title": "Beyond Chinchilla-Optimal: Accounting for Inference in Language Model\n  Scaling Laws",
        "authors": [
            "Nikhil Sardana",
            "Jonathan Frankle"
        ],
        "abstract": "Large language model (LLM) scaling laws are empirical formulas that estimate\nchanges in model quality as a result of increasing parameter count and training\ndata. However, these formulas, including the popular DeepMind Chinchilla\nscaling laws, neglect to include the cost of inference. We modify the\nChinchilla scaling laws to calculate the optimal LLM parameter count and\npre-training data size to train and deploy a model of a given quality and\ninference demand. We conduct our analysis both in terms of a compute budget and\nreal-world costs and find that LLM researchers expecting reasonably large\ninference demand (~1B requests) should train models smaller and longer than\nChinchilla-optimal.",
        "publication_date": "2023-12-31T10:53:58Z",
        "upvotes": 25
    },
    "2401.00788": {
        "url": "https://arxiv.org/abs/2401.00788",
        "title": "Astraios: Parameter-Efficient Instruction Tuning Code Large Language\n  Models",
        "authors": [
            "Terry Yue Zhuo",
            "Armel Zebaze",
            "Nitchakarn Suppattarachai",
            "Leandro von Werra",
            "Harm de Vries",
            "Qian Liu",
            "Niklas Muennighoff"
        ],
        "abstract": "The high cost of full-parameter fine-tuning (FFT) of Large Language Models\n(LLMs) has led to a series of parameter-efficient fine-tuning (PEFT) methods.\nHowever, it remains unclear which methods provide the best cost-performance\ntrade-off at different model scales. We introduce Astraios, a suite of 28\ninstruction-tuned OctoCoder models using 7 tuning methods and 4 model sizes up\nto 16 billion parameters. Through investigations across 5 tasks and 8 different\ndatasets encompassing both code comprehension and code generation tasks, we\nfind that FFT generally leads to the best downstream performance across all\nscales, and PEFT methods differ significantly in their efficacy based on the\nmodel scale. LoRA usually offers the most favorable trade-off between cost and\nperformance. Further investigation into the effects of these methods on both\nmodel robustness and code security reveals that larger models tend to\ndemonstrate reduced robustness and less security. At last, we explore the\nrelationships among updated parameters, cross-entropy loss, and task\nperformance. We find that the tuning effectiveness observed in small models\ngeneralizes well to larger models, and the validation loss in instruction\ntuning can be a reliable indicator of overall downstream performance.",
        "publication_date": "2024-01-01T15:30:19Z",
        "upvotes": 21
    },
    "2401.00849": {
        "url": "https://arxiv.org/abs/2401.00849",
        "title": "COSMO: COntrastive Streamlined MultimOdal Model with Interleaved\n  Pre-Training",
        "authors": [
            "Alex Jinpeng Wang",
            "Linjie Li",
            "Kevin Qinghong Lin",
            "Jianfeng Wang",
            "Kevin Lin",
            "Zhengyuan Yang",
            "Lijuan Wang",
            "Mike Zheng Shou"
        ],
        "abstract": "In the evolution of Vision-Language Pre-training, shifting from short-text\ncomprehension to encompassing extended textual contexts is pivotal. Recent\nautoregressive vision-language models like \\cite{flamingo, palme}, leveraging\nthe long-context capability of Large Language Models, have excelled in few-shot\ntext generation tasks but face challenges in alignment tasks. Addressing this\ngap, we introduce the contrastive loss into text generation models, presenting\nthe COntrastive-Streamlined MultimOdal framework (\\ModelName), strategically\npartitioning the language model into dedicated unimodal text processing and\nadept multimodal data handling components. \\ModelName, our unified framework,\nmerges unimodal and multimodal elements, enhancing model performance for tasks\ninvolving textual and visual data while notably reducing learnable parameters.\nHowever, these models demand extensive long-text datasets, yet the availability\nof high-quality long-text video datasets remains limited. To bridge this gap,\nthis work introduces \\VideoDatasetName, an inaugural interleaved video-text\ndataset featuring comprehensive captions, marking a significant step forward.\nDemonstrating its impact, we illustrate how \\VideoDatasetName{} enhances model\nperformance in image-text tasks. With 34% learnable parameters and utilizing\n72\\% of the available data, our model demonstrates significant superiority over\nOpenFlamingo~\\cite{openflamingo}. For instance, in the 4-shot flickr captioning\ntask, performance notably improves from 57.2% to 65.\\%. The contributions of\n\\ModelName{} and \\VideoDatasetName{} are underscored by notable performance\ngains across 14 diverse downstream datasets encompassing both image-text and\nvideo-text tasks.",
        "publication_date": "2024-01-01T18:58:42Z",
        "upvotes": 14
    },
    "2401.00434": {
        "url": "https://arxiv.org/abs/2401.00434",
        "title": "GeoGalactica: A Scientific Large Language Model in Geoscience",
        "authors": [
            "Zhouhan Lin",
            "Cheng Deng",
            "Le Zhou",
            "Tianhang Zhang",
            "Yi Xu",
            "Yutong Xu",
            "Zhongmou He",
            "Yuanyuan Shi",
            "Beiya Dai",
            "Yunchong Song",
            "Boyi Zeng",
            "Qiyuan Chen",
            "Tao Shi",
            "Tianyu Huang",
            "Yiwei Xu",
            "Shu Wang",
            "Luoyi Fu",
            "Weinan Zhang",
            "Junxian He",
            "Chao Ma",
            "Yunqiang Zhu",
            "Xinbing Wang",
            "Chenghu Zhou"
        ],
        "abstract": "Large language models (LLMs) have achieved huge success for their general\nknowledge and ability to solve a wide spectrum of tasks in natural language\nprocessing (NLP). Due to their impressive abilities, LLMs have shed light on\npotential inter-discipline applications to foster scientific discoveries of a\nspecific domain by using artificial intelligence (AI for science, AI4S). In the\nmeantime, utilizing NLP techniques in geoscience research and practice is wide\nand convoluted, contributing from knowledge extraction and document\nclassification to question answering and knowledge discovery. In this work, we\ntake the initial step to leverage LLM for science, through a rather\nstraightforward approach. We try to specialize an LLM into geoscience, by\nfurther pre-training the model with a vast amount of texts in geoscience, as\nwell as supervised fine-tuning (SFT) the resulting model with our custom\ncollected instruction tuning dataset. These efforts result in a model\nGeoGalactica consisting of 30 billion parameters. To our best knowledge, it is\nthe largest language model for the geoscience domain. More specifically,\nGeoGalactica is from further pre-training of Galactica. We train GeoGalactica\nover a geoscience-related text corpus containing 65 billion tokens curated from\nextensive data sources in the big science project Deep-time Digital Earth\n(DDE), preserving as the largest geoscience-specific text corpus. Then we\nfine-tune the model with 1 million pairs of instruction-tuning data consisting\nof questions that demand professional geoscience knowledge to answer. In this\ntechnical report, we will illustrate in detail all aspects of GeoGalactica,\nincluding data collection, data cleaning, base model selection, pre-training,\nSFT, and evaluation. We open-source our data curation tools and the checkpoints\nof GeoGalactica during the first 3/4 of pre-training.",
        "publication_date": "2023-12-31T09:22:54Z",
        "upvotes": 8
    },
    "2401.00134": {
        "url": "https://arxiv.org/abs/2401.00134",
        "title": "Unicron: Economizing Self-Healing LLM Training at Scale",
        "authors": [
            "Tao He",
            "Xue Li",
            "Zhibin Wang",
            "Kun Qian",
            "Jingbo Xu",
            "Wenyuan Yu",
            "Jingren Zhou"
        ],
        "abstract": "Training large-scale language models is increasingly critical in various\ndomains, but it is hindered by frequent failures, leading to significant time\nand economic costs. Current failure recovery methods in cloud-based settings\ninadequately address the diverse and complex scenarios that arise, focusing\nnarrowly on erasing downtime for individual tasks without considering the\noverall cost impact on a cluster. We introduce Unicron, a workload manager\ndesigned for efficient self-healing in large-scale language model training.\nUnicron optimizes the training process by minimizing failure-related costs\nacross multiple concurrent tasks within a cluster. Its key features include\nin-band error detection for real-time error identification without extra\noverhead, a dynamic cost-aware plan generation mechanism for optimal\nreconfiguration, and an efficient transition strategy to reduce downtime during\nstate changes. Deployed on a 128-GPU distributed cluster, Unicron demonstrates\nup to a 1.9x improvement in training efficiency over state-of-the-art methods,\nsignificantly reducing failure recovery costs and enhancing the reliability of\nlarge-scale language model training.",
        "publication_date": "2023-12-30T04:06:16Z",
        "upvotes": 8
    },
    "2401.00246": {
        "url": "https://arxiv.org/abs/2401.00246",
        "title": "Boosting Large Language Model for Speech Synthesis: An Empirical Study",
        "authors": [
            "Hongkun Hao",
            "Long Zhou",
            "Shujie Liu",
            "Jinyu Li",
            "Shujie Hu",
            "Rui Wang",
            "Furu Wei"
        ],
        "abstract": "Large language models (LLMs) have made significant advancements in natural\nlanguage processing and are concurrently extending the language ability to\nother modalities, such as speech and vision. Nevertheless, most of the previous\nwork focuses on prompting LLMs with perception abilities like auditory\ncomprehension, and the effective approach for augmenting LLMs with speech\nsynthesis capabilities remains ambiguous. In this paper, we conduct a\ncomprehensive empirical exploration of boosting LLMs with the ability to\ngenerate speech, by combining pre-trained LLM LLaMA/OPT and text-to-speech\nsynthesis model VALL-E. We compare three integration methods between LLMs and\nspeech synthesis models, including directly fine-tuned LLMs, superposed layers\nof LLMs and VALL-E, and coupled LLMs and VALL-E using LLMs as a powerful text\nencoder. Experimental results show that, using LoRA method to fine-tune LLMs\ndirectly to boost the speech synthesis capability does not work well, and\nsuperposed LLMs and VALL-E can improve the quality of generated speech both in\nspeaker similarity and word error rate (WER). Among these three methods,\ncoupled methods leveraging LLMs as the text encoder can achieve the best\nperformance, making it outperform original speech synthesis models with a\nconsistently better speaker similarity and a significant (10.9%) WER reduction.",
        "publication_date": "2023-12-30T14:20:04Z",
        "upvotes": 7
    },
    "2401.00604": {
        "url": "https://arxiv.org/abs/2401.00604",
        "title": "SteinDreamer: Variance Reduction for Text-to-3D Score Distillation via\n  Stein Identity",
        "authors": [
            "Peihao Wang",
            "Zhiwen Fan",
            "Dejia Xu",
            "Dilin Wang",
            "Sreyas Mohan",
            "Forrest Iandola",
            "Rakesh Ranjan",
            "Yilei Li",
            "Qiang Liu",
            "Zhangyang Wang",
            "Vikas Chandra"
        ],
        "abstract": "Score distillation has emerged as one of the most prevalent approaches for\ntext-to-3D asset synthesis. Essentially, score distillation updates 3D\nparameters by lifting and back-propagating scores averaged over different\nviews. In this paper, we reveal that the gradient estimation in score\ndistillation is inherent to high variance. Through the lens of variance\nreduction, the effectiveness of SDS and VSD can be interpreted as applications\nof various control variates to the Monte Carlo estimator of the distilled\nscore. Motivated by this rethinking and based on Stein's identity, we propose a\nmore general solution to reduce variance for score distillation, termed Stein\nScore Distillation (SSD). SSD incorporates control variates constructed by\nStein identity, allowing for arbitrary baseline functions. This enables us to\ninclude flexible guidance priors and network architectures to explicitly\noptimize for variance reduction. In our experiments, the overall pipeline,\ndubbed SteinDreamer, is implemented by instantiating the control variate with a\nmonocular depth estimator. The results suggest that SSD can effectively reduce\nthe distillation variance and consistently improve visual quality for both\nobject- and scene-level generation. Moreover, we demonstrate that SteinDreamer\nachieves faster convergence than existing methods due to more stable gradient\nupdates.",
        "publication_date": "2023-12-31T23:04:25Z",
        "upvotes": 4
    },
    "2401.00908": {
        "url": "https://arxiv.org/abs/2401.00908",
        "title": "DocLLM: A layout-aware generative language model for multimodal document\n  understanding",
        "authors": [
            "Dongsheng Wang",
            "Natraj Raman",
            "Mathieu Sibue",
            "Zhiqiang Ma",
            "Petr Babkin",
            "Simerjot Kaur",
            "Yulong Pei",
            "Armineh Nourbakhsh",
            "Xiaomo Liu"
        ],
        "abstract": "Enterprise documents such as forms, invoices, receipts, reports, contracts,\nand other similar records, often carry rich semantics at the intersection of\ntextual and spatial modalities. The visual cues offered by their complex\nlayouts play a crucial role in comprehending these documents effectively. In\nthis paper, we present DocLLM, a lightweight extension to traditional large\nlanguage models (LLMs) for reasoning over visual documents, taking into account\nboth textual semantics and spatial layout. Our model differs from existing\nmultimodal LLMs by avoiding expensive image encoders and focuses exclusively on\nbounding box information to incorporate the spatial layout structure.\nSpecifically, the cross-alignment between text and spatial modalities is\ncaptured by decomposing the attention mechanism in classical transformers to a\nset of disentangled matrices. Furthermore, we devise a pre-training objective\nthat learns to infill text segments. This approach allows us to address\nirregular layouts and heterogeneous content frequently encountered in visual\ndocuments. The pre-trained model is fine-tuned using a large-scale instruction\ndataset, covering four core document intelligence tasks. We demonstrate that\nour solution outperforms SotA LLMs on 14 out of 16 datasets across all tasks,\nand generalizes well to 4 out of 5 previously unseen datasets.",
        "publication_date": "2023-12-31T22:37:52Z",
        "upvotes": 169
    },
    "2401.01335": {
        "url": "https://arxiv.org/abs/2401.01335",
        "title": "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language\n  Models",
        "authors": [
            "Zixiang Chen",
            "Yihe Deng",
            "Huizhuo Yuan",
            "Kaixuan Ji",
            "Quanquan Gu"
        ],
        "abstract": "Harnessing the power of human-annotated data through Supervised Fine-Tuning\n(SFT) is pivotal for advancing Large Language Models (LLMs). In this paper, we\ndelve into the prospect of growing a strong LLM out of a weak one without the\nneed for acquiring additional human-annotated data. We propose a new\nfine-tuning method called Self-Play fIne-tuNing (SPIN), which starts from a\nsupervised fine-tuned model. At the heart of SPIN lies a self-play mechanism,\nwhere the LLM refines its capability by playing against instances of itself.\nMore specifically, the LLM generates its own training data from its previous\niterations, refining its policy by discerning these self-generated responses\nfrom those obtained from human-annotated data. Our method progressively\nelevates the LLM from a nascent model to a formidable one, unlocking the full\npotential of human-annotated demonstration data for SFT. Theoretically, we\nprove that the global optimum to the training objective function of our method\nis achieved only when the LLM policy aligns with the target data distribution.\nEmpirically, we evaluate our method on several benchmark datasets including the\nHuggingFace Open LLM Leaderboard, MT-Bench, and datasets from Big-Bench. Our\nresults show that SPIN can significantly improve the LLM's performance across a\nvariety of benchmarks and even outperform models trained through direct\npreference optimization (DPO) supplemented with extra GPT-4 preference data.\nThis sheds light on the promise of self-play, enabling the achievement of\nhuman-level performance in LLMs without the need for expert opponents. Codes\nare available at https://github.com/uclaml/SPIN.",
        "publication_date": "2024-01-02T18:53:13Z",
        "upvotes": 61
    },
    "2401.01055": {
        "url": "https://arxiv.org/abs/2401.01055",
        "title": "LLaMA Beyond English: An Empirical Study on Language Capability Transfer",
        "authors": [
            "Jun Zhao",
            "Zhihao Zhang",
            "Luhui Gao",
            "Qi Zhang",
            "Tao Gui",
            "Xuanjing Huang"
        ],
        "abstract": "In recent times, substantial advancements have been witnessed in large\nlanguage models (LLMs), exemplified by ChatGPT, showcasing remarkable\nproficiency across a range of complex tasks. However, many mainstream LLMs\n(e.g. LLaMA) are pretrained on English-dominant corpus, which limits their\nperformance in other non-English languages. In this paper, we focus on how to\neffectively transfer the capabilities of language generation and following\ninstructions to a non-English language. To answer this question, we conduct an\nextensive empirical investigation based on LLaMA, accumulating over 1440 GPU\nhours. We analyze the impact of key factors such as vocabulary extension,\nfurther pretraining, and instruction tuning on transfer. To accurately assess\nthe model's level of knowledge, we employ four widely used standardized testing\nbenchmarks: C-Eval, MMLU, AGI-Eval, and GAOKAO-Bench. Furthermore, a\ncomprehensive evaluation of the model's response quality is conducted,\nconsidering aspects such as accuracy, fluency, informativeness, logical\ncoherence, and harmlessness, based on LLM-Eval, a benchmarks consisting\ninstruction tasks from 17 diverse categories. Our evaluation results\ndemonstrate that comparable performance to state-of-the-art transfer models can\nbe achieved with less than 1% of the pretraining data, both in terms of\nknowledge alignment and response quality. Furthermore, the experimental\noutcomes across the thirteen low-resource languages also exhibit similar\ntrends. We anticipate that the conclusions revealed by the experiments will aid\nthe community in developing non-English LLMs.",
        "publication_date": "2024-01-02T06:29:02Z",
        "upvotes": 49
    },
    "2401.01325": {
        "url": "https://arxiv.org/abs/2401.01325",
        "title": "LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning",
        "authors": [
            "Hongye Jin",
            "Xiaotian Han",
            "Jingfeng Yang",
            "Zhimeng Jiang",
            "Zirui Liu",
            "Chia-Yuan Chang",
            "Huiyuan Chen",
            "Xia Hu"
        ],
        "abstract": "It is well known that LLMs cannot generalize well to long contexts whose\nlengths are larger than the training sequence length. This poses challenges\nwhen employing LLMs for processing long input sequences during inference. In\nthis work, we argue that LLMs themselves have inherent capabilities to handle\nlong contexts without fine-tuning. To achieve this goal, we propose SelfExtend\nto extend the context window of LLMs by constructing bi-level attention\ninformation: the grouped attention and the neighbor attention. The grouped\nattention captures the dependencies among tokens that are far apart, while\nneighbor attention captures dependencies among adjacent tokens within a\nspecified range. The two-level attentions are computed based on the original\nmodel's self-attention mechanism during inference. With minor code\nmodification, our SelfExtend can effortlessly extend existing LLMs' context\nwindow without any fine-tuning. We conduct comprehensive experiments on\nmultiple benchmarks and the results show that our SelfExtend can effectively\nextend existing LLMs' context window length. The code can be found at\n\\url{https://github.com/datamllab/LongLM}.",
        "publication_date": "2024-01-02T18:30:51Z",
        "upvotes": 24
    },
    "2401.00935": {
        "url": "https://arxiv.org/abs/2401.00935",
        "title": "Boundary Attention: Learning to Localize Boundaries under High Noise",
        "authors": [
            "Mia Gaia Polansky",
            "Charles Herrmann",
            "Junhwa Hur",
            "Deqing Sun",
            "Dor Verbin",
            "Todd Zickler"
        ],
        "abstract": "We present a differentiable model that infers explicit boundaries, including\ncurves, corners and junctions, using a mechanism that we call boundary\nattention. Boundary attention is a boundary-aware local attention operation\nthat, when applied densely and repeatedly, progressively refines a field of\nvariables that specify an unrasterized description of the local boundary\nstructure in every overlapping patch within an image. It operates in a\nbottom-up fashion, similar to classical methods for sub-pixel edge localization\nand edge-linking, but with a higher-dimensional description of local boundary\nstructure, a notion of spatial consistency that is learned instead of designed,\nand a sequence of operations that is end-to-end differentiable. We train our\nmodel using simple synthetic data and then evaluate it using photographs that\nwere captured under low-light conditions with variable amounts of noise. We\nfind that our method generalizes to natural images corrupted by real sensor\nnoise, and predicts consistent boundaries under increasingly noisy conditions\nwhere other state-of-the-art methods fail.",
        "publication_date": "2024-01-01T19:00:55Z",
        "upvotes": 16
    },
    "2401.01256": {
        "url": "https://arxiv.org/abs/2401.01256",
        "title": "VideoDrafter: Content-Consistent Multi-Scene Video Generation with LLM",
        "authors": [
            "Fuchen Long",
            "Zhaofan Qiu",
            "Ting Yao",
            "Tao Mei"
        ],
        "abstract": "The recent innovations and breakthroughs in diffusion models have\nsignificantly expanded the possibilities of generating high-quality videos for\nthe given prompts. Most existing works tackle the single-scene scenario with\nonly one video event occurring in a single background. Extending to generate\nmulti-scene videos nevertheless is not trivial and necessitates to nicely\nmanage the logic in between while preserving the consistent visual appearance\nof key content across video scenes. In this paper, we propose a novel\nframework, namely VideoDrafter, for content-consistent multi-scene video\ngeneration. Technically, VideoDrafter leverages Large Language Models (LLM) to\nconvert the input prompt into comprehensive multi-scene script that benefits\nfrom the logical knowledge learnt by LLM. The script for each scene includes a\nprompt describing the event, the foreground/background entities, as well as\ncamera movement. VideoDrafter identifies the common entities throughout the\nscript and asks LLM to detail each entity. The resultant entity description is\nthen fed into a text-to-image model to generate a reference image for each\nentity. Finally, VideoDrafter outputs a multi-scene video by generating each\nscene video via a diffusion process that takes the reference images, the\ndescriptive prompt of the event and camera movement into account. The diffusion\nmodel incorporates the reference images as the condition and alignment to\nstrengthen the content consistency of multi-scene videos. Extensive experiments\ndemonstrate that VideoDrafter outperforms the SOTA video generation models in\nterms of visual quality, content consistency, and user preference.",
        "publication_date": "2024-01-02T15:56:48Z",
        "upvotes": 16
    },
    "2401.01286": {
        "url": "https://arxiv.org/abs/2401.01286",
        "title": "A Comprehensive Study of Knowledge Editing for Large Language Models",
        "authors": [
            "Ningyu Zhang",
            "Yunzhi Yao",
            "Bozhong Tian",
            "Peng Wang",
            "Shumin Deng",
            "Mengru Wang",
            "Zekun Xi",
            "Shengyu Mao",
            "Jintian Zhang",
            "Yuansheng Ni",
            "Siyuan Cheng",
            "Ziwen Xu",
            "Xin Xu",
            "Jia-Chen Gu",
            "Yong Jiang",
            "Pengjun Xie",
            "Fei Huang",
            "Lei Liang",
            "Zhiqiang Zhang",
            "Xiaowei Zhu",
            "Jun Zhou",
            "Huajun Chen"
        ],
        "abstract": "Large Language Models (LLMs) have shown extraordinary capabilities in\nunderstanding and generating text that closely mirrors human communication.\nHowever, a primary limitation lies in the significant computational demands\nduring training, arising from their extensive parameterization. This challenge\nis further intensified by the dynamic nature of the world, necessitating\nfrequent updates to LLMs to correct outdated information or integrate new\nknowledge, thereby ensuring their continued relevance. Note that many\napplications demand continual model adjustments post-training to address\ndeficiencies or undesirable behaviors. There is an increasing interest in\nefficient, lightweight methods for on-the-fly model modifications. To this end,\nrecent years have seen a burgeoning in the techniques of knowledge editing for\nLLMs, which aim to efficiently modify LLMs' behaviors within specific domains\nwhile preserving overall performance across various inputs. In this paper, we\nfirst define the knowledge editing problem and then provide a comprehensive\nreview of cutting-edge approaches. Drawing inspiration from educational and\ncognitive research theories, we propose a unified categorization criterion that\nclassifies knowledge editing methods into three groups: resorting to external\nknowledge, merging knowledge into the model, and editing intrinsic knowledge.\nFurthermore, we introduce a new benchmark, KnowEdit, for a comprehensive\nempirical evaluation of representative knowledge editing approaches.\nAdditionally, we provide an in-depth analysis of knowledge location, which can\ngive a deeper understanding of the knowledge structures inherent within LLMs.\nFinally, we discuss several potential applications of knowledge editing,\noutlining its broad and impactful implications.",
        "publication_date": "2024-01-02T16:54:58Z",
        "upvotes": 15
    },
    "2401.00896": {
        "url": "https://arxiv.org/abs/2401.00896",
        "title": "TrailBlazer: Trajectory Control for Diffusion-Based Video Generation",
        "authors": [
            "Wan-Duo Kurt Ma",
            "J. P. Lewis",
            "W. Bastiaan Kleijn"
        ],
        "abstract": "Within recent approaches to text-to-video (T2V) generation, achieving\ncontrollability in the synthesized video is often a challenge. Typically, this\nissue is addressed by providing low-level per-frame guidance in the form of\nedge maps, depth maps, or an existing video to be altered. However, the process\nof obtaining such guidance can be labor-intensive. This paper focuses on\nenhancing controllability in video synthesis by employing straightforward\nbounding boxes to guide the subject in various ways, all without the need for\nneural network training, finetuning, optimization at inference time, or the use\nof pre-existing videos. Our algorithm, TrailBlazer, is constructed upon a\npre-trained (T2V) model, and easy to implement. The subject is directed by a\nbounding box through the proposed spatial and temporal attention map editing.\nMoreover, we introduce the concept of keyframing, allowing the subject\ntrajectory and overall appearance to be guided by both a moving bounding box\nand corresponding prompts, without the need to provide a detailed mask. The\nmethod is efficient, with negligible additional computation relative to the\nunderlying pre-trained model. Despite the simplicity of the bounding box\nguidance, the resulting motion is surprisingly natural, with emergent effects\nincluding perspective and movement toward the virtual camera as the box size\nincreases.",
        "publication_date": "2023-12-31T10:51:52Z",
        "upvotes": 12
    },
    "2401.01173": {
        "url": "https://arxiv.org/abs/2401.01173",
        "title": "En3D: An Enhanced Generative Model for Sculpting 3D Humans from 2D\n  Synthetic Data",
        "authors": [
            "Yifang Men",
            "Biwen Lei",
            "Yuan Yao",
            "Miaomiao Cui",
            "Zhouhui Lian",
            "Xuansong Xie"
        ],
        "abstract": "We present En3D, an enhanced generative scheme for sculpting high-quality 3D\nhuman avatars. Unlike previous works that rely on scarce 3D datasets or limited\n2D collections with imbalanced viewing angles and imprecise pose priors, our\napproach aims to develop a zero-shot 3D generative scheme capable of producing\nvisually realistic, geometrically accurate and content-wise diverse 3D humans\nwithout relying on pre-existing 3D or 2D assets. To address this challenge, we\nintroduce a meticulously crafted workflow that implements accurate physical\nmodeling to learn the enhanced 3D generative model from synthetic 2D data.\nDuring inference, we integrate optimization modules to bridge the gap between\nrealistic appearances and coarse 3D shapes. Specifically, En3D comprises three\nmodules: a 3D generator that accurately models generalizable 3D humans with\nrealistic appearance from synthesized balanced, diverse, and structured human\nimages; a geometry sculptor that enhances shape quality using multi-view normal\nconstraints for intricate human anatomy; and a texturing module that\ndisentangles explicit texture maps with fidelity and editability, leveraging\nsemantical UV partitioning and a differentiable rasterizer. Experimental\nresults show that our approach significantly outperforms prior works in terms\nof image quality, geometry accuracy and content diversity. We also showcase the\napplicability of our generated avatars for animation and editing, as well as\nthe scalability of our approach for content-style free adaptation.",
        "publication_date": "2024-01-02T12:06:31Z",
        "upvotes": 9
    },
    "2401.00909": {
        "url": "https://arxiv.org/abs/2401.00909",
        "title": "Taming Mode Collapse in Score Distillation for Text-to-3D Generation",
        "authors": [
            "Peihao Wang",
            "Dejia Xu",
            "Zhiwen Fan",
            "Dilin Wang",
            "Sreyas Mohan",
            "Forrest Iandola",
            "Rakesh Ranjan",
            "Yilei Li",
            "Qiang Liu",
            "Zhangyang Wang",
            "Vikas Chandra"
        ],
        "abstract": "Despite the remarkable performance of score distillation in text-to-3D\ngeneration, such techniques notoriously suffer from view inconsistency issues,\nalso known as \"Janus\" artifact, where the generated objects fake each view with\nmultiple front faces. Although empirically effective methods have approached\nthis problem via score debiasing or prompt engineering, a more rigorous\nperspective to explain and tackle this problem remains elusive. In this paper,\nwe reveal that the existing score distillation-based text-to-3D generation\nframeworks degenerate to maximal likelihood seeking on each view independently\nand thus suffer from the mode collapse problem, manifesting as the Janus\nartifact in practice. To tame mode collapse, we improve score distillation by\nre-establishing the entropy term in the corresponding variational objective,\nwhich is applied to the distribution of rendered images. Maximizing the entropy\nencourages diversity among different views in generated 3D assets, thereby\nmitigating the Janus problem. Based on this new objective, we derive a new\nupdate rule for 3D score distillation, dubbed Entropic Score Distillation\n(ESD). We theoretically reveal that ESD can be simplified and implemented by\njust adopting the classifier-free guidance trick upon variational score\ndistillation. Although embarrassingly straightforward, our extensive\nexperiments successfully demonstrate that ESD can be an effective treatment for\nJanus artifacts in score distillation.",
        "publication_date": "2023-12-31T22:47:06Z",
        "upvotes": 8
    },
    "2401.01117": {
        "url": "https://arxiv.org/abs/2401.01117",
        "title": "Q-Refine: A Perceptual Quality Refiner for AI-Generated Image",
        "authors": [
            "Chunyi Li",
            "Haoning Wu",
            "Zicheng Zhang",
            "Hongkun Hao",
            "Kaiwei Zhang",
            "Lei Bai",
            "Xiaohong Liu",
            "Xiongkuo Min",
            "Weisi Lin",
            "Guangtao Zhai"
        ],
        "abstract": "With the rapid evolution of the Text-to-Image (T2I) model in recent years,\ntheir unsatisfactory generation result has become a challenge. However,\nuniformly refining AI-Generated Images (AIGIs) of different qualities not only\nlimited optimization capabilities for low-quality AIGIs but also brought\nnegative optimization to high-quality AIGIs. To address this issue, a\nquality-award refiner named Q-Refine is proposed. Based on the preference of\nthe Human Visual System (HVS), Q-Refine uses the Image Quality Assessment (IQA)\nmetric to guide the refining process for the first time, and modify images of\ndifferent qualities through three adaptive pipelines. Experimental shows that\nfor mainstream T2I models, Q-Refine can perform effective optimization to AIGIs\nof different qualities. It can be a general refiner to optimize AIGIs from both\nfidelity and aesthetic quality levels, thus expanding the application of the\nT2I generation models.",
        "publication_date": "2024-01-02T09:11:23Z",
        "upvotes": 6
    },
    "2401.01885": {
        "url": "https://arxiv.org/abs/2401.01885",
        "title": "From Audio to Photoreal Embodiment: Synthesizing Humans in Conversations",
        "authors": [
            "Evonne Ng",
            "Javier Romero",
            "Timur Bagautdinov",
            "Shaojie Bai",
            "Trevor Darrell",
            "Angjoo Kanazawa",
            "Alexander Richard"
        ],
        "abstract": "We present a framework for generating full-bodied photorealistic avatars that\ngesture according to the conversational dynamics of a dyadic interaction. Given\nspeech audio, we output multiple possibilities of gestural motion for an\nindividual, including face, body, and hands. The key behind our method is in\ncombining the benefits of sample diversity from vector quantization with the\nhigh-frequency details obtained through diffusion to generate more dynamic,\nexpressive motion. We visualize the generated motion using highly\nphotorealistic avatars that can express crucial nuances in gestures (e.g.\nsneers and smirks). To facilitate this line of research, we introduce a\nfirst-of-its-kind multi-view conversational dataset that allows for\nphotorealistic reconstruction. Experiments show our model generates appropriate\nand diverse gestures, outperforming both diffusion- and VQ-only methods.\nFurthermore, our perceptual evaluation highlights the importance of\nphotorealism (vs. meshes) in accurately assessing subtle motion details in\nconversational gestures. Code and dataset available online.",
        "publication_date": "2024-01-03T18:55:16Z",
        "upvotes": 26
    },
    "2401.01808": {
        "url": "https://arxiv.org/abs/2401.01808",
        "title": "aMUSEd: An Open MUSE Reproduction",
        "authors": [
            "Suraj Patil",
            "William Berman",
            "Robin Rombach",
            "Patrick von Platen"
        ],
        "abstract": "We present aMUSEd, an open-source, lightweight masked image model (MIM) for\ntext-to-image generation based on MUSE. With 10 percent of MUSE's parameters,\naMUSEd is focused on fast image generation. We believe MIM is under-explored\ncompared to latent diffusion, the prevailing approach for text-to-image\ngeneration. Compared to latent diffusion, MIM requires fewer inference steps\nand is more interpretable. Additionally, MIM can be fine-tuned to learn\nadditional styles with only a single image. We hope to encourage further\nexploration of MIM by demonstrating its effectiveness on large-scale\ntext-to-image generation and releasing reproducible training code. We also\nrelease checkpoints for two models which directly produce images at 256x256 and\n512x512 resolutions.",
        "publication_date": "2024-01-03T16:10:07Z",
        "upvotes": 26
    },
    "2401.01614": {
        "url": "https://arxiv.org/abs/2401.01614",
        "title": "GPT-4V(ision) is a Generalist Web Agent, if Grounded",
        "authors": [
            "Boyuan Zheng",
            "Boyu Gou",
            "Jihyung Kil",
            "Huan Sun",
            "Yu Su"
        ],
        "abstract": "The recent development on large multimodal models (LMMs), especially\nGPT-4V(ision) and Gemini, has been quickly expanding the capability boundaries\nof multimodal models beyond traditional tasks like image captioning and visual\nquestion answering. In this work, we explore the potential of LMMs like GPT-4V\nas a generalist web agent that can follow natural language instructions to\ncomplete tasks on any given website. We propose SEEACT, a generalist web agent\nthat harnesses the power of LMMs for integrated visual understanding and acting\non the web. We evaluate on the recent MIND2WEB benchmark. In addition to\nstandard offline evaluation on cached websites, we enable a new online\nevaluation setting by developing a tool that allows running web agents on live\nwebsites. We show that GPT-4V presents a great potential for web agents -- it\ncan successfully complete 51.1 of the tasks on live websites if we manually\nground its textual plans into actions on the websites. This substantially\noutperforms text-only LLMs like GPT-4 or smaller models (FLAN-T5 and BLIP-2)\nspecifically fine-tuned for web agents. However, grounding still remains a\nmajor challenge. Existing LMM grounding strategies like set-of-mark prompting\nturns out to be not effective for web agents, and the best grounding strategy\nwe develop in this paper leverages both the HTML structure and visuals. Yet,\nthere is still a substantial gap with oracle grounding, leaving ample room for\nfurther improvement. All code, data, and evaluation tools are available at\nhttps://github.com/OSU-NLP-Group/SeeAct.",
        "publication_date": "2024-01-03T08:33:09Z",
        "upvotes": 19
    },
    "2401.01702": {
        "url": "https://arxiv.org/abs/2401.01702",
        "title": "Image Sculpting: Precise Object Editing with 3D Geometry Control",
        "authors": [
            "Jiraphon Yenphraphai",
            "Xichen Pan",
            "Sainan Liu",
            "Daniele Panozzo",
            "Saining Xie"
        ],
        "abstract": "We present Image Sculpting, a new framework for editing 2D images by\nincorporating tools from 3D geometry and graphics. This approach differs\nmarkedly from existing methods, which are confined to 2D spaces and typically\nrely on textual instructions, leading to ambiguity and limited control. Image\nSculpting converts 2D objects into 3D, enabling direct interaction with their\n3D geometry. Post-editing, these objects are re-rendered into 2D, merging into\nthe original image to produce high-fidelity results through a coarse-to-fine\nenhancement process. The framework supports precise, quantifiable, and\nphysically-plausible editing options such as pose editing, rotation,\ntranslation, 3D composition, carving, and serial addition. It marks an initial\nstep towards combining the creative freedom of generative models with the\nprecision of graphics pipelines.",
        "publication_date": "2024-01-02T18:59:35Z",
        "upvotes": 18
    },
    "2401.01827": {
        "url": "https://arxiv.org/abs/2401.01827",
        "title": "Moonshot: Towards Controllable Video Generation and Editing with\n  Multimodal Conditions",
        "authors": [
            "David Junhao Zhang",
            "Dongxu Li",
            "Hung Le",
            "Mike Zheng Shou",
            "Caiming Xiong",
            "Doyen Sahoo"
        ],
        "abstract": "Most existing video diffusion models (VDMs) are limited to mere text\nconditions. Thereby, they are usually lacking in control over visual appearance\nand geometry structure of the generated videos. This work presents Moonshot, a\nnew video generation model that conditions simultaneously on multimodal inputs\nof image and text. The model builts upon a core module, called multimodal video\nblock (MVB), which consists of conventional spatialtemporal layers for\nrepresenting video features, and a decoupled cross-attention layer to address\nimage and text inputs for appearance conditioning. In addition, we carefully\ndesign the model architecture such that it can optionally integrate with\npre-trained image ControlNet modules for geometry visual conditions, without\nneeding of extra training overhead as opposed to prior methods. Experiments\nshow that with versatile multimodal conditioning mechanisms, Moonshot\ndemonstrates significant improvement on visual quality and temporal consistency\ncompared to existing models. In addition, the model can be easily repurposed\nfor a variety of generative applications, such as personalized video\ngeneration, image animation and video editing, unveiling its potential to serve\nas a fundamental architecture for controllable video generation. Models will be\nmade public on https://github.com/salesforce/LAVIS.",
        "publication_date": "2024-01-03T16:43:47Z",
        "upvotes": 13
    },
    "2401.01647": {
        "url": "https://arxiv.org/abs/2401.01647",
        "title": "SIGNeRF: Scene Integrated Generation for Neural Radiance Fields",
        "authors": [
            "Jan-Niklas Dihlmann",
            "Andreas Engelhardt",
            "Hendrik Lensch"
        ],
        "abstract": "Advances in image diffusion models have recently led to notable improvements\nin the generation of high-quality images. In combination with Neural Radiance\nFields (NeRFs), they enabled new opportunities in 3D generation. However, most\ngenerative 3D approaches are object-centric and applying them to editing\nexisting photorealistic scenes is not trivial. We propose SIGNeRF, a novel\napproach for fast and controllable NeRF scene editing and scene-integrated\nobject generation. A new generative update strategy ensures 3D consistency\nacross the edited images, without requiring iterative optimization. We find\nthat depth-conditioned diffusion models inherently possess the capability to\ngenerate 3D consistent views by requesting a grid of images instead of single\nviews. Based on these insights, we introduce a multi-view reference sheet of\nmodified images. Our method updates an image collection consistently based on\nthe reference sheet and refines the original NeRF with the newly generated\nimage set in one go. By exploiting the depth conditioning mechanism of the\nimage diffusion model, we gain fine control over the spatial location of the\nedit and enforce shape guidance by a selected region or an external mesh.",
        "publication_date": "2024-01-03T09:46:43Z",
        "upvotes": 12
    },
    "2401.01862": {
        "url": "https://arxiv.org/abs/2401.01862",
        "title": "A Vision Check-up for Language Models",
        "authors": [
            "Pratyusha Sharma",
            "Tamar Rott Shaham",
            "Manel Baradad",
            "Stephanie Fu",
            "Adrian Rodriguez-Munoz",
            "Shivam Duggal",
            "Phillip Isola",
            "Antonio Torralba"
        ],
        "abstract": "What does learning to model relationships between strings teach large\nlanguage models (LLMs) about the visual world? We systematically evaluate LLMs'\nabilities to generate and recognize an assortment of visual concepts of\nincreasing complexity and then demonstrate how a preliminary visual\nrepresentation learning system can be trained using models of text. As language\nmodels lack the ability to consume or output visual information as pixels, we\nuse code to represent images in our study. Although LLM-generated images do not\nlook like natural images, results on image generation and the ability of models\nto correct these generated images indicate that precise modeling of strings can\nteach language models about numerous aspects of the visual world. Furthermore,\nexperiments on self-supervised visual representation learning, utilizing images\ngenerated with text models, highlight the potential to train vision models\ncapable of making semantic assessments of natural images using just LLMs.",
        "publication_date": "2024-01-03T18:09:33Z",
        "upvotes": 8
    },
    "2401.01854": {
        "url": "https://arxiv.org/abs/2401.01854",
        "title": "Multilingual Instruction Tuning With Just a Pinch of Multilinguality",
        "authors": [
            "Uri Shaham",
            "Jonathan Herzig",
            "Roee Aharoni",
            "Idan Szpektor",
            "Reut Tsarfaty",
            "Matan Eyal"
        ],
        "abstract": "As instruction-tuned large language models (LLMs) gain global adoption, their\nability to follow instructions in multiple languages becomes increasingly\ncrucial. In this work, we investigate how multilinguality during instruction\ntuning of a multilingual LLM affects instruction-following across languages\nfrom the pre-training corpus. We first show that many languages transfer some\ninstruction-following capabilities to other languages from even monolingual\ntuning. Furthermore, we find that only 40 multilingual examples integrated in\nan English tuning set substantially improve multilingual instruction-following,\nboth in seen and unseen languages during tuning. In general, we observe that\nmodels tuned on multilingual mixtures exhibit comparable or superior\nperformance in multiple languages compared to monolingually tuned models,\ndespite training on 10x fewer examples in those languages. Finally, we find\nthat diversifying the instruction tuning set with even just 2-4 languages\nsignificantly improves cross-lingual generalization. Our results suggest that\nbuilding massively multilingual instruction-tuned models can be done with only\na very small set of multilingual instruction-responses.",
        "publication_date": "2024-01-03T17:48:10Z",
        "upvotes": 8
    },
    "2401.01461": {
        "url": "https://arxiv.org/abs/2401.01461",
        "title": "Efficient Hybrid Zoom using Camera Fusion on Mobile Phones",
        "authors": [
            "Xiaotong Wu",
            "Wei-Sheng Lai",
            "YiChang Shih",
            "Charles Herrmann",
            "Michael Krainin",
            "Deqing Sun",
            "Chia-Kai Liang"
        ],
        "abstract": "DSLR cameras can achieve multiple zoom levels via shifting lens distances or\nswapping lens types. However, these techniques are not possible on smartphone\ndevices due to space constraints. Most smartphone manufacturers adopt a hybrid\nzoom system: commonly a Wide (W) camera at a low zoom level and a Telephoto (T)\ncamera at a high zoom level. To simulate zoom levels between W and T, these\nsystems crop and digitally upsample images from W, leading to significant\ndetail loss. In this paper, we propose an efficient system for hybrid zoom\nsuper-resolution on mobile devices, which captures a synchronous pair of W and\nT shots and leverages machine learning models to align and transfer details\nfrom T to W. We further develop an adaptive blending method that accounts for\ndepth-of-field mismatches, scene occlusion, flow uncertainty, and alignment\nerrors. To minimize the domain gap, we design a dual-phone camera rig to\ncapture real-world inputs and ground-truths for supervised training. Our method\ngenerates a 12-megapixel image in 500ms on a mobile platform and compares\nfavorably against state-of-the-art methods under extensive evaluation on\nreal-world scenarios.",
        "publication_date": "2024-01-02T23:28:20Z",
        "upvotes": 7
    },
    "2401.01792": {
        "url": "https://arxiv.org/abs/2401.01792",
        "title": "CoMoSVC: Consistency Model-based Singing Voice Conversion",
        "authors": [
            "Yiwen Lu",
            "Zhen Ye",
            "Wei Xue",
            "Xu Tan",
            "Qifeng Liu",
            "Yike Guo"
        ],
        "abstract": "The diffusion-based Singing Voice Conversion (SVC) methods have achieved\nremarkable performances, producing natural audios with high similarity to the\ntarget timbre. However, the iterative sampling process results in slow\ninference speed, and acceleration thus becomes crucial. In this paper, we\npropose CoMoSVC, a consistency model-based SVC method, which aims to achieve\nboth high-quality generation and high-speed sampling. A diffusion-based teacher\nmodel is first specially designed for SVC, and a student model is further\ndistilled under self-consistency properties to achieve one-step sampling.\nExperiments on a single NVIDIA GTX4090 GPU reveal that although CoMoSVC has a\nsignificantly faster inference speed than the state-of-the-art (SOTA)\ndiffusion-based SVC system, it still achieves comparable or superior conversion\nperformance based on both subjective and objective metrics. Audio samples and\ncodes are available at https://comosvc.github.io/.",
        "publication_date": "2024-01-03T15:47:17Z",
        "upvotes": 7
    },
    "2401.01755": {
        "url": "https://arxiv.org/abs/2401.01755",
        "title": "Incremental FastPitch: Chunk-based High Quality Text to Speech",
        "authors": [
            "Muyang Du",
            "Chuan Liu",
            "Junjie Lai"
        ],
        "abstract": "Parallel text-to-speech models have been widely applied for real-time speech\nsynthesis, and they offer more controllability and a much faster synthesis\nprocess compared with conventional auto-regressive models. Although parallel\nmodels have benefits in many aspects, they become naturally unfit for\nincremental synthesis due to their fully parallel architecture such as\ntransformer. In this work, we propose Incremental FastPitch, a novel FastPitch\nvariant capable of incrementally producing high-quality Mel chunks by improving\nthe architecture with chunk-based FFT blocks, training with receptive-field\nconstrained chunk attention masks, and inference with fixed size past model\nstates. Experimental results show that our proposal can produce speech quality\ncomparable to the parallel FastPitch, with a significant lower latency that\nallows even lower response time for real-time speech applications.",
        "publication_date": "2024-01-03T14:17:35Z",
        "upvotes": 6
    },
    "2401.01699": {
        "url": "https://arxiv.org/abs/2401.01699",
        "title": "WordArt Designer API: User-Driven Artistic Typography Synthesis with\n  Large Language Models on ModelScope",
        "authors": [
            "Jun-Yan He",
            "Zhi-Qi Cheng",
            "Chenyang Li",
            "Jingdong Sun",
            "Wangmeng Xiang",
            "Yusen Hu",
            "Xianhui Lin",
            "Xiaoyang Kang",
            "Zengke Jin",
            "Bin Luo",
            "Yifeng Geng",
            "Xuansong Xie",
            "Jingren Zhou"
        ],
        "abstract": "This paper introduces the WordArt Designer API, a novel framework for\nuser-driven artistic typography synthesis utilizing Large Language Models\n(LLMs) on ModelScope. We address the challenge of simplifying artistic\ntypography for non-professionals by offering a dynamic, adaptive, and\ncomputationally efficient alternative to traditional rigid templates. Our\napproach leverages the power of LLMs to understand and interpret user input,\nfacilitating a more intuitive design process. We demonstrate through various\ncase studies how users can articulate their aesthetic preferences and\nfunctional requirements, which the system then translates into unique and\ncreative typographic designs. Our evaluations indicate significant improvements\nin user satisfaction, design flexibility, and creative expression over existing\nsystems. The WordArt Designer API not only democratizes the art of typography\nbut also opens up new possibilities for personalized digital communication and\ndesign.",
        "publication_date": "2024-01-03T12:06:02Z",
        "upvotes": 6
    },
    "2401.02385": {
        "url": "https://arxiv.org/abs/2401.02385",
        "title": "TinyLlama: An Open-Source Small Language Model",
        "authors": [
            "Peiyuan Zhang",
            "Guangtao Zeng",
            "Tianduo Wang",
            "Wei Lu"
        ],
        "abstract": "We present TinyLlama, a compact 1.1B language model pretrained on around 1\ntrillion tokens for approximately 3 epochs. Building on the architecture and\ntokenizer of Llama 2, TinyLlama leverages various advances contributed by the\nopen-source community (e.g., FlashAttention), achieving better computational\nefficiency. Despite its relatively small size, TinyLlama demonstrates\nremarkable performance in a series of downstream tasks. It significantly\noutperforms existing open-source language models with comparable sizes. Our\nmodel checkpoints and code are publicly available on GitHub at\nhttps://github.com/jzhang38/TinyLlama.",
        "publication_date": "2024-01-04T17:54:59Z",
        "upvotes": 79
    },
    "2401.02038": {
        "url": "https://arxiv.org/abs/2401.02038",
        "title": "Understanding LLMs: A Comprehensive Overview from Training to Inference",
        "authors": [
            "Yiheng Liu",
            "Hao He",
            "Tianle Han",
            "Xu Zhang",
            "Mengyuan Liu",
            "Jiaming Tian",
            "Yutong Zhang",
            "Jiaqi Wang",
            "Xiaohui Gao",
            "Tianyang Zhong",
            "Yi Pan",
            "Shaochen Xu",
            "Zihao Wu",
            "Zhengliang Liu",
            "Xin Zhang",
            "Shu Zhang",
            "Xintao Hu",
            "Tuo Zhang",
            "Ning Qiang",
            "Tianming Liu",
            "Bao Ge"
        ],
        "abstract": "The introduction of ChatGPT has led to a significant increase in the\nutilization of Large Language Models (LLMs) for addressing downstream tasks.\nThere's an increasing focus on cost-efficient training and deployment within\nthis context. Low-cost training and deployment of LLMs represent the future\ndevelopment trend. This paper reviews the evolution of large language model\ntraining techniques and inference deployment technologies aligned with this\nemerging trend. The discussion on training includes various aspects, including\ndata preprocessing, training architecture, pre-training tasks, parallel\ntraining, and relevant content related to model fine-tuning. On the inference\nside, the paper covers topics such as model compression, parallel computation,\nmemory scheduling, and structural optimization. It also explores LLMs'\nutilization and provides insights into their future development.",
        "publication_date": "2024-01-04T02:43:57Z",
        "upvotes": 58
    },
    "2401.02415": {
        "url": "https://arxiv.org/abs/2401.02415",
        "title": "LLaMA Pro: Progressive LLaMA with Block Expansion",
        "authors": [
            "Chengyue Wu",
            "Yukang Gan",
            "Yixiao Ge",
            "Zeyu Lu",
            "Jiahao Wang",
            "Ye Feng",
            "Ping Luo",
            "Ying Shan"
        ],
        "abstract": "Humans generally acquire new skills without compromising the old; however,\nthe opposite holds for Large Language Models (LLMs), e.g., from LLaMA to\nCodeLLaMA. To this end, we propose a new post-pretraining method for LLMs with\nan expansion of Transformer blocks. We tune the expanded blocks using only new\ncorpus, efficiently and effectively improving the model's knowledge without\ncatastrophic forgetting. In this paper, we experiment on the corpus of code and\nmath, yielding LLaMA Pro-8.3B, a versatile foundation model initialized from\nLLaMA2-7B, excelling in general tasks, programming, and mathematics. LLaMA Pro\nand its instruction-following counterpart (LLaMA Pro-Instruct) achieve advanced\nperformance among various benchmarks, demonstrating superiority over existing\nopen models in the LLaMA family and the immense potential of reasoning and\naddressing diverse tasks as an intelligent agent. Our findings provide valuable\ninsights into integrating natural and programming languages, laying a solid\nfoundation for developing advanced language agents that operate effectively in\nvarious environments.",
        "publication_date": "2024-01-04T18:59:12Z",
        "upvotes": 49
    },
    "2401.02412": {
        "url": "https://arxiv.org/abs/2401.02412",
        "title": "LLM Augmented LLMs: Expanding Capabilities through Composition",
        "authors": [
            "Rachit Bansal",
            "Bidisha Samanta",
            "Siddharth Dalmia",
            "Nitish Gupta",
            "Shikhar Vashishth",
            "Sriram Ganapathy",
            "Abhishek Bapna",
            "Prateek Jain",
            "Partha Talukdar"
        ],
        "abstract": "Foundational models with billions of parameters which have been trained on\nlarge corpora of data have demonstrated non-trivial skills in a variety of\ndomains. However, due to their monolithic structure, it is challenging and\nexpensive to augment them or impart new skills. On the other hand, due to their\nadaptation abilities, several new instances of these models are being trained\ntowards new domains and tasks. In this work, we study the problem of efficient\nand practical composition of existing foundation models with more specific\nmodels to enable newer capabilities. To this end, we propose CALM --\nComposition to Augment Language Models -- which introduces cross-attention\nbetween models to compose their representations and enable new capabilities.\nSalient features of CALM are: (i) Scales up LLMs on new tasks by 're-using'\nexisting LLMs along with a few additional parameters and data, (ii) Existing\nmodel weights are kept intact, and hence preserves existing capabilities, and\n(iii) Applies to diverse domains and settings. We illustrate that augmenting\nPaLM2-S with a smaller model trained on low-resource languages results in an\nabsolute improvement of up to 13\\% on tasks like translation into English and\narithmetic reasoning for low-resource languages. Similarly, when PaLM2-S is\naugmented with a code-specific model, we see a relative improvement of 40\\%\nover the base model for code generation and explanation tasks -- on-par with\nfully fine-tuned counterparts.",
        "publication_date": "2024-01-04T18:53:01Z",
        "upvotes": 35
    },
    "2401.01952": {
        "url": "https://arxiv.org/abs/2401.01952",
        "title": "Instruct-Imagen: Image Generation with Multi-modal Instruction",
        "authors": [
            "Hexiang Hu",
            "Kelvin C. K. Chan",
            "Yu-Chuan Su",
            "Wenhu Chen",
            "Yandong Li",
            "Kihyuk Sohn",
            "Yang Zhao",
            "Xue Ben",
            "Boqing Gong",
            "William Cohen",
            "Ming-Wei Chang",
            "Xuhui Jia"
        ],
        "abstract": "This paper presents instruct-imagen, a model that tackles heterogeneous image\ngeneration tasks and generalizes across unseen tasks. We introduce *multi-modal\ninstruction* for image generation, a task representation articulating a range\nof generation intents with precision. It uses natural language to amalgamate\ndisparate modalities (e.g., text, edge, style, subject, etc.), such that\nabundant generation intents can be standardized in a uniform format.\n  We then build instruct-imagen by fine-tuning a pre-trained text-to-image\ndiffusion model with a two-stage framework. First, we adapt the model using the\nretrieval-augmented training, to enhance model's capabilities to ground its\ngeneration on external multimodal context. Subsequently, we fine-tune the\nadapted model on diverse image generation tasks that requires vision-language\nunderstanding (e.g., subject-driven generation, etc.), each paired with a\nmulti-modal instruction encapsulating the task's essence. Human evaluation on\nvarious image generation datasets reveals that instruct-imagen matches or\nsurpasses prior task-specific models in-domain and demonstrates promising\ngeneralization to unseen and more complex tasks.",
        "publication_date": "2024-01-03T19:31:58Z",
        "upvotes": 29
    },
    "2401.02117": {
        "url": "https://arxiv.org/abs/2401.02117",
        "title": "Mobile ALOHA: Learning Bimanual Mobile Manipulation with Low-Cost\n  Whole-Body Teleoperation",
        "authors": [
            "Zipeng Fu",
            "Tony Z. Zhao",
            "Chelsea Finn"
        ],
        "abstract": "Imitation learning from human demonstrations has shown impressive performance\nin robotics. However, most results focus on table-top manipulation, lacking the\nmobility and dexterity necessary for generally useful tasks. In this work, we\ndevelop a system for imitating mobile manipulation tasks that are bimanual and\nrequire whole-body control. We first present Mobile ALOHA, a low-cost and\nwhole-body teleoperation system for data collection. It augments the ALOHA\nsystem with a mobile base, and a whole-body teleoperation interface. Using data\ncollected with Mobile ALOHA, we then perform supervised behavior cloning and\nfind that co-training with existing static ALOHA datasets boosts performance on\nmobile manipulation tasks. With 50 demonstrations for each task, co-training\ncan increase success rates by up to 90%, allowing Mobile ALOHA to autonomously\ncomplete complex mobile manipulation tasks such as sauteing and serving a piece\nof shrimp, opening a two-door wall cabinet to store heavy cooking pots, calling\nand entering an elevator, and lightly rinsing a used pan using a kitchen\nfaucet. Project website: https://mobile-aloha.github.io",
        "publication_date": "2024-01-04T07:55:53Z",
        "upvotes": 25
    },
    "2401.02411": {
        "url": "https://arxiv.org/abs/2401.02411",
        "title": "What You See is What You GAN: Rendering Every Pixel for High-Fidelity\n  Geometry in 3D GANs",
        "authors": [
            "Alex Trevithick",
            "Matthew Chan",
            "Towaki Takikawa",
            "Umar Iqbal",
            "Shalini De Mello",
            "Manmohan Chandraker",
            "Ravi Ramamoorthi",
            "Koki Nagano"
        ],
        "abstract": "3D-aware Generative Adversarial Networks (GANs) have shown remarkable\nprogress in learning to generate multi-view-consistent images and 3D geometries\nof scenes from collections of 2D images via neural volume rendering. Yet, the\nsignificant memory and computational costs of dense sampling in volume\nrendering have forced 3D GANs to adopt patch-based training or employ\nlow-resolution rendering with post-processing 2D super resolution, which\nsacrifices multiview consistency and the quality of resolved geometry.\nConsequently, 3D GANs have not yet been able to fully resolve the rich 3D\ngeometry present in 2D images. In this work, we propose techniques to scale\nneural volume rendering to the much higher resolution of native 2D images,\nthereby resolving fine-grained 3D geometry with unprecedented detail. Our\napproach employs learning-based samplers for accelerating neural rendering for\n3D GAN training using up to 5 times fewer depth samples. This enables us to\nexplicitly \"render every pixel\" of the full-resolution image during training\nand inference without post-processing superresolution in 2D. Together with our\nstrategy to learn high-quality surface geometry, our method synthesizes\nhigh-resolution 3D geometry and strictly view-consistent images while\nmaintaining image quality on par with baselines relying on post-processing\nsuper resolution. We demonstrate state-of-the-art 3D gemetric quality on FFHQ\nand AFHQ, setting a new standard for unsupervised learning of 3D shapes in 3D\nGANs.",
        "publication_date": "2024-01-04T18:50:38Z",
        "upvotes": 12
    },
    "2401.02330": {
        "url": "https://arxiv.org/abs/2401.02330",
        "title": "LLaVA-Phi: Efficient Multi-Modal Assistant with Small Language Model",
        "authors": [
            "Yichen Zhu",
            "Minjie Zhu",
            "Ning Liu",
            "Zhicai Ou",
            "Xiaofeng Mou",
            "Jian Tang"
        ],
        "abstract": "In this paper, we introduce LLaVA-$\\phi$ (LLaVA-Phi), an efficient\nmulti-modal assistant that harnesses the power of the recently advanced small\nlanguage model, Phi-2, to facilitate multi-modal dialogues. LLaVA-Phi marks a\nnotable advancement in the realm of compact multi-modal models. It demonstrates\nthat even smaller language models, with as few as 2.7B parameters, can\neffectively engage in intricate dialogues that integrate both textual and\nvisual elements, provided they are trained with high-quality corpora. Our model\ndelivers commendable performance on publicly available benchmarks that\nencompass visual comprehension, reasoning, and knowledge-based perception.\nBeyond its remarkable performance in multi-modal dialogue tasks, our model\nopens new avenues for applications in time-sensitive environments and systems\nthat require real-time interaction, such as embodied agents. It highlights the\npotential of smaller language models to achieve sophisticated levels of\nunderstanding and interaction, while maintaining greater resource\nefficiency.The project is available at {https://github.com/zhuyiche/llava-phi}.",
        "publication_date": "2024-01-04T16:07:43Z",
        "upvotes": 11
    },
    "2401.02416": {
        "url": "https://arxiv.org/abs/2401.02416",
        "title": "ODIN: A Single Model for 2D and 3D Perception",
        "authors": [
            "Ayush Jain",
            "Pushkal Katara",
            "Nikolaos Gkanatsios",
            "Adam W. Harley",
            "Gabriel Sarch",
            "Kriti Aggarwal",
            "Vishrav Chaudhary",
            "Katerina Fragkiadaki"
        ],
        "abstract": "State-of-the-art models on contemporary 3D perception benchmarks like ScanNet\nconsume and label dataset-provided 3D point clouds, obtained through post\nprocessing of sensed multiview RGB-D images. They are typically trained\nin-domain, forego large-scale 2D pre-training and outperform alternatives that\nfeaturize the posed RGB-D multiview images instead. The gap in performance\nbetween methods that consume posed images versus post-processed 3D point clouds\nhas fueled the belief that 2D and 3D perception require distinct model\narchitectures. In this paper, we challenge this view and propose ODIN\n(Omni-Dimensional INstance segmentation), a model that can segment and label\nboth 2D RGB images and 3D point clouds, using a transformer architecture that\nalternates between 2D within-view and 3D cross-view information fusion. Our\nmodel differentiates 2D and 3D feature operations through the positional\nencodings of the tokens involved, which capture pixel coordinates for 2D patch\ntokens and 3D coordinates for 3D feature tokens. ODIN achieves state-of-the-art\nperformance on ScanNet200, Matterport3D and AI2THOR 3D instance segmentation\nbenchmarks, and competitive performance on ScanNet, S3DIS and COCO. It\noutperforms all previous works by a wide margin when the sensed 3D point cloud\nis used in place of the point cloud sampled from 3D mesh. When used as the 3D\nperception engine in an instructable embodied agent architecture, it sets a new\nstate-of-the-art on the TEACh action-from-dialogue benchmark. Our code and\ncheckpoints can be found at the project website: https://odin-seg.github.io.",
        "publication_date": "2024-01-04T18:59:25Z",
        "upvotes": 10
    },
    "2401.02072": {
        "url": "https://arxiv.org/abs/2401.02072",
        "title": "ICE-GRT: Instruction Context Enhancement by Generative Reinforcement\n  based Transformers",
        "authors": [
            "Chen Zheng",
            "Ke Sun",
            "Da Tang",
            "Yukun Ma",
            "Yuyu Zhang",
            "Chenguang Xi",
            "Xun Zhou"
        ],
        "abstract": "The emergence of Large Language Models (LLMs) such as ChatGPT and LLaMA\nencounter limitations in domain-specific tasks, with these models often lacking\ndepth and accuracy in specialized areas, and exhibiting a decrease in general\ncapabilities when fine-tuned, particularly analysis ability in small sized\nmodels. To address these gaps, we introduce ICE-GRT, utilizing Reinforcement\nLearning from Human Feedback (RLHF) grounded in Proximal Policy Optimization\n(PPO), demonstrating remarkable ability in in-domain scenarios without\ncompromising general task performance. Our exploration of ICE-GRT highlights\nits understanding and reasoning ability to not only generate robust answers but\nalso to provide detailed analyses of the reasons behind the answer. This\ncapability marks a significant progression beyond the scope of Supervised\nFine-Tuning models. The success of ICE-GRT is dependent on several crucial\nfactors, including Appropriate Data, Reward Size Scaling, KL-Control, Advantage\nNormalization, etc. The ICE-GRT model exhibits state-of-the-art performance in\ndomain-specific tasks and across 12 general Language tasks against equivalent\nsize and even larger size LLMs, highlighting the effectiveness of our approach.\nWe provide a comprehensive analysis of the ICE-GRT, underscoring the\nsignificant advancements it brings to the field of LLM.",
        "publication_date": "2024-01-04T05:47:41Z",
        "upvotes": 9
    },
    "2401.02400": {
        "url": "https://arxiv.org/abs/2401.02400",
        "title": "Learning the 3D Fauna of the Web",
        "authors": [
            "Zizhang Li",
            "Dor Litvak",
            "Ruining Li",
            "Yunzhi Zhang",
            "Tomas Jakab",
            "Christian Rupprecht",
            "Shangzhe Wu",
            "Andrea Vedaldi",
            "Jiajun Wu"
        ],
        "abstract": "Learning 3D models of all animals on the Earth requires massively scaling up\nexisting solutions. With this ultimate goal in mind, we develop 3D-Fauna, an\napproach that learns a pan-category deformable 3D animal model for more than\n100 animal species jointly. One crucial bottleneck of modeling animals is the\nlimited availability of training data, which we overcome by simply learning\nfrom 2D Internet images. We show that prior category-specific attempts fail to\ngeneralize to rare species with limited training images. We address this\nchallenge by introducing the Semantic Bank of Skinned Models (SBSM), which\nautomatically discovers a small set of base animal shapes by combining\ngeometric inductive priors with semantic knowledge implicitly captured by an\noff-the-shelf self-supervised feature extractor. To train such a model, we also\ncontribute a new large-scale dataset of diverse animal species. At inference\ntime, given a single image of any quadruped animal, our model reconstructs an\narticulated 3D mesh in a feed-forward fashion within seconds.",
        "publication_date": "2024-01-04T18:32:48Z",
        "upvotes": 8
    },
    "2401.02015": {
        "url": "https://arxiv.org/abs/2401.02015",
        "title": "Improving Diffusion-Based Image Synthesis with Context Prediction",
        "authors": [
            "Ling Yang",
            "Jingwei Liu",
            "Shenda Hong",
            "Zhilong Zhang",
            "Zhilin Huang",
            "Zheming Cai",
            "Wentao Zhang",
            "Bin Cui"
        ],
        "abstract": "Diffusion models are a new class of generative models, and have dramatically\npromoted image generation with unprecedented quality and diversity. Existing\ndiffusion models mainly try to reconstruct input image from a corrupted one\nwith a pixel-wise or feature-wise constraint along spatial axes. However, such\npoint-based reconstruction may fail to make each predicted pixel/feature fully\npreserve its neighborhood context, impairing diffusion-based image synthesis.\nAs a powerful source of automatic supervisory signal, context has been well\nstudied for learning representations. Inspired by this, we for the first time\npropose ConPreDiff to improve diffusion-based image synthesis with context\nprediction. We explicitly reinforce each point to predict its neighborhood\ncontext (i.e., multi-stride features/tokens/pixels) with a context decoder at\nthe end of diffusion denoising blocks in training stage, and remove the decoder\nfor inference. In this way, each point can better reconstruct itself by\npreserving its semantic connections with neighborhood context. This new\nparadigm of ConPreDiff can generalize to arbitrary discrete and continuous\ndiffusion backbones without introducing extra parameters in sampling procedure.\nExtensive experiments are conducted on unconditional image generation,\ntext-to-image generation and image inpainting tasks. Our ConPreDiff\nconsistently outperforms previous methods and achieves a new SOTA text-to-image\ngeneration results on MS-COCO, with a zero-shot FID score of 6.21.",
        "publication_date": "2024-01-04T01:10:56Z",
        "upvotes": 6
    },
    "2401.01970": {
        "url": "https://arxiv.org/abs/2401.01970",
        "title": "FMGS: Foundation Model Embedded 3D Gaussian Splatting for Holistic 3D\n  Scene Understanding",
        "authors": [
            "Xingxing Zuo",
            "Pouya Samangouei",
            "Yunwen Zhou",
            "Yan Di",
            "Mingyang Li"
        ],
        "abstract": "Precisely perceiving the geometric and semantic properties of real-world 3D\nobjects is crucial for the continued evolution of augmented reality and robotic\napplications. To this end, we present \\algfull{} (\\algname{}), which\nincorporates vision-language embeddings of foundation models into 3D Gaussian\nSplatting (GS). The key contribution of this work is an efficient method to\nreconstruct and represent 3D vision-language models. This is achieved by\ndistilling feature maps generated from image-based foundation models into those\nrendered from our 3D model. To ensure high-quality rendering and fast training,\nwe introduce a novel scene representation by integrating strengths from both GS\nand multi-resolution hash encodings (MHE). Our effective training procedure\nalso introduces a pixel alignment loss that makes the rendered feature distance\nof same semantic entities close, following the pixel-level semantic boundaries.\nOur results demonstrate remarkable multi-view semantic consistency,\nfacilitating diverse downstream tasks, beating state-of-the-art methods by\n$\\mathbf{10.2}$ percent on open-vocabulary language-based object detection,\ndespite that we are $\\mathbf{851\\times}$ faster for inference. This research\nexplores the intersection of vision, language, and 3D scene representation,\npaving the way for enhanced scene understanding in uncontrolled real-world\nenvironments. We plan to release the code upon paper acceptance.",
        "publication_date": "2024-01-03T20:39:02Z",
        "upvotes": 6
    },
    "2401.01974": {
        "url": "https://arxiv.org/abs/2401.01974",
        "title": "Towards Truly Zero-shot Compositional Visual Reasoning with LLMs as\n  Programmers",
        "authors": [
            "Aleksandar Stani\u0107",
            "Sergi Caelles",
            "Michael Tschannen"
        ],
        "abstract": "Visual reasoning is dominated by end-to-end neural networks scaled to\nbillions of model parameters and training examples. However, even the largest\nmodels struggle with compositional reasoning, generalization, fine-grained\nspatial and temporal reasoning, and counting. Visual reasoning with large\nlanguage models (LLMs) as controllers can, in principle, address these\nlimitations by decomposing the task and solving subtasks by orchestrating a set\nof (visual) tools. Recently, these models achieved great performance on tasks\nsuch as compositional visual question answering, visual grounding, and video\ntemporal reasoning. Nevertheless, in their current form, these models heavily\nrely on human engineering of in-context examples in the prompt, which are often\ndataset- and task-specific and require significant labor by highly skilled\nprogrammers. In this work, we present a framework that mitigates these issues\nby introducing spatially and temporally abstract routines and by leveraging a\nsmall number of labeled examples to automatically generate in-context examples,\nthereby avoiding human-created in-context examples. On a number of visual\nreasoning tasks, we show that our framework leads to consistent gains in\nperformance, makes LLMs as controllers setup more robust, and removes the need\nfor human engineering of in-context examples.",
        "publication_date": "2024-01-03T20:48:47Z",
        "upvotes": 4
    },
    "2401.02954": {
        "url": "https://arxiv.org/abs/2401.02954",
        "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
        "authors": [
            "DeepSeek-AI",
            ":",
            "Xiao Bi",
            "Deli Chen",
            "Guanting Chen",
            "Shanhuang Chen",
            "Damai Dai",
            "Chengqi Deng",
            "Honghui Ding",
            "Kai Dong",
            "Qiushi Du",
            "Zhe Fu",
            "Huazuo Gao",
            "Kaige Gao",
            "Wenjun Gao",
            "Ruiqi Ge",
            "Kang Guan",
            "Daya Guo",
            "Jianzhong Guo",
            "Guangbo Hao",
            "Zhewen Hao",
            "Ying He",
            "Wenjie Hu",
            "Panpan Huang",
            "Erhang Li",
            "Guowei Li",
            "Jiashi Li",
            "Yao Li",
            "Y. K. Li",
            "Wenfeng Liang",
            "Fangyun Lin",
            "A. X. Liu",
            "Bo Liu",
            "Wen Liu",
            "Xiaodong Liu",
            "Xin Liu",
            "Yiyuan Liu",
            "Haoyu Lu",
            "Shanghao Lu",
            "Fuli Luo",
            "Shirong Ma",
            "Xiaotao Nie",
            "Tian Pei",
            "Yishi Piao",
            "Junjie Qiu",
            "Hui Qu",
            "Tongzheng Ren",
            "Zehui Ren",
            "Chong Ruan",
            "Zhangli Sha",
            "Zhihong Shao",
            "Junxiao Song",
            "Xuecheng Su",
            "Jingxiang Sun",
            "Yaofeng Sun",
            "Minghui Tang",
            "Bingxuan Wang",
            "Peiyi Wang",
            "Shiyu Wang",
            "Yaohui Wang",
            "Yongji Wang",
            "Tong Wu",
            "Y. Wu",
            "Xin Xie",
            "Zhenda Xie",
            "Ziwei Xie",
            "Yiliang Xiong",
            "Hanwei Xu",
            "R. X. Xu",
            "Yanhong Xu",
            "Dejian Yang",
            "Yuxiang You",
            "Shuiping Yu",
            "Xingkai Yu",
            "B. Zhang",
            "Haowei Zhang",
            "Lecong Zhang",
            "Liyue Zhang",
            "Mingchuan Zhang",
            "Minghua Zhang",
            "Wentao Zhang",
            "Yichao Zhang",
            "Chenggang Zhao",
            "Yao Zhao",
            "Shangyan Zhou",
            "Shunfeng Zhou",
            "Qihao Zhu",
            "Yuheng Zou"
        ],
        "abstract": "The rapid development of open-source large language models (LLMs) has been\ntruly remarkable. However, the scaling law described in previous literature\npresents varying conclusions, which casts a dark cloud over scaling LLMs. We\ndelve into the study of scaling laws and present our distinctive findings that\nfacilitate scaling of large scale models in two commonly used open-source\nconfigurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek\nLLM, a project dedicated to advancing open-source language models with a\nlong-term perspective. To support the pre-training phase, we have developed a\ndataset that currently consists of 2 trillion tokens and is continuously\nexpanding. We further conduct supervised fine-tuning (SFT) and Direct\nPreference Optimization (DPO) on DeepSeek LLM Base models, resulting in the\ncreation of DeepSeek Chat models. Our evaluation results demonstrate that\nDeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in\nthe domains of code, mathematics, and reasoning. Furthermore, open-ended\nevaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance\ncompared to GPT-3.5.",
        "publication_date": "2024-01-05T18:59:13Z",
        "upvotes": 38
    },
    "2401.02823": {
        "url": "https://arxiv.org/abs/2401.02823",
        "title": "DocGraphLM: Documental Graph Language Model for Information Extraction",
        "authors": [
            "Dongsheng Wang",
            "Zhiqiang Ma",
            "Armineh Nourbakhsh",
            "Kang Gu",
            "Sameena Shah"
        ],
        "abstract": "Advances in Visually Rich Document Understanding (VrDU) have enabled\ninformation extraction and question answering over documents with complex\nlayouts. Two tropes of architectures have emerged -- transformer-based models\ninspired by LLMs, and Graph Neural Networks. In this paper, we introduce\nDocGraphLM, a novel framework that combines pre-trained language models with\ngraph semantics. To achieve this, we propose 1) a joint encoder architecture to\nrepresent documents, and 2) a novel link prediction approach to reconstruct\ndocument graphs. DocGraphLM predicts both directions and distances between\nnodes using a convergent joint loss function that prioritizes neighborhood\nrestoration and downweighs distant node detection. Our experiments on three\nSotA datasets show consistent improvement on IE and QA tasks with the adoption\nof graph features. Moreover, we report that adopting the graph features\naccelerates convergence in the learning process during training, despite being\nsolely constructed through link prediction.",
        "publication_date": "2024-01-05T14:15:36Z",
        "upvotes": 32
    },
    "2401.02957": {
        "url": "https://arxiv.org/abs/2401.02957",
        "title": "Denoising Vision Transformers",
        "authors": [
            "Jiawei Yang",
            "Katie Z Luo",
            "Jiefeng Li",
            "Kilian Q Weinberger",
            "Yonglong Tian",
            "Yue Wang"
        ],
        "abstract": "We delve into a nuanced but significant challenge inherent to Vision\nTransformers (ViTs): feature maps of these models exhibit grid-like artifacts,\nwhich detrimentally hurt the performance of ViTs in downstream tasks. Our\ninvestigations trace this fundamental issue down to the positional embeddings\nat the input stage. To address this, we propose a novel noise model, which is\nuniversally applicable to all ViTs. Specifically, the noise model dissects ViT\noutputs into three components: a semantics term free from noise artifacts and\ntwo artifact-related terms that are conditioned on pixel locations. Such a\ndecomposition is achieved by enforcing cross-view feature consistency with\nneural fields in a per-image basis. This per-image optimization process\nextracts artifact-free features from raw ViT outputs, providing clean features\nfor offline applications. Expanding the scope of our solution to support online\nfunctionality, we introduce a learnable denoiser to predict artifact-free\nfeatures directly from unprocessed ViT outputs, which shows remarkable\ngeneralization capabilities to novel data without the need for per-image\noptimization. Our two-stage approach, termed Denoising Vision Transformers\n(DVT), does not require re-training existing pre-trained ViTs and is\nimmediately applicable to any Transformer-based architecture. We evaluate our\nmethod on a variety of representative ViTs (DINO, MAE, DeiT-III, EVA02, CLIP,\nDINOv2, DINOv2-reg). Extensive evaluations demonstrate that our DVT\nconsistently and significantly improves existing state-of-the-art\ngeneral-purpose models in semantic and geometric tasks across multiple datasets\n(e.g., +3.84 mIoU). We hope our study will encourage a re-evaluation of ViT\ndesign, especially regarding the naive use of positional embeddings.",
        "publication_date": "2024-01-05T18:59:52Z",
        "upvotes": 26
    },
    "2401.02677": {
        "url": "https://arxiv.org/abs/2401.02677",
        "title": "Progressive Knowledge Distillation Of Stable Diffusion XL Using Layer\n  Level Loss",
        "authors": [
            "Yatharth Gupta",
            "Vishnu V. Jaddipal",
            "Harish Prabhala",
            "Sayak Paul",
            "Patrick Von Platen"
        ],
        "abstract": "Stable Diffusion XL (SDXL) has become the best open source text-to-image\nmodel (T2I) for its versatility and top-notch image quality. Efficiently\naddressing the computational demands of SDXL models is crucial for wider reach\nand applicability. In this work, we introduce two scaled-down variants, Segmind\nStable Diffusion (SSD-1B) and Segmind-Vega, with 1.3B and 0.74B parameter\nUNets, respectively, achieved through progressive removal using layer-level\nlosses focusing on reducing the model size while preserving generative quality.\nWe release these models weights at https://hf.co/Segmind. Our methodology\ninvolves the elimination of residual networks and transformer blocks from the\nU-Net structure of SDXL, resulting in significant reductions in parameters, and\nlatency. Our compact models effectively emulate the original SDXL by\ncapitalizing on transferred knowledge, achieving competitive results against\nlarger multi-billion parameter SDXL. Our work underscores the efficacy of\nknowledge distillation coupled with layer-level losses in reducing model size\nwhile preserving the high-quality generative capabilities of SDXL, thus\nfacilitating more accessible deployment in resource-constrained environments.",
        "publication_date": "2024-01-05T07:21:46Z",
        "upvotes": 21
    },
    "2401.02955": {
        "url": "https://arxiv.org/abs/2401.02955",
        "title": "Open-Vocabulary SAM: Segment and Recognize Twenty-thousand Classes\n  Interactively",
        "authors": [
            "Haobo Yuan",
            "Xiangtai Li",
            "Chong Zhou",
            "Yining Li",
            "Kai Chen",
            "Chen Change Loy"
        ],
        "abstract": "The CLIP and Segment Anything Model (SAM) are remarkable vision foundation\nmodels (VFMs). SAM excels in segmentation tasks across diverse domains, while\nCLIP is renowned for its zero-shot recognition capabilities. This paper\npresents an in-depth exploration of integrating these two models into a unified\nframework. Specifically, we introduce the Open-Vocabulary SAM, a SAM-inspired\nmodel designed for simultaneous interactive segmentation and recognition,\nleveraging two unique knowledge transfer modules: SAM2CLIP and CLIP2SAM. The\nformer adapts SAM's knowledge into the CLIP via distillation and learnable\ntransformer adapters, while the latter transfers CLIP knowledge into SAM,\nenhancing its recognition capabilities. Extensive experiments on various\ndatasets and detectors show the effectiveness of Open-Vocabulary SAM in both\nsegmentation and recognition tasks, significantly outperforming the naive\nbaselines of simply combining SAM and CLIP. Furthermore, aided with image\nclassification data training, our method can segment and recognize\napproximately 22,000 classes.",
        "publication_date": "2024-01-05T18:59:22Z",
        "upvotes": 16
    },
    "2401.02839": {
        "url": "https://arxiv.org/abs/2401.02839",
        "title": "Pheme: Efficient and Conversational Speech Generation",
        "authors": [
            "Pawe\u0142 Budzianowski",
            "Taras Sereda",
            "Tomasz Cichy",
            "Ivan Vuli\u0107"
        ],
        "abstract": "In recent years, speech generation has seen remarkable progress, now\nachieving one-shot generation capability that is often virtually\nindistinguishable from real human voice. Integrating such advancements in\nspeech generation with large language models might revolutionize a wide range\nof applications. However, certain applications, such as assistive\nconversational systems, require natural and conversational speech generation\ntools that also operate efficiently in real time. Current state-of-the-art\nmodels like VALL-E and SoundStorm, powered by hierarchical neural audio codecs,\nrequire large neural components and extensive training data to work well. In\ncontrast, MQTTS aims to build more compact conversational TTS models while\ncapitalizing on smaller-scale real-life conversational speech data. However,\nits autoregressive nature yields high inference latency and thus limits its\nreal-time usage. In order to mitigate the current limitations of the\nstate-of-the-art TTS models while capitalizing on their strengths, in this work\nwe introduce the Pheme model series that 1) offers compact yet high-performing\nmodels, 2) allows for parallel speech generation of 3) natural conversational\nspeech, and 4) it can be trained efficiently on smaller-scale conversational\ndata, cutting data demands by more than 10x but still matching the quality of\nthe autoregressive TTS models. We also show that through simple teacher-student\ndistillation we can meet significant improvements in voice quality for\nsingle-speaker setups on top of pretrained Pheme checkpoints, relying solely on\nsynthetic speech generated by much larger teacher models. Audio samples and\npretrained models are available online.",
        "publication_date": "2024-01-05T14:47:20Z",
        "upvotes": 14
    },
    "2401.02669": {
        "url": "https://arxiv.org/abs/2401.02669",
        "title": "Infinite-LLM: Efficient LLM Service for Long Context with DistAttention\n  and Distributed KVCache",
        "authors": [
            "Bin Lin",
            "Tao Peng",
            "Chen Zhang",
            "Minmin Sun",
            "Lanbo Li",
            "Hanyu Zhao",
            "Wencong Xiao",
            "Qi Xu",
            "Xiafei Qiu",
            "Shen Li",
            "Zhigang Ji",
            "Yong Li",
            "Wei Lin"
        ],
        "abstract": "The rapid proliferation of Large Language Models (LLMs) has been a driving\nforce in the growth of cloud-based LLM services, which are now integral to\nadvancing AI applications. However, the dynamic auto-regressive nature of LLM\nservice, along with the need to support exceptionally long context lengths,\ndemands the flexible allocation and release of substantial resources. This\npresents considerable challenges in designing cloud-based LLM service systems,\nwhere inefficient management can lead to performance degradation or resource\nwastage. In response to these challenges, this work introduces DistAttention, a\nnovel distributed attention algorithm that segments the KV Cache into smaller,\nmanageable units, enabling distributed processing and storage of the attention\nmodule. Based on that, we propose DistKV-LLM, a distributed LLM serving system\nthat dynamically manages KV Cache and effectively orchestrates all accessible\nGPU and CPU memories spanning across the data center. This ensures a\nhigh-performance LLM service on the cloud, adaptable to a broad range of\ncontext lengths. Validated in a cloud environment with 32 NVIDIA A100 GPUs in\nconfigurations from 2 to 32 instances, our system exhibited 1.03-2.4x\nend-to-end throughput improvements and supported context lengths 2-19x longer\nthan current state-of-the-art LLM service systems, as evidenced by extensive\ntesting across 18 datasets with context lengths up to 1,900K.",
        "publication_date": "2024-01-05T06:53:00Z",
        "upvotes": 9
    },
    "2401.04088": {
        "url": "https://arxiv.org/abs/2401.04088",
        "title": "Mixtral of Experts",
        "authors": [
            "Albert Q. Jiang",
            "Alexandre Sablayrolles",
            "Antoine Roux",
            "Arthur Mensch",
            "Blanche Savary",
            "Chris Bamford",
            "Devendra Singh Chaplot",
            "Diego de las Casas",
            "Emma Bou Hanna",
            "Florian Bressand",
            "Gianna Lengyel",
            "Guillaume Bour",
            "Guillaume Lample",
            "L\u00e9lio Renard Lavaud",
            "Lucile Saulnier",
            "Marie-Anne Lachaux",
            "Pierre Stock",
            "Sandeep Subramanian",
            "Sophia Yang",
            "Szymon Antoniak",
            "Teven Le Scao",
            "Th\u00e9ophile Gervet",
            "Thibaut Lavril",
            "Thomas Wang",
            "Timoth\u00e9e Lacroix",
            "William El Sayed"
        ],
        "abstract": "We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model.\nMixtral has the same architecture as Mistral 7B, with the difference that each\nlayer is composed of 8 feedforward blocks (i.e. experts). For every token, at\neach layer, a router network selects two experts to process the current state\nand combine their outputs. Even though each token only sees two experts, the\nselected experts can be different at each timestep. As a result, each token has\naccess to 47B parameters, but only uses 13B active parameters during inference.\nMixtral was trained with a context size of 32k tokens and it outperforms or\nmatches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular,\nMixtral vastly outperforms Llama 2 70B on mathematics, code generation, and\nmultilingual benchmarks. We also provide a model fine-tuned to follow\ninstructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo,\nClaude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both\nthe base and instruct models are released under the Apache 2.0 license.",
        "publication_date": "2024-01-08T18:47:34Z",
        "upvotes": 145
    },
    "2401.04081": {
        "url": "https://arxiv.org/abs/2401.04081",
        "title": "MoE-Mamba: Efficient Selective State Space Models with Mixture of\n  Experts",
        "authors": [
            "Maciej Pi\u00f3ro",
            "Kamil Ciebiera",
            "Krystian Kr\u00f3l",
            "Jan Ludziejewski",
            "Micha\u0142 Krutul",
            "Jakub Krajewski",
            "Szymon Antoniak",
            "Piotr Mi\u0142o\u015b",
            "Marek Cygan",
            "Sebastian Jaszczur"
        ],
        "abstract": "State Space Models (SSMs) have become serious contenders in the field of\nsequential modeling, challenging the dominance of Transformers. At the same\ntime, Mixture of Experts (MoE) has significantly improved Transformer-based\nLarge Language Models, including recent state-of-the-art open models. We\npropose that to unlock the potential of SSMs for scaling, they should be\ncombined with MoE. We showcase this on Mamba, a recent SSM-based model that\nachieves remarkable performance. Our model, MoE-Mamba, outperforms both Mamba\nand baseline Transformer-MoE. In particular, MoE-Mamba reaches the same\nperformance as Mamba in $2.35\\times$ fewer training steps while preserving the\ninference performance gains of Mamba against Transformer.",
        "publication_date": "2024-01-08T18:35:07Z",
        "upvotes": 68
    },
    "2401.02994": {
        "url": "https://arxiv.org/abs/2401.02994",
        "title": "Blending Is All You Need: Cheaper, Better Alternative to\n  Trillion-Parameters LLM",
        "authors": [
            "Xiaoding Lu",
            "Zongyi Liu",
            "Adian Liusie",
            "Vyas Raina",
            "Vineet Mudupalli",
            "Yuwen Zhang",
            "William Beauchamp"
        ],
        "abstract": "In conversational AI research, there's a noticeable trend towards developing\nmodels with a larger number of parameters, exemplified by models like ChatGPT.\nWhile these expansive models tend to generate increasingly better chat\nresponses, they demand significant computational resources and memory. This\nstudy explores a pertinent question: Can a combination of smaller models\ncollaboratively achieve comparable or enhanced performance relative to a\nsingular large model? We introduce an approach termed \"blending\", a\nstraightforward yet effective method of integrating multiple chat AIs. Our\nempirical evidence suggests that when specific smaller models are\nsynergistically blended, they can potentially outperform or match the\ncapabilities of much larger counterparts. For instance, integrating just three\nmodels of moderate size (6B/13B paramaeters) can rival or even surpass the\nperformance metrics of a substantially larger model like ChatGPT (175B+\nparamaters). This hypothesis is rigorously tested using A/B testing\nmethodologies with a large user base on the Chai research platform over a span\nof thirty days. The findings underscore the potential of the \"blending\"\nstrategy as a viable approach for enhancing chat AI efficacy without a\ncorresponding surge in computational demands.",
        "publication_date": "2024-01-04T07:45:49Z",
        "upvotes": 44
    },
    "2401.03462": {
        "url": "https://arxiv.org/abs/2401.03462",
        "title": "Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon",
        "authors": [
            "Peitian Zhang",
            "Zheng Liu",
            "Shitao Xiao",
            "Ninglu Shao",
            "Qiwei Ye",
            "Zhicheng Dou"
        ],
        "abstract": "The utilization of long contexts poses a big challenge for LLMs due to their\nlimited context window size. Although the context window can be extended\nthrough fine-tuning, it will result in a considerable cost at both training and\ninference time, and exert an unfavorable impact to the LLM's original\ncapabilities. In this work, we propose a new method called Activation Beacon,\nwhich condenses LLM's raw activations into compact forms such that the LLM can\nperceive a longer context with a limited context window. Activation Beacon is\nintroduced as a plug-in module, which fully preserves the LLM's original\ncapability in short contexts. It works with the sliding window to streamingly\nprocess the long context, which leads to a competitive memory and time\nefficiency in both training and inference. Activation Beacon is trained with\nshort-sequence data of diversified condensing ratios. Thanks to such a\ntreatment, it can be effectively learned to support different context lengths\nwith a small training cost. Our experiment verifies Activation Beacon's\neffectiveness of context extension: it can remarkably accomplish high-quality\nextension of Llama-2-7B's context by $\\times100$ times (from 4K to 400K);\nmeanwhile, it can also achieve superior performances across a variety of\nlong-context language modeling and understanding tasks. The source code and\nmodel checkpoint are available at\n\\url{https://github.com/FlagOpen/FlagEmbedding}.",
        "publication_date": "2024-01-07T11:57:40Z",
        "upvotes": 25
    },
    "2401.04092": {
        "url": "https://arxiv.org/abs/2401.04092",
        "title": "GPT-4V(ision) is a Human-Aligned Evaluator for Text-to-3D Generation",
        "authors": [
            "Tong Wu",
            "Guandao Yang",
            "Zhibing Li",
            "Kai Zhang",
            "Ziwei Liu",
            "Leonidas Guibas",
            "Dahua Lin",
            "Gordon Wetzstein"
        ],
        "abstract": "Despite recent advances in text-to-3D generative methods, there is a notable\nabsence of reliable evaluation metrics. Existing metrics usually focus on a\nsingle criterion each, such as how well the asset aligned with the input text.\nThese metrics lack the flexibility to generalize to different evaluation\ncriteria and might not align well with human preferences. Conducting user\npreference studies is an alternative that offers both adaptability and\nhuman-aligned results. User studies, however, can be very expensive to scale.\nThis paper presents an automatic, versatile, and human-aligned evaluation\nmetric for text-to-3D generative models. To this end, we first develop a prompt\ngenerator using GPT-4V to generate evaluating prompts, which serve as input to\ncompare text-to-3D models. We further design a method instructing GPT-4V to\ncompare two 3D assets according to user-defined criteria. Finally, we use these\npairwise comparison results to assign these models Elo ratings. Experimental\nresults suggest our metric strongly align with human preference across\ndifferent evaluation criteria.",
        "publication_date": "2024-01-08T18:52:09Z",
        "upvotes": 18
    },
    "2401.03506": {
        "url": "https://arxiv.org/abs/2401.03506",
        "title": "DiarizationLM: Speaker Diarization Post-Processing with Large Language\n  Models",
        "authors": [
            "Quan Wang",
            "Yiling Huang",
            "Guanlong Zhao",
            "Evan Clark",
            "Wei Xia",
            "Hank Liao"
        ],
        "abstract": "In this paper, we introduce DiarizationLM, a framework to leverage large\nlanguage models (LLM) to post-process the outputs from a speaker diarization\nsystem. Various goals can be achieved with the proposed framework, such as\nimproving the readability of the diarized transcript, or reducing the word\ndiarization error rate (WDER). In this framework, the outputs of the automatic\nspeech recognition (ASR) and speaker diarization systems are represented as a\ncompact textual format, which is included in the prompt to an optionally\nfinetuned LLM. The outputs of the LLM can be used as the refined diarization\nresults with the desired enhancement. As a post-processing step, this framework\ncan be easily applied to any off-the-shelf ASR and speaker diarization systems\nwithout retraining existing components. Our experiments show that a finetuned\nPaLM 2-S model can reduce the WDER by rel. 55.5% on the Fisher telephone\nconversation dataset, and rel. 44.9% on the Callhome English dataset.",
        "publication_date": "2024-01-07T14:54:57Z",
        "upvotes": 12
    },
    "2401.03003": {
        "url": "https://arxiv.org/abs/2401.03003",
        "title": "AST-T5: Structure-Aware Pretraining for Code Generation and\n  Understanding",
        "authors": [
            "Linyuan Gong",
            "Mostafa Elhoushi",
            "Alvin Cheung"
        ],
        "abstract": "Large language models (LLMs) have made significant advancements in\ncode-related tasks, yet many LLMs treat code as simple sequences, neglecting\nits structured nature. We introduce AST-T5, a novel pretraining paradigm that\nleverages the Abstract Syntax Tree (AST) for enhanced code generation,\ntranspilation, and understanding. Using dynamic programming, our AST-Aware\nSegmentation retains code structure, while our AST-Aware Span Corruption\nobjective equips the model to reconstruct various code structures. Unlike other\nmodels, AST-T5 avoids intricate program analyses or architectural changes, so\nit integrates seamlessly with any encoder-decoder Transformer. Evaluations show\nthat AST-T5 consistently outperforms similar-sized LMs across various\ncode-related tasks. Structure-awareness makes AST-T5 particularly powerful in\ncode-to-code tasks, surpassing CodeT5 by 2 points in exact match score for the\nBugs2Fix task and by 3 points in exact match score for Java-C# Transpilation in\nCodeXGLUE. Our code and model are publicly available at\nhttps://github.com/gonglinyuan/ast_t5.",
        "publication_date": "2024-01-05T06:51:08Z",
        "upvotes": 12
    },
    "2401.03065": {
        "url": "https://arxiv.org/abs/2401.03065",
        "title": "CRUXEval: A Benchmark for Code Reasoning, Understanding and Execution",
        "authors": [
            "Alex Gu",
            "Baptiste Rozi\u00e8re",
            "Hugh Leather",
            "Armando Solar-Lezama",
            "Gabriel Synnaeve",
            "Sida I. Wang"
        ],
        "abstract": "We present CRUXEval (Code Reasoning, Understanding, and eXecution\nEvaluation), a benchmark consisting of 800 Python functions (3-13 lines). Each\nfunction comes with an input-output pair, leading to two natural tasks: input\nprediction and output prediction. First, we propose a generic recipe for\ngenerating our execution benchmark which can be used to create future variation\nof the benchmark. Second, we evaluate twenty code models on our benchmark and\ndiscover that many recent high-scoring models on HumanEval do not show the same\nimprovements on our benchmark. Third, we show that simple CoT and fine-tuning\nschemes can improve performance on our benchmark but remain far from solving\nit. The best setup, GPT-4 with chain of thought (CoT), achieves a pass@1 of 75%\nand 81% on input and output prediction, respectively. In contrast, Code Llama\n34B achieves a pass@1 of 50% and 46% on input and output prediction,\nhighlighting the gap between open and closed source models. As no model is\nclose to acing CRUXEval, we provide examples of consistent GPT-4 failures on\nsimple programs as a lens into its code reasoning capabilities and areas for\nimprovement.",
        "publication_date": "2024-01-05T20:53:51Z",
        "upvotes": 10
    },
    "2401.02987": {
        "url": "https://arxiv.org/abs/2401.02987",
        "title": "Has Your Pretrained Model Improved? A Multi-head Posterior Based\n  Approach",
        "authors": [
            "Prince Aboagye",
            "Yan Zheng",
            "Junpeng Wang",
            "Uday Singh Saini",
            "Xin Dai",
            "Michael Yeh",
            "Yujie Fan",
            "Zhongfang Zhuang",
            "Shubham Jain",
            "Liang Wang",
            "Wei Zhang"
        ],
        "abstract": "The emergence of pre-trained models has significantly impacted Natural\nLanguage Processing (NLP) and Computer Vision to relational datasets.\nTraditionally, these models are assessed through fine-tuned downstream tasks.\nHowever, this raises the question of how to evaluate these models more\nefficiently and more effectively. In this study, we explore a novel approach\nwhere we leverage the meta-features associated with each entity as a source of\nworldly knowledge and employ entity representations from the models. We propose\nusing the consistency between these representations and the meta-features as a\nmetric for evaluating pre-trained models. Our method's effectiveness is\ndemonstrated across various domains, including models with relational datasets,\nlarge language models and image models.",
        "publication_date": "2024-01-02T17:08:26Z",
        "upvotes": 8
    },
    "2401.03804": {
        "url": "https://arxiv.org/abs/2401.03804",
        "title": "TeleChat Technical Report",
        "authors": [
            "Zhongjiang He",
            "Zihan Wang",
            "Xinzhang Liu",
            "Shixuan Liu",
            "Yitong Yao",
            "Yuyao Huang",
            "Xuelong Li",
            "Yongxiang Li",
            "Zhonghao Che",
            "Zhaoxi Zhang",
            "Yan Wang",
            "Xin Wang",
            "Luwen Pu",
            "Huinan Xu",
            "Ruiyu Fang",
            "Yu Zhao",
            "Jie Zhang",
            "Xiaomeng Huang",
            "Zhilong Lu",
            "Jiaxin Peng",
            "Wenjun Zheng",
            "Shiquan Wang",
            "Bingkai Yang",
            "Xuewei he",
            "Zhuoru Jiang",
            "Qiyi Xie",
            "Yanhan Zhang",
            "Zhongqiu Li",
            "Lingling Shi",
            "Weiwei Fu",
            "Yin Zhang",
            "Zilu Huang",
            "Sishi Xiong",
            "Yuxiang Zhang",
            "Chao Wang",
            "Shuangyong Song"
        ],
        "abstract": "In this technical report, we present TeleChat, a collection of large language\nmodels (LLMs) with parameters of 3 billion, 7 billion and 12 billion. It\nincludes pretrained language models as well as fine-tuned chat models that is\naligned with human preferences. TeleChat is initially pretrained on an\nextensive corpus containing a diverse collection of texts from both English and\nChinese languages, including trillions of tokens. Subsequently, the model\nundergoes fine-tuning to align with human preferences, following a detailed\nmethodology that we describe. We evaluate the performance of TeleChat on\nvarious tasks, including language understanding, mathematics, reasoning, code\ngeneration, and knowledge-based question answering. Our findings indicate that\nTeleChat achieves comparable performance to other open-source models of similar\nsize across a wide range of public benchmarks. To support future research and\napplications utilizing LLMs, we release the fine-tuned model checkpoints of\nTeleChat's 7B and 12B variant, along with code and a portion of our pretraining\ndata, to the public community.",
        "publication_date": "2024-01-08T10:43:19Z",
        "upvotes": 7
    },
    "2401.04099": {
        "url": "https://arxiv.org/abs/2401.04099",
        "title": "AGG: Amortized Generative 3D Gaussians for Single Image to 3D",
        "authors": [
            "Dejia Xu",
            "Ye Yuan",
            "Morteza Mardani",
            "Sifei Liu",
            "Jiaming Song",
            "Zhangyang Wang",
            "Arash Vahdat"
        ],
        "abstract": "Given the growing need for automatic 3D content creation pipelines, various\n3D representations have been studied to generate 3D objects from a single\nimage. Due to its superior rendering efficiency, 3D Gaussian splatting-based\nmodels have recently excelled in both 3D reconstruction and generation. 3D\nGaussian splatting approaches for image to 3D generation are often\noptimization-based, requiring many computationally expensive score-distillation\nsteps. To overcome these challenges, we introduce an Amortized Generative 3D\nGaussian framework (AGG) that instantly produces 3D Gaussians from a single\nimage, eliminating the need for per-instance optimization. Utilizing an\nintermediate hybrid representation, AGG decomposes the generation of 3D\nGaussian locations and other appearance attributes for joint optimization.\nMoreover, we propose a cascaded pipeline that first generates a coarse\nrepresentation of the 3D data and later upsamples it with a 3D Gaussian\nsuper-resolution module. Our method is evaluated against existing\noptimization-based 3D Gaussian frameworks and sampling-based pipelines\nutilizing other 3D representations, where AGG showcases competitive generation\nabilities both qualitatively and quantitatively while being several orders of\nmagnitude faster. Project page: https://ir1d.github.io/AGG/",
        "publication_date": "2024-01-08T18:56:33Z",
        "upvotes": 5
    },
    "2401.04468": {
        "url": "https://arxiv.org/abs/2401.04468",
        "title": "MagicVideo-V2: Multi-Stage High-Aesthetic Video Generation",
        "authors": [
            "Weimin Wang",
            "Jiawei Liu",
            "Zhijie Lin",
            "Jiangqiao Yan",
            "Shuo Chen",
            "Chetwin Low",
            "Tuyen Hoang",
            "Jie Wu",
            "Jun Hao Liew",
            "Hanshu Yan",
            "Daquan Zhou",
            "Jiashi Feng"
        ],
        "abstract": "The growing demand for high-fidelity video generation from textual\ndescriptions has catalyzed significant research in this field. In this work, we\nintroduce MagicVideo-V2 that integrates the text-to-image model, video motion\ngenerator, reference image embedding module and frame interpolation module into\nan end-to-end video generation pipeline. Benefiting from these architecture\ndesigns, MagicVideo-V2 can generate an aesthetically pleasing, high-resolution\nvideo with remarkable fidelity and smoothness. It demonstrates superior\nperformance over leading Text-to-Video systems such as Runway, Pika 1.0, Morph,\nMoon Valley and Stable Video Diffusion model via user evaluation at large\nscale.",
        "publication_date": "2024-01-09T10:12:52Z",
        "upvotes": 45
    },
    "2401.04577": {
        "url": "https://arxiv.org/abs/2401.04577",
        "title": "Masked Audio Generation using a Single Non-Autoregressive Transformer",
        "authors": [
            "Alon Ziv",
            "Itai Gat",
            "Gael Le Lan",
            "Tal Remez",
            "Felix Kreuk",
            "Alexandre D\u00e9fossez",
            "Jade Copet",
            "Gabriel Synnaeve",
            "Yossi Adi"
        ],
        "abstract": "We introduce MAGNeT, a masked generative sequence modeling method that\noperates directly over several streams of audio tokens. Unlike prior work,\nMAGNeT is comprised of a single-stage, non-autoregressive transformer. During\ntraining, we predict spans of masked tokens obtained from a masking scheduler,\nwhile during inference we gradually construct the output sequence using several\ndecoding steps. To further enhance the quality of the generated audio, we\nintroduce a novel rescoring method in which, we leverage an external\npre-trained model to rescore and rank predictions from MAGNeT, which will be\nthen used for later decoding steps. Lastly, we explore a hybrid version of\nMAGNeT, in which we fuse between autoregressive and non-autoregressive models\nto generate the first few seconds in an autoregressive manner while the rest of\nthe sequence is being decoded in parallel. We demonstrate the efficiency of\nMAGNeT for the task of text-to-music and text-to-audio generation and conduct\nan extensive empirical evaluation, considering both objective metrics and human\nstudies. The proposed approach is comparable to the evaluated baselines, while\nbeing significantly faster (x7 faster than the autoregressive baseline).\nThrough ablation studies and analysis, we shed light on the importance of each\nof the components comprising MAGNeT, together with pointing to the trade-offs\nbetween autoregressive and non-autoregressive modeling, considering latency,\nthroughput, and generation quality. Samples are available on our demo page\nhttps://pages.cs.huji.ac.il/adiyoss-lab/MAGNeT.",
        "publication_date": "2024-01-09T14:29:39Z",
        "upvotes": 36
    },
    "2401.04658": {
        "url": "https://arxiv.org/abs/2401.04658",
        "title": "Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence\n  Lengths in Large Language Models",
        "authors": [
            "Zhen Qin",
            "Weigao Sun",
            "Dong Li",
            "Xuyang Shen",
            "Weixuan Sun",
            "Yiran Zhong"
        ],
        "abstract": "Linear attention is an efficient attention mechanism that has recently\nemerged as a promising alternative to conventional softmax attention. With its\nability to process tokens in linear computational complexities, linear\nattention, in theory, can handle sequences of unlimited length without\nsacrificing speed, i.e., maintaining a constant training speed for various\nsequence lengths with a fixed memory consumption. However, due to the issue\nwith cumulative summation (cumsum), current linear attention algorithms cannot\ndemonstrate their theoretical advantage in a causal setting. In this paper, we\npresent Lightning Attention-2, the first linear attention implementation that\nenables linear attention to realize its theoretical computational benefits. To\nachieve this, we leverage the thought of tiling, separately handling the\nintra-block and inter-block components in linear attention calculation.\nSpecifically, we utilize the conventional attention computation mechanism for\nthe intra-blocks and apply linear attention kernel tricks for the inter-blocks.\nA tiling technique is adopted through both forward and backward procedures to\ntake full advantage of the GPU hardware. We implement our algorithm in Triton\nto make it IO-aware and hardware-friendly. Various experiments are conducted on\ndifferent model sizes and sequence lengths. Lightning Attention-2 retains\nconsistent training and inference speed regardless of input sequence length and\nis significantly faster than other attention mechanisms. The source code is\navailable at https://github.com/OpenNLPLab/lightning-attention.",
        "publication_date": "2024-01-09T16:27:28Z",
        "upvotes": 23
    },
    "2401.04398": {
        "url": "https://arxiv.org/abs/2401.04398",
        "title": "Chain-of-Table: Evolving Tables in the Reasoning Chain for Table\n  Understanding",
        "authors": [
            "Zilong Wang",
            "Hao Zhang",
            "Chun-Liang Li",
            "Julian Martin Eisenschlos",
            "Vincent Perot",
            "Zifeng Wang",
            "Lesly Miculicich",
            "Yasuhisa Fujii",
            "Jingbo Shang",
            "Chen-Yu Lee",
            "Tomas Pfister"
        ],
        "abstract": "Table-based reasoning with large language models (LLMs) is a promising\ndirection to tackle many table understanding tasks, such as table-based\nquestion answering and fact verification. Compared with generic reasoning,\ntable-based reasoning requires the extraction of underlying semantics from both\nfree-form questions and semi-structured tabular data. Chain-of-Thought and its\nsimilar approaches incorporate the reasoning chain in the form of textual\ncontext, but it is still an open question how to effectively leverage tabular\ndata in the reasoning chain. We propose the Chain-of-Table framework, where\ntabular data is explicitly used in the reasoning chain as a proxy for\nintermediate thoughts. Specifically, we guide LLMs using in-context learning to\niteratively generate operations and update the table to represent a tabular\nreasoning chain. LLMs can therefore dynamically plan the next operation based\non the results of the previous ones. This continuous evolution of the table\nforms a chain, showing the reasoning process for a given tabular problem. The\nchain carries structured information of the intermediate results, enabling more\naccurate and reliable predictions. Chain-of-Table achieves new state-of-the-art\nperformance on WikiTQ, FeTaQA, and TabFact benchmarks across multiple LLM\nchoices.",
        "publication_date": "2024-01-09T07:46:26Z",
        "upvotes": 18
    },
    "2401.04718": {
        "url": "https://arxiv.org/abs/2401.04718",
        "title": "Jump Cut Smoothing for Talking Heads",
        "authors": [
            "Xiaojuan Wang",
            "Taesung Park",
            "Yang Zhou",
            "Eli Shechtman",
            "Richard Zhang"
        ],
        "abstract": "A jump cut offers an abrupt, sometimes unwanted change in the viewing\nexperience. We present a novel framework for smoothing these jump cuts, in the\ncontext of talking head videos. We leverage the appearance of the subject from\nthe other source frames in the video, fusing it with a mid-level representation\ndriven by DensePose keypoints and face landmarks. To achieve motion, we\ninterpolate the keypoints and landmarks between the end frames around the cut.\nWe then use an image translation network from the keypoints and source frames,\nto synthesize pixels. Because keypoints can contain errors, we propose a\ncross-modal attention scheme to select and pick the most appropriate source\namongst multiple options for each key point. By leveraging this mid-level\nrepresentation, our method can achieve stronger results than a strong video\ninterpolation baseline. We demonstrate our method on various jump cuts in the\ntalking head videos, such as cutting filler words, pauses, and even random\ncuts. Our experiments show that we can achieve seamless transitions, even in\nthe challenging cases where the talking head rotates or moves drastically in\nthe jump cut.",
        "publication_date": "2024-01-09T18:44:48Z",
        "upvotes": 16
    },
    "2401.04575": {
        "url": "https://arxiv.org/abs/2401.04575",
        "title": "Let's Go Shopping (LGS) -- Web-Scale Image-Text Dataset for Visual\n  Concept Understanding",
        "authors": [
            "Yatong Bai",
            "Utsav Garg",
            "Apaar Shanker",
            "Haoming Zhang",
            "Samyak Parajuli",
            "Erhan Bas",
            "Isidora Filipovic",
            "Amelia N. Chu",
            "Eugenia D Fomitcheva",
            "Elliot Branson",
            "Aerin Kim",
            "Somayeh Sojoudi",
            "Kyunghyun Cho"
        ],
        "abstract": "Vision and vision-language applications of neural networks, such as image\nclassification and captioning, rely on large-scale annotated datasets that\nrequire non-trivial data-collecting processes. This time-consuming endeavor\nhinders the emergence of large-scale datasets, limiting researchers and\npractitioners to a small number of choices. Therefore, we seek more efficient\nways to collect and annotate images. Previous initiatives have gathered\ncaptions from HTML alt-texts and crawled social media postings, but these data\nsources suffer from noise, sparsity, or subjectivity. For this reason, we turn\nto commercial shopping websites whose data meet three criteria: cleanliness,\ninformativeness, and fluency. We introduce the Let's Go Shopping (LGS) dataset,\na large-scale public dataset with 15 million image-caption pairs from publicly\navailable e-commerce websites. When compared with existing general-domain\ndatasets, the LGS images focus on the foreground object and have less complex\nbackgrounds. Our experiments on LGS show that the classifiers trained on\nexisting benchmark datasets do not readily generalize to e-commerce data, while\nspecific self-supervised visual feature extractors can better generalize.\nFurthermore, LGS's high-quality e-commerce-focused images and bimodal nature\nmake it advantageous for vision-language bi-modal tasks: LGS enables\nimage-captioning models to generate richer captions and helps text-to-image\ngeneration models achieve e-commerce style transfer.",
        "publication_date": "2024-01-09T14:24:29Z",
        "upvotes": 14
    },
    "2401.04695": {
        "url": "https://arxiv.org/abs/2401.04695",
        "title": "Narrowing the Knowledge Evaluation Gap: Open-Domain Question Answering\n  with Multi-Granularity Answers",
        "authors": [
            "Gal Yona",
            "Roee Aharoni",
            "Mor Geva"
        ],
        "abstract": "Factual questions typically can be answered correctly at different levels of\ngranularity. For example, both ``August 4, 1961'' and ``1961'' are correct\nanswers to the question ``When was Barack Obama born?''. Standard question\nanswering (QA) evaluation protocols, however, do not explicitly take this into\naccount and compare a predicted answer against answers of a single granularity\nlevel. In this work, we propose GRANOLA QA, a novel evaluation setting where a\npredicted answer is evaluated in terms of accuracy and informativeness against\na set of multi-granularity answers. We present a simple methodology for\nenriching existing datasets with multi-granularity answers, and create\nGRANOLA-EQ, a multi-granularity version of the EntityQuestions dataset. We\nevaluate a range of decoding methods on GRANOLA-EQ, including a new algorithm,\ncalled Decoding with Response Aggregation (DRAG), that is geared towards\naligning the response granularity with the model's uncertainty. Our experiments\nshow that large language models with standard decoding tend to generate\nspecific answers, which are often incorrect. In contrast, when evaluated on\nmulti-granularity answers, DRAG yields a nearly 20 point increase in accuracy\non average, which further increases for rare entities. Overall, this reveals\nthat standard evaluation and decoding schemes may significantly underestimate\nthe knowledge encapsulated in LMs.",
        "publication_date": "2024-01-09T17:44:36Z",
        "upvotes": 8
    },
    "2401.04283": {
        "url": "https://arxiv.org/abs/2401.04283",
        "title": "FADI-AEC: Fast Score Based Diffusion Model Guided by Far-end Signal for\n  Acoustic Echo Cancellation",
        "authors": [
            "Yang Liu",
            "Li Wan",
            "Yun Li",
            "Yiteng Huang",
            "Ming Sun",
            "James Luan",
            "Yangyang Shi",
            "Xin Lei"
        ],
        "abstract": "Despite the potential of diffusion models in speech enhancement, their\ndeployment in Acoustic Echo Cancellation (AEC) has been restricted. In this\npaper, we propose DI-AEC, pioneering a diffusion-based stochastic regeneration\napproach dedicated to AEC. Further, we propose FADI-AEC, fast score-based\ndiffusion AEC framework to save computational demands, making it favorable for\nedge devices. It stands out by running the score model once per frame,\nachieving a significant surge in processing efficiency. Apart from that, we\nintroduce a novel noise generation technique where far-end signals are\nutilized, incorporating both far-end and near-end signals to refine the score\nmodel's accuracy. We test our proposed method on the ICASSP2023 Microsoft deep\necho cancellation challenge evaluation dataset, where our method outperforms\nsome of the end-to-end methods and other diffusion based echo cancellation\nmethods.",
        "publication_date": "2024-01-08T23:38:04Z",
        "upvotes": 3
    },
    "2401.05252": {
        "url": "https://arxiv.org/abs/2401.05252",
        "title": "PIXART-\u03b4: Fast and Controllable Image Generation with Latent\n  Consistency Models",
        "authors": [
            "Junsong Chen",
            "Yue Wu",
            "Simian Luo",
            "Enze Xie",
            "Sayak Paul",
            "Ping Luo",
            "Hang Zhao",
            "Zhenguo Li"
        ],
        "abstract": "This technical report introduces PIXART-{\\delta}, a text-to-image synthesis\nframework that integrates the Latent Consistency Model (LCM) and ControlNet\ninto the advanced PIXART-{\\alpha} model. PIXART-{\\alpha} is recognized for its\nability to generate high-quality images of 1024px resolution through a\nremarkably efficient training process. The integration of LCM in\nPIXART-{\\delta} significantly accelerates the inference speed, enabling the\nproduction of high-quality images in just 2-4 steps. Notably, PIXART-{\\delta}\nachieves a breakthrough 0.5 seconds for generating 1024x1024 pixel images,\nmarking a 7x improvement over the PIXART-{\\alpha}. Additionally,\nPIXART-{\\delta} is designed to be efficiently trainable on 32GB V100 GPUs\nwithin a single day. With its 8-bit inference capability (von Platen et al.,\n2023), PIXART-{\\delta} can synthesize 1024px images within 8GB GPU memory\nconstraints, greatly enhancing its usability and accessibility. Furthermore,\nincorporating a ControlNet-like module enables fine-grained control over\ntext-to-image diffusion models. We introduce a novel ControlNet-Transformer\narchitecture, specifically tailored for Transformers, achieving explicit\ncontrollability alongside high-quality image generation. As a state-of-the-art,\nopen-source image generation model, PIXART-{\\delta} offers a promising\nalternative to the Stable Diffusion family of models, contributing\nsignificantly to text-to-image synthesis.",
        "publication_date": "2024-01-10T16:27:38Z",
        "upvotes": 42
    },
    "2401.05335": {
        "url": "https://arxiv.org/abs/2401.05335",
        "title": "InseRF: Text-Driven Generative Object Insertion in Neural 3D Scenes",
        "authors": [
            "Mohamad Shahbazi",
            "Liesbeth Claessens",
            "Michael Niemeyer",
            "Edo Collins",
            "Alessio Tonioni",
            "Luc Van Gool",
            "Federico Tombari"
        ],
        "abstract": "We introduce InseRF, a novel method for generative object insertion in the\nNeRF reconstructions of 3D scenes. Based on a user-provided textual description\nand a 2D bounding box in a reference viewpoint, InseRF generates new objects in\n3D scenes. Recently, methods for 3D scene editing have been profoundly\ntransformed, owing to the use of strong priors of text-to-image diffusion\nmodels in 3D generative modeling. Existing methods are mostly effective in\nediting 3D scenes via style and appearance changes or removing existing\nobjects. Generating new objects, however, remains a challenge for such methods,\nwhich we address in this study. Specifically, we propose grounding the 3D\nobject insertion to a 2D object insertion in a reference view of the scene. The\n2D edit is then lifted to 3D using a single-view object reconstruction method.\nThe reconstructed object is then inserted into the scene, guided by the priors\nof monocular depth estimation methods. We evaluate our method on various 3D\nscenes and provide an in-depth analysis of the proposed components. Our\nexperiments with generative insertion of objects in several 3D scenes indicate\nthe effectiveness of our method compared to the existing methods. InseRF is\ncapable of controllable and 3D-consistent object insertion without requiring\nexplicit 3D information as input. Please visit our project page at\nhttps://mohamad-shahbazi.github.io/inserf.",
        "publication_date": "2024-01-10T18:59:53Z",
        "upvotes": 25
    },
    "2401.05334": {
        "url": "https://arxiv.org/abs/2401.05334",
        "title": "URHand: Universal Relightable Hands",
        "authors": [
            "Zhaoxi Chen",
            "Gyeongsik Moon",
            "Kaiwen Guo",
            "Chen Cao",
            "Stanislav Pidhorskyi",
            "Tomas Simon",
            "Rohan Joshi",
            "Yuan Dong",
            "Yichen Xu",
            "Bernardo Pires",
            "He Wen",
            "Lucas Evans",
            "Bo Peng",
            "Julia Buffalini",
            "Autumn Trimble",
            "Kevyn McPhail",
            "Melissa Schoeller",
            "Shoou-I Yu",
            "Javier Romero",
            "Michael Zollh\u00f6fer",
            "Yaser Sheikh",
            "Ziwei Liu",
            "Shunsuke Saito"
        ],
        "abstract": "Existing photorealistic relightable hand models require extensive\nidentity-specific observations in different views, poses, and illuminations,\nand face challenges in generalizing to natural illuminations and novel\nidentities. To bridge this gap, we present URHand, the first universal\nrelightable hand model that generalizes across viewpoints, poses,\nilluminations, and identities. Our model allows few-shot personalization using\nimages captured with a mobile phone, and is ready to be photorealistically\nrendered under novel illuminations. To simplify the personalization process\nwhile retaining photorealism, we build a powerful universal relightable prior\nbased on neural relighting from multi-view images of hands captured in a light\nstage with hundreds of identities. The key challenge is scaling the\ncross-identity training while maintaining personalized fidelity and sharp\ndetails without compromising generalization under natural illuminations. To\nthis end, we propose a spatially varying linear lighting model as the neural\nrenderer that takes physics-inspired shading as input feature. By removing\nnon-linear activations and bias, our specifically designed lighting model\nexplicitly keeps the linearity of light transport. This enables single-stage\ntraining from light-stage data while generalizing to real-time rendering under\narbitrary continuous illuminations across diverse identities. In addition, we\nintroduce the joint learning of a physically based model and our neural\nrelighting model, which further improves fidelity and generalization. Extensive\nexperiments show that our approach achieves superior performance over existing\nmethods in terms of both quality and generalizability. We also demonstrate\nquick personalization of URHand from a short phone scan of an unseen identity.",
        "publication_date": "2024-01-10T18:59:51Z",
        "upvotes": 20
    },
    "2401.04925": {
        "url": "https://arxiv.org/abs/2401.04925",
        "title": "The Impact of Reasoning Step Length on Large Language Models",
        "authors": [
            "Mingyu Jin",
            "Qinkai Yu",
            "Dong Shu",
            "Haiyan Zhao",
            "Wenyue Hua",
            "Yanda Meng",
            "Yongfeng Zhang",
            "Mengnan Du"
        ],
        "abstract": "Chain of Thought (CoT) is significant in improving the reasoning abilities of\nlarge language models (LLMs). However, the correlation between the\neffectiveness of CoT and the length of reasoning steps in prompts remains\nlargely unknown. To shed light on this, we have conducted several empirical\nexperiments to explore the relations. Specifically, we design experiments that\nexpand and compress the rationale reasoning steps within CoT demonstrations,\nwhile keeping all other factors constant. We have the following key findings.\nFirst, the results indicate that lengthening the reasoning steps in prompts,\neven without adding new information into the prompt, considerably enhances\nLLMs' reasoning abilities across multiple datasets. Alternatively, shortening\nthe reasoning steps, even while preserving the key information, significantly\ndiminishes the reasoning abilities of models. This finding highlights the\nimportance of the number of steps in CoT prompts and provides practical\nguidance to make better use of LLMs' potential in complex problem-solving\nscenarios. Second, we also investigated the relationship between the\nperformance of CoT and the rationales used in demonstrations. Surprisingly, the\nresult shows that even incorrect rationales can yield favorable outcomes if\nthey maintain the requisite length of inference. Third, we observed that the\nadvantages of increasing reasoning steps are task-dependent: simpler tasks\nrequire fewer steps, whereas complex tasks gain significantly from longer\ninference sequences.",
        "publication_date": "2024-01-10T04:37:38Z",
        "upvotes": 15
    },
    "2401.05033": {
        "url": "https://arxiv.org/abs/2401.05033",
        "title": "Bootstrapping LLM-based Task-Oriented Dialogue Agents via Self-Talk",
        "authors": [
            "Dennis Ulmer",
            "Elman Mansimov",
            "Kaixiang Lin",
            "Justin Sun",
            "Xibin Gao",
            "Yi Zhang"
        ],
        "abstract": "Large language models (LLMs) are powerful dialogue agents, but specializing\nthem towards fulfilling a specific function can be challenging. Instructing\ntuning, i.e. tuning models on instruction and sample responses generated by\nhumans (Ouyang et al., 2022), has proven as an effective method to do so, yet\nrequires a number of data samples that a) might not be available or b) costly\nto generate. Furthermore, this cost increases when the goal is to make the LLM\nfollow a specific workflow within a dialogue instead of single instructions.\nInspired by the self-play technique in reinforcement learning and the use of\nLLMs to simulate human agents, we propose a more effective method for data\ncollection through LLMs engaging in a conversation in various roles. This\napproach generates a training data via \"self-talk\" of LLMs that can be refined\nand utilized for supervised fine-tuning. We introduce an automated way to\nmeasure the (partial) success of a dialogue. This metric is used to filter the\ngenerated conversational data that is fed back in LLM for training. Based on\nour automated and human evaluations of conversation quality, we demonstrate\nthat such self-talk data improves results. In addition, we examine the various\ncharacteristics that showcase the quality of generated dialogues and how they\ncan be connected to their potential utility as training data.",
        "publication_date": "2024-01-10T09:49:10Z",
        "upvotes": 12
    },
    "2401.05314": {
        "url": "https://arxiv.org/abs/2401.05314",
        "title": "ANIM-400K: A Large-Scale Dataset for Automated End-To-End Dubbing of\n  Video",
        "authors": [
            "Kevin Cai",
            "Chonghua Liu",
            "David M. Chan"
        ],
        "abstract": "The Internet's wealth of content, with up to 60% published in English,\nstarkly contrasts the global population, where only 18.8% are English speakers,\nand just 5.1% consider it their native language, leading to disparities in\nonline information access. Unfortunately, automated processes for dubbing of\nvideo - replacing the audio track of a video with a translated alternative -\nremains a complex and challenging task due to pipelines, necessitating precise\ntiming, facial movement synchronization, and prosody matching. While end-to-end\ndubbing offers a solution, data scarcity continues to impede the progress of\nboth end-to-end and pipeline-based methods. In this work, we introduce\nAnim-400K, a comprehensive dataset of over 425K aligned animated video segments\nin Japanese and English supporting various video-related tasks, including\nautomated dubbing, simultaneous translation, guided video summarization, and\ngenre/theme/style classification. Our dataset is made publicly available for\nresearch purposes at https://github.com/davidmchan/Anim400K.",
        "publication_date": "2024-01-10T18:32:38Z",
        "upvotes": 7
    },
    "2401.05293": {
        "url": "https://arxiv.org/abs/2401.05293",
        "title": "Score Distillation Sampling with Learned Manifold Corrective",
        "authors": [
            "Thiemo Alldieck",
            "Nikos Kolotouros",
            "Cristian Sminchisescu"
        ],
        "abstract": "Score Distillation Sampling (SDS) is a recent but already widely popular\nmethod that relies on an image diffusion model to control optimization problems\nusing text prompts. In this paper, we conduct an in-depth analysis of the SDS\nloss function, identify an inherent problem with its formulation, and propose a\nsurprisingly easy but effective fix. Specifically, we decompose the loss into\ndifferent factors and isolate the component responsible for noisy gradients. In\nthe original formulation, high text guidance is used to account for the noise,\nleading to unwanted side effects. Instead, we train a shallow network mimicking\nthe timestep-dependent denoising deficiency of the image diffusion model in\norder to effectively factor it out. We demonstrate the versatility and the\neffectiveness of our novel loss formulation through several qualitative and\nquantitative experiments, including optimization-based image synthesis and\nediting, zero-shot image translation network training, and text-to-3D\nsynthesis.",
        "publication_date": "2024-01-10T17:51:46Z",
        "upvotes": 6
    },
    "2401.05561": {
        "url": "https://arxiv.org/abs/2401.05561",
        "title": "TrustLLM: Trustworthiness in Large Language Models",
        "authors": [
            "Lichao Sun",
            "Yue Huang",
            "Haoran Wang",
            "Siyuan Wu",
            "Qihui Zhang",
            "Yuan Li",
            "Chujie Gao",
            "Yixin Huang",
            "Wenhan Lyu",
            "Yixuan Zhang",
            "Xiner Li",
            "Zhengliang Liu",
            "Yixin Liu",
            "Yijue Wang",
            "Zhikun Zhang",
            "Bertie Vidgen",
            "Bhavya Kailkhura",
            "Caiming Xiong",
            "Chaowei Xiao",
            "Chunyuan Li",
            "Eric Xing",
            "Furong Huang",
            "Hao Liu",
            "Heng Ji",
            "Hongyi Wang",
            "Huan Zhang",
            "Huaxiu Yao",
            "Manolis Kellis",
            "Marinka Zitnik",
            "Meng Jiang",
            "Mohit Bansal",
            "James Zou",
            "Jian Pei",
            "Jian Liu",
            "Jianfeng Gao",
            "Jiawei Han",
            "Jieyu Zhao",
            "Jiliang Tang",
            "Jindong Wang",
            "Joaquin Vanschoren",
            "John Mitchell",
            "Kai Shu",
            "Kaidi Xu",
            "Kai-Wei Chang",
            "Lifang He",
            "Lifu Huang",
            "Michael Backes",
            "Neil Zhenqiang Gong",
            "Philip S. Yu",
            "Pin-Yu Chen",
            "Quanquan Gu",
            "Ran Xu",
            "Rex Ying",
            "Shuiwang Ji",
            "Suman Jana",
            "Tianlong Chen",
            "Tianming Liu",
            "Tianyi Zhou",
            "William Wang",
            "Xiang Li",
            "Xiangliang Zhang",
            "Xiao Wang",
            "Xing Xie",
            "Xun Chen",
            "Xuyu Wang",
            "Yan Liu",
            "Yanfang Ye",
            "Yinzhi Cao",
            "Yong Chen",
            "Yue Zhao"
        ],
        "abstract": "Large language models (LLMs), exemplified by ChatGPT, have gained\nconsiderable attention for their excellent natural language processing\ncapabilities. Nonetheless, these LLMs present many challenges, particularly in\nthe realm of trustworthiness. Therefore, ensuring the trustworthiness of LLMs\nemerges as an important topic. This paper introduces TrustLLM, a comprehensive\nstudy of trustworthiness in LLMs, including principles for different dimensions\nof trustworthiness, established benchmark, evaluation, and analysis of\ntrustworthiness for mainstream LLMs, and discussion of open challenges and\nfuture directions. Specifically, we first propose a set of principles for\ntrustworthy LLMs that span eight different dimensions. Based on these\nprinciples, we further establish a benchmark across six dimensions including\ntruthfulness, safety, fairness, robustness, privacy, and machine ethics. We\nthen present a study evaluating 16 mainstream LLMs in TrustLLM, consisting of\nover 30 datasets. Our findings firstly show that in general trustworthiness and\nutility (i.e., functional effectiveness) are positively related. Secondly, our\nobservations reveal that proprietary LLMs generally outperform most open-source\ncounterparts in terms of trustworthiness, raising concerns about the potential\nrisks of widely accessible open-source LLMs. However, a few open-source LLMs\ncome very close to proprietary ones. Thirdly, it is important to note that some\nLLMs may be overly calibrated towards exhibiting trustworthiness, to the extent\nthat they compromise their utility by mistakenly treating benign prompts as\nharmful and consequently not responding. Finally, we emphasize the importance\nof ensuring transparency not only in the models themselves but also in the\ntechnologies that underpin trustworthiness. Knowing the specific trustworthy\ntechnologies that have been employed is crucial for analyzing their\neffectiveness.",
        "publication_date": "2024-01-10T22:07:21Z",
        "upvotes": 62
    },
    "2401.06105": {
        "url": "https://arxiv.org/abs/2401.06105",
        "title": "PALP: Prompt Aligned Personalization of Text-to-Image Models",
        "authors": [
            "Moab Arar",
            "Andrey Voynov",
            "Amir Hertz",
            "Omri Avrahami",
            "Shlomi Fruchter",
            "Yael Pritch",
            "Daniel Cohen-Or",
            "Ariel Shamir"
        ],
        "abstract": "Content creators often aim to create personalized images using personal\nsubjects that go beyond the capabilities of conventional text-to-image models.\nAdditionally, they may want the resulting image to encompass a specific\nlocation, style, ambiance, and more. Existing personalization methods may\ncompromise personalization ability or the alignment to complex textual prompts.\nThis trade-off can impede the fulfillment of user prompts and subject fidelity.\nWe propose a new approach focusing on personalization methods for a\n\\emph{single} prompt to address this issue. We term our approach prompt-aligned\npersonalization. While this may seem restrictive, our method excels in\nimproving text alignment, enabling the creation of images with complex and\nintricate prompts, which may pose a challenge for current techniques. In\nparticular, our method keeps the personalized model aligned with a target\nprompt using an additional score distillation sampling term. We demonstrate the\nversatility of our method in multi- and single-shot settings and further show\nthat it can compose multiple subjects or use inspiration from reference images,\nsuch as artworks. We compare our approach quantitatively and qualitatively with\nexisting baselines and state-of-the-art techniques.",
        "publication_date": "2024-01-11T18:35:33Z",
        "upvotes": 46
    },
    "2401.06066": {
        "url": "https://arxiv.org/abs/2401.06066",
        "title": "DeepSeekMoE: Towards Ultimate Expert Specialization in\n  Mixture-of-Experts Language Models",
        "authors": [
            "Damai Dai",
            "Chengqi Deng",
            "Chenggang Zhao",
            "R. X. Xu",
            "Huazuo Gao",
            "Deli Chen",
            "Jiashi Li",
            "Wangding Zeng",
            "Xingkai Yu",
            "Y. Wu",
            "Zhenda Xie",
            "Y. K. Li",
            "Panpan Huang",
            "Fuli Luo",
            "Chong Ruan",
            "Zhifang Sui",
            "Wenfeng Liang"
        ],
        "abstract": "In the era of large language models, Mixture-of-Experts (MoE) is a promising\narchitecture for managing computational costs when scaling up model parameters.\nHowever, conventional MoE architectures like GShard, which activate the top-$K$\nout of $N$ experts, face challenges in ensuring expert specialization, i.e.\neach expert acquires non-overlapping and focused knowledge. In response, we\npropose the DeepSeekMoE architecture towards ultimate expert specialization. It\ninvolves two principal strategies: (1) finely segmenting the experts into $mN$\nones and activating $mK$ from them, allowing for a more flexible combination of\nactivated experts; (2) isolating $K_s$ experts as shared ones, aiming at\ncapturing common knowledge and mitigating redundancy in routed experts.\nStarting from a modest scale with 2B parameters, we demonstrate that\nDeepSeekMoE 2B achieves comparable performance with GShard 2.9B, which has 1.5\ntimes the expert parameters and computation. In addition, DeepSeekMoE 2B nearly\napproaches the performance of its dense counterpart with the same number of\ntotal parameters, which set the upper bound of MoE models. Subsequently, we\nscale up DeepSeekMoE to 16B parameters and show that it achieves comparable\nperformance with LLaMA2 7B, with only about 40% of computations. Further, our\npreliminary efforts to scale up DeepSeekMoE to 145B parameters consistently\nvalidate its substantial advantages over the GShard architecture, and show its\nperformance comparable with DeepSeek 67B, using only 28.5% (maybe even 18.2%)\nof computations.",
        "publication_date": "2024-01-11T17:31:42Z",
        "upvotes": 34
    },
    "2401.06104": {
        "url": "https://arxiv.org/abs/2401.06104",
        "title": "Transformers are Multi-State RNNs",
        "authors": [
            "Matanel Oren",
            "Michael Hassid",
            "Yossi Adi",
            "Roy Schwartz"
        ],
        "abstract": "Transformers are considered conceptually different compared to the previous\ngeneration of state-of-the-art NLP models - recurrent neural networks (RNNs).\nIn this work, we demonstrate that decoder-only transformers can in fact be\nconceptualized as infinite multi-state RNNs - an RNN variant with unlimited\nhidden state size. We further show that pretrained transformers can be\nconverted into $\\textit{finite}$ multi-state RNNs by fixing the size of their\nhidden state. We observe that several existing transformers cache compression\ntechniques can be framed as such conversion policies, and introduce a novel\npolicy, TOVA, which is simpler compared to these policies. Our experiments with\nseveral long range tasks indicate that TOVA outperforms all other baseline\npolicies, while being nearly on par with the full (infinite) model, and using\nin some cases only $\\frac{1}{8}$ of the original cache size. Our results\nindicate that transformer decoder LLMs often behave in practice as RNNs. They\nalso lay out the option of mitigating one of their most painful computational\nbottlenecks - the size of their cache memory. We publicly release our code at\nhttps://github.com/schwartz-lab-NLP/TOVA.",
        "publication_date": "2024-01-11T18:35:26Z",
        "upvotes": 33
    },
    "2401.06080": {
        "url": "https://arxiv.org/abs/2401.06080",
        "title": "Secrets of RLHF in Large Language Models Part II: Reward Modeling",
        "authors": [
            "Binghai Wang",
            "Rui Zheng",
            "Lu Chen",
            "Yan Liu",
            "Shihan Dou",
            "Caishuang Huang",
            "Wei Shen",
            "Senjie Jin",
            "Enyu Zhou",
            "Chenyu Shi",
            "Songyang Gao",
            "Nuo Xu",
            "Yuhao Zhou",
            "Xiaoran Fan",
            "Zhiheng Xi",
            "Jun Zhao",
            "Xiao Wang",
            "Tao Ji",
            "Hang Yan",
            "Lixing Shen",
            "Zhan Chen",
            "Tao Gui",
            "Qi Zhang",
            "Xipeng Qiu",
            "Xuanjing Huang",
            "Zuxuan Wu",
            "Yu-Gang Jiang"
        ],
        "abstract": "Reinforcement Learning from Human Feedback (RLHF) has become a crucial\ntechnology for aligning language models with human values and intentions,\nenabling models to produce more helpful and harmless responses. Reward models\nare trained as proxies for human preferences to drive reinforcement learning\noptimization. While reward models are often considered central to achieving\nhigh performance, they face the following challenges in practical applications:\n(1) Incorrect and ambiguous preference pairs in the dataset may hinder the\nreward model from accurately capturing human intent. (2) Reward models trained\non data from a specific distribution often struggle to generalize to examples\noutside that distribution and are not suitable for iterative RLHF training.\n  In this report, we attempt to address these two issues. (1) From a data\nperspective, we propose a method to measure the strength of preferences within\nthe data, based on a voting mechanism of multiple reward models. Experimental\nresults confirm that data with varying preference strengths have different\nimpacts on reward model performance. We introduce a series of novel methods to\nmitigate the influence of incorrect and ambiguous preferences in the dataset\nand fully leverage high-quality preference data. (2) From an algorithmic\nstandpoint, we introduce contrastive learning to enhance the ability of reward\nmodels to distinguish between chosen and rejected responses, thereby improving\nmodel generalization. Furthermore, we employ meta-learning to enable the reward\nmodel to maintain the ability to differentiate subtle differences in\nout-of-distribution samples, and this approach can be utilized for iterative\nRLHF optimization.",
        "publication_date": "2024-01-11T17:56:59Z",
        "upvotes": 23
    },
    "2401.05566": {
        "url": "https://arxiv.org/abs/2401.05566",
        "title": "Sleeper Agents: Training Deceptive LLMs that Persist Through Safety\n  Training",
        "authors": [
            "Evan Hubinger",
            "Carson Denison",
            "Jesse Mu",
            "Mike Lambert",
            "Meg Tong",
            "Monte MacDiarmid",
            "Tamera Lanham",
            "Daniel M. Ziegler",
            "Tim Maxwell",
            "Newton Cheng",
            "Adam Jermyn",
            "Amanda Askell",
            "Ansh Radhakrishnan",
            "Cem Anil",
            "David Duvenaud",
            "Deep Ganguli",
            "Fazl Barez",
            "Jack Clark",
            "Kamal Ndousse",
            "Kshitij Sachan",
            "Michael Sellitto",
            "Mrinank Sharma",
            "Nova DasSarma",
            "Roger Grosse",
            "Shauna Kravec",
            "Yuntao Bai",
            "Zachary Witten",
            "Marina Favaro",
            "Jan Brauner",
            "Holden Karnofsky",
            "Paul Christiano",
            "Samuel R. Bowman",
            "Logan Graham",
            "Jared Kaplan",
            "S\u00f6ren Mindermann",
            "Ryan Greenblatt",
            "Buck Shlegeris",
            "Nicholas Schiefer",
            "Ethan Perez"
        ],
        "abstract": "Humans are capable of strategically deceptive behavior: behaving helpfully in\nmost situations, but then behaving very differently in order to pursue\nalternative objectives when given the opportunity. If an AI system learned such\na deceptive strategy, could we detect it and remove it using current\nstate-of-the-art safety training techniques? To study this question, we\nconstruct proof-of-concept examples of deceptive behavior in large language\nmodels (LLMs). For example, we train models that write secure code when the\nprompt states that the year is 2023, but insert exploitable code when the\nstated year is 2024. We find that such backdoor behavior can be made\npersistent, so that it is not removed by standard safety training techniques,\nincluding supervised fine-tuning, reinforcement learning, and adversarial\ntraining (eliciting unsafe behavior and then training to remove it). The\nbackdoor behavior is most persistent in the largest models and in models\ntrained to produce chain-of-thought reasoning about deceiving the training\nprocess, with the persistence remaining even when the chain-of-thought is\ndistilled away. Furthermore, rather than removing backdoors, we find that\nadversarial training can teach models to better recognize their backdoor\ntriggers, effectively hiding the unsafe behavior. Our results suggest that,\nonce a model exhibits deceptive behavior, standard techniques could fail to\nremove such deception and create a false impression of safety.",
        "publication_date": "2024-01-10T22:14:35Z",
        "upvotes": 23
    },
    "2401.05675": {
        "url": "https://arxiv.org/abs/2401.05675",
        "title": "Parrot: Pareto-optimal Multi-Reward Reinforcement Learning Framework for\n  Text-to-Image Generation",
        "authors": [
            "Seung Hyun Lee",
            "Yinxiao Li",
            "Junjie Ke",
            "Innfarn Yoo",
            "Han Zhang",
            "Jiahui Yu",
            "Qifei Wang",
            "Fei Deng",
            "Glenn Entis",
            "Junfeng He",
            "Gang Li",
            "Sangpil Kim",
            "Irfan Essa",
            "Feng Yang"
        ],
        "abstract": "Recent works demonstrate that using reinforcement learning (RL) with quality\nrewards can enhance the quality of generated images in text-to-image (T2I)\ngeneration. However, a simple aggregation of multiple rewards may cause\nover-optimization in certain metrics and degradation in others, and it is\nchallenging to manually find the optimal weights. An effective strategy to\njointly optimize multiple rewards in RL for T2I generation is highly desirable.\nThis paper introduces Parrot, a novel multi-reward RL framework for T2I\ngeneration. Through the use of the batch-wise Pareto optimal selection, Parrot\nautomatically identifies the optimal trade-off among different rewards during\nthe RL optimization of the T2I generation. Additionally, Parrot employs a joint\noptimization approach for the T2I model and the prompt expansion network,\nfacilitating the generation of quality-aware text prompts, thus further\nenhancing the final image quality. To counteract the potential catastrophic\nforgetting of the original user prompt due to prompt expansion, we introduce\noriginal prompt centered guidance at inference time, ensuring that the\ngenerated image remains faithful to the user input. Extensive experiments and a\nuser study demonstrate that Parrot outperforms several baseline methods across\nvarious quality criteria, including aesthetics, human preference, image\nsentiment, and text-image alignment.",
        "publication_date": "2024-01-11T05:36:36Z",
        "upvotes": 20
    },
    "2401.06003": {
        "url": "https://arxiv.org/abs/2401.06003",
        "title": "TRIPS: Trilinear Point Splatting for Real-Time Radiance Field Rendering",
        "authors": [
            "Linus Franke",
            "Darius R\u00fcckert",
            "Laura Fink",
            "Marc Stamminger"
        ],
        "abstract": "Point-based radiance field rendering has demonstrated impressive results for\nnovel view synthesis, offering a compelling blend of rendering quality and\ncomputational efficiency. However, also latest approaches in this domain are\nnot without their shortcomings. 3D Gaussian Splatting [Kerbl and Kopanas et al.\n2023] struggles when tasked with rendering highly detailed scenes, due to\nblurring and cloudy artifacts. On the other hand, ADOP [R\\\"uckert et al. 2022]\ncan accommodate crisper images, but the neural reconstruction network decreases\nperformance, it grapples with temporal instability and it is unable to\neffectively address large gaps in the point cloud.\n  In this paper, we present TRIPS (Trilinear Point Splatting), an approach that\ncombines ideas from both Gaussian Splatting and ADOP. The fundamental concept\nbehind our novel technique involves rasterizing points into a screen-space\nimage pyramid, with the selection of the pyramid layer determined by the\nprojected point size. This approach allows rendering arbitrarily large points\nusing a single trilinear write. A lightweight neural network is then used to\nreconstruct a hole-free image including detail beyond splat resolution.\nImportantly, our render pipeline is entirely differentiable, allowing for\nautomatic optimization of both point sizes and positions.\n  Our evaluation demonstrate that TRIPS surpasses existing state-of-the-art\nmethods in terms of rendering quality while maintaining a real-time frame rate\nof 60 frames per second on readily available hardware. This performance extends\nto challenging scenarios, such as scenes featuring intricate geometry,\nexpansive landscapes, and auto-exposed footage.\n  The project page is located at: https://lfranke.github.io/trips/",
        "publication_date": "2024-01-11T16:06:36Z",
        "upvotes": 19
    },
    "2401.06102": {
        "url": "https://arxiv.org/abs/2401.06102",
        "title": "Patchscopes: A Unifying Framework for Inspecting Hidden Representations\n  of Language Models",
        "authors": [
            "Asma Ghandeharioun",
            "Avi Caciularu",
            "Adam Pearce",
            "Lucas Dixon",
            "Mor Geva"
        ],
        "abstract": "Inspecting the information encoded in hidden representations of large\nlanguage models (LLMs) can explain models' behavior and verify their alignment\nwith human values. Given the capabilities of LLMs in generating\nhuman-understandable text, we propose leveraging the model itself to explain\nits internal representations in natural language. We introduce a framework\ncalled Patchscopes and show how it can be used to answer a wide range of\nquestions about an LLM's computation. We show that prior interpretability\nmethods based on projecting representations into the vocabulary space and\nintervening on the LLM computation can be viewed as instances of this\nframework. Moreover, several of their shortcomings such as failure in\ninspecting early layers or lack of expressivity can be mitigated by\nPatchscopes. Beyond unifying prior inspection techniques, Patchscopes also\nopens up new possibilities such as using a more capable model to explain the\nrepresentations of a smaller model, and unlocks new applications such as\nself-correction in multi-hop reasoning.",
        "publication_date": "2024-01-11T18:33:48Z",
        "upvotes": 18
    },
    "2401.06121": {
        "url": "https://arxiv.org/abs/2401.06121",
        "title": "TOFU: A Task of Fictitious Unlearning for LLMs",
        "authors": [
            "Pratyush Maini",
            "Zhili Feng",
            "Avi Schwarzschild",
            "Zachary C. Lipton",
            "J. Zico Kolter"
        ],
        "abstract": "Large language models trained on massive corpora of data from the web can\nmemorize and reproduce sensitive or private data raising both legal and ethical\nconcerns. Unlearning, or tuning models to forget information present in their\ntraining data, provides us with a way to protect private data after training.\nAlthough several methods exist for such unlearning, it is unclear to what\nextent they result in models equivalent to those where the data to be forgotten\nwas never learned in the first place. To address this challenge, we present\nTOFU, a Task of Fictitious Unlearning, as a benchmark aimed at helping deepen\nour understanding of unlearning. We offer a dataset of 200 diverse synthetic\nauthor profiles, each consisting of 20 question-answer pairs, and a subset of\nthese profiles called the forget set that serves as the target for unlearning.\nWe compile a suite of metrics that work together to provide a holistic picture\nof unlearning efficacy. Finally, we provide a set of baseline results from\nexisting unlearning algorithms. Importantly, none of the baselines we consider\nshow effective unlearning motivating continued efforts to develop approaches\nfor unlearning that effectively tune models so that they truly behave as if\nthey were never trained on the forget data at all.",
        "publication_date": "2024-01-11T18:57:12Z",
        "upvotes": 14
    },
    "2401.05654": {
        "url": "https://arxiv.org/abs/2401.05654",
        "title": "Towards Conversational Diagnostic AI",
        "authors": [
            "Tao Tu",
            "Anil Palepu",
            "Mike Schaekermann",
            "Khaled Saab",
            "Jan Freyberg",
            "Ryutaro Tanno",
            "Amy Wang",
            "Brenna Li",
            "Mohamed Amin",
            "Nenad Tomasev",
            "Shekoofeh Azizi",
            "Karan Singhal",
            "Yong Cheng",
            "Le Hou",
            "Albert Webson",
            "Kavita Kulkarni",
            "S Sara Mahdavi",
            "Christopher Semturs",
            "Juraj Gottweis",
            "Joelle Barral",
            "Katherine Chou",
            "Greg S Corrado",
            "Yossi Matias",
            "Alan Karthikesalingam",
            "Vivek Natarajan"
        ],
        "abstract": "At the heart of medicine lies the physician-patient dialogue, where skillful\nhistory-taking paves the way for accurate diagnosis, effective management, and\nenduring trust. Artificial Intelligence (AI) systems capable of diagnostic\ndialogue could increase accessibility, consistency, and quality of care.\nHowever, approximating clinicians' expertise is an outstanding grand challenge.\nHere, we introduce AMIE (Articulate Medical Intelligence Explorer), a Large\nLanguage Model (LLM) based AI system optimized for diagnostic dialogue.\n  AMIE uses a novel self-play based simulated environment with automated\nfeedback mechanisms for scaling learning across diverse disease conditions,\nspecialties, and contexts. We designed a framework for evaluating\nclinically-meaningful axes of performance including history-taking, diagnostic\naccuracy, management reasoning, communication skills, and empathy. We compared\nAMIE's performance to that of primary care physicians (PCPs) in a randomized,\ndouble-blind crossover study of text-based consultations with validated patient\nactors in the style of an Objective Structured Clinical Examination (OSCE). The\nstudy included 149 case scenarios from clinical providers in Canada, the UK,\nand India, 20 PCPs for comparison with AMIE, and evaluations by specialist\nphysicians and patient actors. AMIE demonstrated greater diagnostic accuracy\nand superior performance on 28 of 32 axes according to specialist physicians\nand 24 of 26 axes according to patient actors. Our research has several\nlimitations and should be interpreted with appropriate caution. Clinicians were\nlimited to unfamiliar synchronous text-chat which permits large-scale\nLLM-patient interactions but is not representative of usual clinical practice.\nWhile further research is required before AMIE could be translated to\nreal-world settings, the results represent a milestone towards conversational\ndiagnostic AI.",
        "publication_date": "2024-01-11T04:25:06Z",
        "upvotes": 13
    },
    "2401.06129": {
        "url": "https://arxiv.org/abs/2401.06129",
        "title": "Distilling Vision-Language Models on Millions of Videos",
        "authors": [
            "Yue Zhao",
            "Long Zhao",
            "Xingyi Zhou",
            "Jialin Wu",
            "Chun-Te Chu",
            "Hui Miao",
            "Florian Schroff",
            "Hartwig Adam",
            "Ting Liu",
            "Boqing Gong",
            "Philipp Kr\u00e4henb\u00fchl",
            "Liangzhe Yuan"
        ],
        "abstract": "The recent advance in vision-language models is largely attributed to the\nabundance of image-text data. We aim to replicate this success for\nvideo-language models, but there simply is not enough human-curated video-text\ndata available. We thus resort to fine-tuning a video-language model from a\nstrong image-language baseline with synthesized instructional data. The\nresulting video-language model is then used to auto-label millions of videos to\ngenerate high-quality captions. We show the adapted video-language model\nperforms well on a wide range of video-language benchmarks. For instance, it\nsurpasses the best prior result on open-ended NExT-QA by 2.8%. Besides, our\nmodel generates detailed descriptions for previously unseen videos, which\nprovide better textual supervision than existing methods. Experiments show that\na video-language dual-encoder model contrastively trained on these\nauto-generated captions is 3.8% better than the strongest baseline that also\nleverages vision-language models. Our best model outperforms state-of-the-art\nmethods on MSR-VTT zero-shot text-to-video retrieval by 6%.",
        "publication_date": "2024-01-11T18:59:53Z",
        "upvotes": 13
    },
    "2401.06071": {
        "url": "https://arxiv.org/abs/2401.06071",
        "title": "GroundingGPT:Language Enhanced Multi-modal Grounding Model",
        "authors": [
            "Zhaowei Li",
            "Qi Xu",
            "Dong Zhang",
            "Hang Song",
            "Yiqing Cai",
            "Qi Qi",
            "Ran Zhou",
            "Junting Pan",
            "Zefeng Li",
            "Van Tu Vu",
            "Zhida Huang",
            "Tao Wang"
        ],
        "abstract": "Multi-modal large language models have demonstrated impressive performance\nacross various tasks in different modalities. However, existing multi-modal\nmodels primarily emphasize capturing global information within each modality\nwhile neglecting the importance of perceiving local information across\nmodalities. Consequently, these models lack the ability to effectively\nunderstand the fine-grained details of input data, limiting their performance\nin tasks that require a more nuanced understanding. To address this limitation,\nthere is a compelling need to develop models that enable fine-grained\nunderstanding across multiple modalities, thereby enhancing their applicability\nto a wide range of tasks. In this paper, we propose GroundingGPT, a language\nenhanced multi-modal grounding model. Beyond capturing global information like\nother multi-modal models, our proposed model excels at tasks demanding a\ndetailed understanding of local information within the input. It demonstrates\nprecise identification and localization of specific regions in images or\nmoments in videos. To achieve this objective, we design a diversified dataset\nconstruction pipeline, resulting in a multi-modal, multi-granularity dataset\nfor model training. The code, dataset, and demo of our model can be found at\nhttps: //github.com/lzw-lzw/GroundingGPT.",
        "publication_date": "2024-01-11T17:41:57Z",
        "upvotes": 10
    },
    "2401.05391": {
        "url": "https://arxiv.org/abs/2401.05391",
        "title": "Efficient LLM inference solution on Intel GPU",
        "authors": [
            "Hui Wu",
            "Yi Gan",
            "Feng Yuan",
            "Jing Ma",
            "Wei Zhu",
            "Yutao Xu",
            "Hong Zhu",
            "Yuhua Zhu",
            "Xiaoli Liu",
            "Jinghui Gu"
        ],
        "abstract": "Transformer based Large Language Models (LLMs) have been widely used in many\nfields, and the efficiency of LLM inference becomes hot topic in real\napplications. However, LLMs are usually complicatedly designed in model\nstructure with massive operations and perform inference in the auto-regressive\nmode, making it a challenging task to design a system with high efficiency.\n  In this paper, we propose an efficient LLM inference solution with low\nlatency and high throughput. Firstly, we simplify the LLM decoder layer by\nfusing data movement and element-wise operations to reduce the memory access\nfrequency and lower system latency. We also propose a segment KV cache policy\nto keep key/value of the request and response tokens in separate physical\nmemory for effective device memory management, helping enlarge the runtime\nbatch size and improve system throughput. A customized\nScaled-Dot-Product-Attention kernel is designed to match our fusion policy\nbased on the segment KV cache solution. We implement our LLM inference solution\non Intel GPU and publish it publicly. Compared with the standard HuggingFace\nimplementation, the proposed solution achieves up to 7x lower token latency and\n27x higher throughput for some popular LLMs on Intel GPU.",
        "publication_date": "2023-12-19T05:40:43Z",
        "upvotes": 8
    },
    "2401.05583": {
        "url": "https://arxiv.org/abs/2401.05583",
        "title": "Diffusion Priors for Dynamic View Synthesis from Monocular Videos",
        "authors": [
            "Chaoyang Wang",
            "Peiye Zhuang",
            "Aliaksandr Siarohin",
            "Junli Cao",
            "Guocheng Qian",
            "Hsin-Ying Lee",
            "Sergey Tulyakov"
        ],
        "abstract": "Dynamic novel view synthesis aims to capture the temporal evolution of visual\ncontent within videos. Existing methods struggle to distinguishing between\nmotion and structure, particularly in scenarios where camera poses are either\nunknown or constrained compared to object motion. Furthermore, with information\nsolely from reference images, it is extremely challenging to hallucinate unseen\nregions that are occluded or partially observed in the given videos. To address\nthese issues, we first finetune a pretrained RGB-D diffusion model on the video\nframes using a customization technique. Subsequently, we distill the knowledge\nfrom the finetuned model to a 4D representations encompassing both dynamic and\nstatic Neural Radiance Fields (NeRF) components. The proposed pipeline achieves\ngeometric consistency while preserving the scene identity. We perform thorough\nexperiments to evaluate the efficacy of the proposed method qualitatively and\nquantitatively. Our results demonstrate the robustness and utility of our\napproach in challenging cases, further advancing dynamic novel view synthesis.",
        "publication_date": "2024-01-10T23:26:41Z",
        "upvotes": 7
    },
    "2401.05735": {
        "url": "https://arxiv.org/abs/2401.05735",
        "title": "Object-Centric Diffusion for Efficient Video Editing",
        "authors": [
            "Kumara Kahatapitiya",
            "Adil Karjauv",
            "Davide Abati",
            "Fatih Porikli",
            "Yuki M. Asano",
            "Amirhossein Habibian"
        ],
        "abstract": "Diffusion-based video editing have reached impressive quality and can\ntransform either the global style, local structure, and attributes of given\nvideo inputs, following textual edit prompts. However, such solutions typically\nincur heavy memory and computational costs to generate temporally-coherent\nframes, either in the form of diffusion inversion and/or cross-frame attention.\nIn this paper, we conduct an analysis of such inefficiencies, and suggest\nsimple yet effective modifications that allow significant speed-ups whilst\nmaintaining quality. Moreover, we introduce Object-Centric Diffusion, coined as\nOCD, to further reduce latency by allocating computations more towards\nforeground edited regions that are arguably more important for perceptual\nquality. We achieve this by two novel proposals: i) Object-Centric Sampling,\ndecoupling the diffusion steps spent on salient regions or background,\nallocating most of the model capacity to the former, and ii) Object-Centric 3D\nToken Merging, which reduces cost of cross-frame attention by fusing redundant\ntokens in unimportant background regions. Both techniques are readily\napplicable to a given video editing model \\textit{without} retraining, and can\ndrastically reduce its memory and computational cost. We evaluate our proposals\non inversion-based and control-signal-based editing pipelines, and show a\nlatency reduction up to 10x for a comparable synthesis quality.",
        "publication_date": "2024-01-11T08:36:15Z",
        "upvotes": 6
    },
    "2401.05749": {
        "url": "https://arxiv.org/abs/2401.05749",
        "title": "A Shocking Amount of the Web is Machine Translated: Insights from\n  Multi-Way Parallelism",
        "authors": [
            "Brian Thompson",
            "Mehak Preet Dhaliwal",
            "Peter Frisch",
            "Tobias Domhan",
            "Marcello Federico"
        ],
        "abstract": "We show that content on the web is often translated into many languages, and\nthe low quality of these multi-way translations indicates they were likely\ncreated using Machine Translation (MT). Multi-way parallel, machine generated\ncontent not only dominates the translations in lower resource languages; it\nalso constitutes a large fraction of the total web content in those languages.\nWe also find evidence of a selection bias in the type of content which is\ntranslated into many languages, consistent with low quality English content\nbeing translated en masse into many lower resource languages, via MT. Our work\nraises serious concerns about training models such as multilingual large\nlanguage models on both monolingual and bilingual data scraped from the web.",
        "publication_date": "2024-01-11T08:56:13Z",
        "upvotes": 6
    },
    "2401.05811": {
        "url": "https://arxiv.org/abs/2401.05811",
        "title": "Tuning LLMs with Contrastive Alignment Instructions for Machine\n  Translation in Unseen, Low-resource Languages",
        "authors": [
            "Zhuoyuan Mao",
            "Yen Yu"
        ],
        "abstract": "This article introduces contrastive alignment instructions (AlignInstruct) to\naddress two challenges in machine translation (MT) on large language models\n(LLMs). One is the expansion of supported languages to previously unseen ones.\nThe second relates to the lack of data in low-resource languages. Model\nfine-tuning through MT instructions (MTInstruct) is a straightforward approach\nto the first challenge. However, MTInstruct is limited by weak cross-lingual\nsignals inherent in the second challenge. AlignInstruct emphasizes\ncross-lingual supervision via a cross-lingual discriminator built using\nstatistical word alignments. Our results based on fine-tuning the BLOOMZ models\n(1b1, 3b, and 7b1) in up to 24 unseen languages showed that: (1) LLMs can\neffectively translate unseen languages using MTInstruct; (2) AlignInstruct led\nto consistent improvements in translation quality across 48 translation\ndirections involving English; (3) Discriminator-based instructions outperformed\ntheir generative counterparts as cross-lingual instructions; (4) AlignInstruct\nimproved performance in 30 zero-shot directions.",
        "publication_date": "2024-01-11T10:28:17Z",
        "upvotes": 5
    },
    "2401.07519": {
        "url": "https://arxiv.org/abs/2401.07519",
        "title": "InstantID: Zero-shot Identity-Preserving Generation in Seconds",
        "authors": [
            "Qixun Wang",
            "Xu Bai",
            "Haofan Wang",
            "Zekui Qin",
            "Anthony Chen",
            "Huaxia Li",
            "Xu Tang",
            "Yao Hu"
        ],
        "abstract": "There has been significant progress in personalized image synthesis with\nmethods such as Textual Inversion, DreamBooth, and LoRA. Yet, their real-world\napplicability is hindered by high storage demands, lengthy fine-tuning\nprocesses, and the need for multiple reference images. Conversely, existing ID\nembedding-based methods, while requiring only a single forward inference, face\nchallenges: they either necessitate extensive fine-tuning across numerous model\nparameters, lack compatibility with community pre-trained models, or fail to\nmaintain high face fidelity. Addressing these limitations, we introduce\nInstantID, a powerful diffusion model-based solution. Our plug-and-play module\nadeptly handles image personalization in various styles using just a single\nfacial image, while ensuring high fidelity. To achieve this, we design a novel\nIdentityNet by imposing strong semantic and weak spatial conditions,\nintegrating facial and landmark images with textual prompts to steer the image\ngeneration. InstantID demonstrates exceptional performance and efficiency,\nproving highly beneficial in real-world applications where identity\npreservation is paramount. Moreover, our work seamlessly integrates with\npopular pre-trained text-to-image diffusion models like SD1.5 and SDXL, serving\nas an adaptable plugin. Our codes and pre-trained checkpoints will be available\nat https://github.com/InstantID/InstantID.",
        "publication_date": "2024-01-15T07:50:18Z",
        "upvotes": 48
    },
    "2401.08541": {
        "url": "https://arxiv.org/abs/2401.08541",
        "title": "Scalable Pre-training of Large Autoregressive Image Models",
        "authors": [
            "Alaaeldin El-Nouby",
            "Michal Klein",
            "Shuangfei Zhai",
            "Miguel Angel Bautista",
            "Alexander Toshev",
            "Vaishaal Shankar",
            "Joshua M Susskind",
            "Armand Joulin"
        ],
        "abstract": "This paper introduces AIM, a collection of vision models pre-trained with an\nautoregressive objective. These models are inspired by their textual\ncounterparts, i.e., Large Language Models (LLMs), and exhibit similar scaling\nproperties. Specifically, we highlight two key findings: (1) the performance of\nthe visual features scale with both the model capacity and the quantity of\ndata, (2) the value of the objective function correlates with the performance\nof the model on downstream tasks. We illustrate the practical implication of\nthese findings by pre-training a 7 billion parameter AIM on 2 billion images,\nthat achieves 84.0% on ImageNet-1k with a frozen trunk. Interestingly, even at\nthis scale, we observe no sign of saturation in performance, suggesting that\nAIM potentially represents a new frontier for training large-scale vision\nmodels. The pre-training of AIM is similar to the pre-training of LLMs, and\ndoes not require any image-specific strategy to stabilize the training at\nscale.",
        "publication_date": "2024-01-16T18:03:37Z",
        "upvotes": 34
    },
    "2401.08417": {
        "url": "https://arxiv.org/abs/2401.08417",
        "title": "Contrastive Preference Optimization: Pushing the Boundaries of LLM\n  Performance in Machine Translation",
        "authors": [
            "Haoran Xu",
            "Amr Sharaf",
            "Yunmo Chen",
            "Weiting Tan",
            "Lingfeng Shen",
            "Benjamin Van Durme",
            "Kenton Murray",
            "Young Jin Kim"
        ],
        "abstract": "Moderate-sized large language models (LLMs) -- those with 7B or 13B\nparameters -- exhibit promising machine translation (MT) performance. However,\neven the top-performing 13B LLM-based translation models, like ALMA, does not\nmatch the performance of state-of-the-art conventional encoder-decoder\ntranslation models or larger-scale LLMs such as GPT-4. In this study, we bridge\nthis performance gap. We first assess the shortcomings of supervised\nfine-tuning for LLMs in the MT task, emphasizing the quality issues present in\nthe reference data, despite being human-generated. Then, in contrast to SFT\nwhich mimics reference translations, we introduce Contrastive Preference\nOptimization (CPO), a novel approach that trains models to avoid generating\nadequate but not perfect translations. Applying CPO to ALMA models with only\n22K parallel sentences and 12M parameters yields significant improvements. The\nresulting model, called ALMA-R, can match or exceed the performance of the WMT\ncompetition winners and GPT-4 on WMT'21, WMT'22 and WMT'23 test datasets.",
        "publication_date": "2024-01-16T15:04:51Z",
        "upvotes": 25
    },
    "2401.06951": {
        "url": "https://arxiv.org/abs/2401.06951",
        "title": "E^2-LLM: Efficient and Extreme Length Extension of Large Language Models",
        "authors": [
            "Jiaheng Liu",
            "Zhiqi Bai",
            "Yuanxing Zhang",
            "Chenchen Zhang",
            "Yu Zhang",
            "Ge Zhang",
            "Jiakai Wang",
            "Haoran Que",
            "Yukang Chen",
            "Wenbo Su",
            "Tiezheng Ge",
            "Jie Fu",
            "Wenhu Chen",
            "Bo Zheng"
        ],
        "abstract": "Typically, training LLMs with long context sizes is computationally\nexpensive, requiring extensive training hours and GPU resources. Existing\nlong-context extension methods usually need additional training procedures to\nsupport corresponding long-context windows, where the long-context training\ndata (e.g., 32k) is needed, and high GPU training costs are assumed. To address\nthe aforementioned issues, we propose an Efficient and Extreme length extension\nmethod for Large Language Models, called E 2 -LLM, with only one training\nprocedure and dramatically reduced computation cost, which also removes the\nneed to collect long-context data. Concretely, first, the training data of our\nE 2 -LLM only requires a short length (e.g., 4k), which reduces the tuning cost\ngreatly. Second, the training procedure on the short training context window is\nperformed only once time, and we can support different evaluation context\nwindows at inference. Third, in E 2 - LLM, based on RoPE position embeddings,\nwe introduce two different augmentation methods on the scale and position index\nparameters for different samples in training. It aims to make the model more\nrobust to the different relative differences when directly interpolating the\narbitrary context length at inference. Comprehensive experimental results on\nmultiple benchmark datasets demonstrate the effectiveness of our E 2 -LLM on\nchallenging long-context tasks.",
        "publication_date": "2024-01-13T02:11:20Z",
        "upvotes": 23
    },
    "2401.08565": {
        "url": "https://arxiv.org/abs/2401.08565",
        "title": "Tuning Language Models by Proxy",
        "authors": [
            "Alisa Liu",
            "Xiaochuang Han",
            "Yizhong Wang",
            "Yulia Tsvetkov",
            "Yejin Choi",
            "Noah A. Smith"
        ],
        "abstract": "Despite the general capabilities of large pretrained language models, they\nconsistently benefit from further adaptation to better achieve desired\nbehaviors. However, tuning these models has become increasingly\nresource-intensive, or impossible when model weights are private. We introduce\nproxy-tuning, a lightweight decoding-time algorithm that operates on top of\nblack-box LMs to achieve the same end as direct tuning, but by accessing only\nits predictions over the output vocabulary, not its parameters. Our method\ntunes a smaller LM, then applies the difference between the predictions of the\nsmall tuned and untuned LMs to shift the original predictions of the larger\nuntuned model in the direction of tuning, while retaining the benefits of\nlarger-scale pretraining. In experiments, when we apply proxy-tuning to\nLlama2-70B using proxies of only 7B size, we can close 88% of the gap between\nLlama2-70B and its truly-tuned chat version, when evaluated across knowledge,\nreasoning, and safety benchmarks. Interestingly, on TruthfulQA, proxy-tuned\nmodels are actually more truthful than directly tuned models, possibly because\ndecoding-time guidance better retains the model's factual knowledge. We then\ndemonstrate the generality of proxy-tuning by applying it to domain adaptation\non code, and task-specific finetuning on question-answering and math problems.\nFinally, we show how to proxy-tune a truly black-box LM, GPT-3.5, for temporal\nadaptation, increasing its knowledge about recent events. Our work demonstrates\nthe promise of using small tuned LMs to efficiently customize large,\npotentially proprietary LMs through decoding-time guidance.",
        "publication_date": "2024-01-16T18:49:55Z",
        "upvotes": 19
    },
    "2401.07004": {
        "url": "https://arxiv.org/abs/2401.07004",
        "title": "Extending LLMs' Context Window with 100 Samples",
        "authors": [
            "Yikai Zhang",
            "Junlong Li",
            "Pengfei Liu"
        ],
        "abstract": "Large Language Models (LLMs) are known to have limited extrapolation ability\nbeyond their pre-trained context window, constraining their application in\ndownstream tasks with lengthy inputs. Recent studies have sought to extend\nLLMs' context window by modifying rotary position embedding (RoPE), a popular\nposition encoding method adopted by well-known LLMs such as LLaMA, PaLM, and\nGPT-NeoX. However, prior works like Position Interpolation (PI) and YaRN are\nresource-intensive and lack comparative experiments to assess their\napplicability. In this work, we identify the inherent need for LLMs' attention\nentropy (i.e. the information entropy of attention scores) to maintain\nstability and introduce a novel extension to RoPE which combines adjusting\nRoPE's base frequency and scaling the attention logits to help LLMs efficiently\nadapt to a larger context window. We validate the superiority of our method in\nboth fine-tuning performance and robustness across different context window\nsizes on various context-demanding tasks. Notably, our method extends the\ncontext window of LLaMA-2-7B-Chat to 16,384 with only 100 samples and 6\ntraining steps, showcasing extraordinary efficiency. Finally, we also explore\nhow data compositions and training curricula affect context window extension\nfor specific downstream tasks, suggesting fine-tuning LLMs with lengthy\nconversations as a good starting point. We release our code and SFT data at\nhttps://github.com/GAIR-NLP/Entropy-ABF.",
        "publication_date": "2024-01-13T07:57:01Z",
        "upvotes": 14
    },
    "2401.07781": {
        "url": "https://arxiv.org/abs/2401.07781",
        "title": "Towards A Better Metric for Text-to-Video Generation",
        "authors": [
            "Jay Zhangjie Wu",
            "Guian Fang",
            "Haoning Wu",
            "Xintao Wang",
            "Yixiao Ge",
            "Xiaodong Cun",
            "David Junhao Zhang",
            "Jia-Wei Liu",
            "Yuchao Gu",
            "Rui Zhao",
            "Weisi Lin",
            "Wynne Hsu",
            "Ying Shan",
            "Mike Zheng Shou"
        ],
        "abstract": "Generative models have demonstrated remarkable capability in synthesizing\nhigh-quality text, images, and videos. For video generation, contemporary\ntext-to-video models exhibit impressive capabilities, crafting visually\nstunning videos. Nonetheless, evaluating such videos poses significant\nchallenges. Current research predominantly employs automated metrics such as\nFVD, IS, and CLIP Score. However, these metrics provide an incomplete analysis,\nparticularly in the temporal assessment of video content, thus rendering them\nunreliable indicators of true video quality. Furthermore, while user studies\nhave the potential to reflect human perception accurately, they are hampered by\ntheir time-intensive and laborious nature, with outcomes that are often tainted\nby subjective bias. In this paper, we investigate the limitations inherent in\nexisting metrics and introduce a novel evaluation pipeline, the Text-to-Video\nScore (T2VScore). This metric integrates two pivotal criteria: (1) Text-Video\nAlignment, which scrutinizes the fidelity of the video in representing the\ngiven text description, and (2) Video Quality, which evaluates the video's\noverall production caliber with a mixture of experts. Moreover, to evaluate the\nproposed metrics and facilitate future improvements on them, we present the\nTVGE dataset, collecting human judgements of 2,543 text-to-video generated\nvideos on the two criteria. Experiments on the TVGE dataset demonstrate the\nsuperiority of the proposed T2VScore on offering a better metric for\ntext-to-video generation.",
        "publication_date": "2024-01-15T15:42:39Z",
        "upvotes": 13
    },
    "2401.07049": {
        "url": "https://arxiv.org/abs/2401.07049",
        "title": "Quantum Denoising Diffusion Models",
        "authors": [
            "Michael K\u00f6lle",
            "Gerhard Stenzel",
            "Jonas Stein",
            "Sebastian Zielinski",
            "Bj\u00f6rn Ommer",
            "Claudia Linnhoff-Popien"
        ],
        "abstract": "In recent years, machine learning models like DALL-E, Craiyon, and Stable\nDiffusion have gained significant attention for their ability to generate\nhigh-resolution images from concise descriptions. Concurrently, quantum\ncomputing is showing promising advances, especially with quantum machine\nlearning which capitalizes on quantum mechanics to meet the increasing\ncomputational requirements of traditional machine learning algorithms. This\npaper explores the integration of quantum machine learning and variational\nquantum circuits to augment the efficacy of diffusion-based image generation\nmodels. Specifically, we address two challenges of classical diffusion models:\ntheir low sampling speed and the extensive parameter requirements. We introduce\ntwo quantum diffusion models and benchmark their capabilities against their\nclassical counterparts using MNIST digits, Fashion MNIST, and CIFAR-10. Our\nmodels surpass the classical models with similar parameter counts in terms of\nperformance metrics FID, SSIM, and PSNR. Moreover, we introduce a consistency\nmodel unitary single sampling architecture that combines the diffusion\nprocedure into a single step, enabling a fast one-step image generation.",
        "publication_date": "2024-01-13T11:38:08Z",
        "upvotes": 12
    },
    "2401.07727": {
        "url": "https://arxiv.org/abs/2401.07727",
        "title": "HexaGen3D: StableDiffusion is just one step away from Fast and Diverse\n  Text-to-3D Generation",
        "authors": [
            "Antoine Mercier",
            "Ramin Nakhli",
            "Mahesh Reddy",
            "Rajeev Yasarla",
            "Hong Cai",
            "Fatih Porikli",
            "Guillaume Berger"
        ],
        "abstract": "Despite the latest remarkable advances in generative modeling, efficient\ngeneration of high-quality 3D assets from textual prompts remains a difficult\ntask. A key challenge lies in data scarcity: the most extensive 3D datasets\nencompass merely millions of assets, while their 2D counterparts contain\nbillions of text-image pairs. To address this, we propose a novel approach\nwhich harnesses the power of large, pretrained 2D diffusion models. More\nspecifically, our approach, HexaGen3D, fine-tunes a pretrained text-to-image\nmodel to jointly predict 6 orthographic projections and the corresponding\nlatent triplane. We then decode these latents to generate a textured mesh.\nHexaGen3D does not require per-sample optimization, and can infer high-quality\nand diverse objects from textual prompts in 7 seconds, offering significantly\nbetter quality-to-latency trade-offs when comparing to existing approaches.\nFurthermore, HexaGen3D demonstrates strong generalization to new objects or\ncompositions.",
        "publication_date": "2024-01-15T14:41:15Z",
        "upvotes": 8
    },
    "2401.09417": {
        "url": "https://arxiv.org/abs/2401.09417",
        "title": "Vision Mamba: Efficient Visual Representation Learning with\n  Bidirectional State Space Model",
        "authors": [
            "Lianghui Zhu",
            "Bencheng Liao",
            "Qian Zhang",
            "Xinlong Wang",
            "Wenyu Liu",
            "Xinggang Wang"
        ],
        "abstract": "Recently the state space models (SSMs) with efficient hardware-aware designs,\ni.e., the Mamba deep learning model, have shown great potential for long\nsequence modeling. Meanwhile building efficient and generic vision backbones\npurely upon SSMs is an appealing direction. However, representing visual data\nis challenging for SSMs due to the position-sensitivity of visual data and the\nrequirement of global context for visual understanding. In this paper, we show\nthat the reliance on self-attention for visual representation learning is not\nnecessary and propose a new generic vision backbone with bidirectional Mamba\nblocks (Vim), which marks the image sequences with position embeddings and\ncompresses the visual representation with bidirectional state space models. On\nImageNet classification, COCO object detection, and ADE20k semantic\nsegmentation tasks, Vim achieves higher performance compared to\nwell-established vision transformers like DeiT, while also demonstrating\nsignificantly improved computation & memory efficiency. For example, Vim is\n2.8$\\times$ faster than DeiT and saves 86.8% GPU memory when performing batch\ninference to extract features on images with a resolution of 1248$\\times$1248.\nThe results demonstrate that Vim is capable of overcoming the computation &\nmemory constraints on performing Transformer-style understanding for\nhigh-resolution images and it has great potential to be the next-generation\nbackbone for vision foundation models. Code is available at\nhttps://github.com/hustvl/Vim.",
        "publication_date": "2024-01-17T18:56:18Z",
        "upvotes": 49
    },
    "2401.08967": {
        "url": "https://arxiv.org/abs/2401.08967",
        "title": "ReFT: Reasoning with Reinforced Fine-Tuning",
        "authors": [
            "Trung Quoc Luong",
            "Xinbo Zhang",
            "Zhanming Jie",
            "Peng Sun",
            "Xiaoran Jin",
            "Hang Li"
        ],
        "abstract": "One way to enhance the reasoning capability of Large Language Models (LLMs)\nis to conduct Supervised Fine-Tuning (SFT) using Chain-of-Thought (CoT)\nannotations. This approach does not show sufficiently strong generalization\nability, however, because the training only relies on the given CoT data. In\nmath problem-solving, for example, there is usually only one annotated\nreasoning path for each question in the training data. Intuitively, it would be\nbetter for the algorithm to learn from multiple annotated reasoning paths given\na question. To address this issue, we propose a simple yet effective approach\ncalled Reinforced Fine-Tuning (ReFT) to enhance the generalizability of\nlearning LLMs for reasoning, with math problem-solving as an example. ReFT\nfirst warmups the model with SFT, and then employs on-line reinforcement\nlearning, specifically the PPO algorithm in this paper, to further fine-tune\nthe model, where an abundance of reasoning paths are automatically sampled\ngiven the question and the rewards are naturally derived from the ground-truth\nanswers. Extensive experiments on GSM8K, MathQA, and SVAMP datasets show that\nReFT significantly outperforms SFT, and the performance can be potentially\nfurther boosted by combining inference-time strategies such as majority voting\nand re-ranking. Note that ReFT obtains the improvement by learning from the\nsame training questions as SFT, without relying on extra or augmented training\nquestions. This indicates a superior generalization ability for ReFT.",
        "publication_date": "2024-01-17T04:43:21Z",
        "upvotes": 26
    },
    "2401.09340": {
        "url": "https://arxiv.org/abs/2401.09340",
        "title": "SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene\n  Understanding",
        "authors": [
            "Baoxiong Jia",
            "Yixin Chen",
            "Huangyue Yu",
            "Yan Wang",
            "Xuesong Niu",
            "Tengyu Liu",
            "Qing Li",
            "Siyuan Huang"
        ],
        "abstract": "3D vision-language grounding, which focuses on aligning language with the 3D\nphysical environment, stands as a cornerstone in the development of embodied\nagents. In comparison to recent advancements in the 2D domain, grounding\nlanguage in 3D scenes faces several significant challenges: (i) the inherent\ncomplexity of 3D scenes due to the diverse object configurations, their rich\nattributes, and intricate relationships; (ii) the scarcity of paired 3D\nvision-language data to support grounded learning; and (iii) the absence of a\nunified learning framework to distill knowledge from grounded 3D data. In this\nwork, we aim to address these three major challenges in 3D vision-language by\nexamining the potential of systematically upscaling 3D vision-language learning\nin indoor environments. We introduce the first million-scale 3D vision-language\ndataset, SceneVerse, encompassing about 68K 3D indoor scenes and comprising\n2.5M vision-language pairs derived from both human annotations and our scalable\nscene-graph-based generation approach. We demonstrate that this scaling allows\nfor a unified pre-training framework, Grounded Pre-training for Scenes (GPS),\nfor 3D vision-language learning. Through extensive experiments, we showcase the\neffectiveness of GPS by achieving state-of-the-art performance on all existing\n3D visual grounding benchmarks. The vast potential of SceneVerse and GPS is\nunveiled through zero-shot transfer experiments in the challenging 3D\nvision-language tasks. Project website: https://scene-verse.github.io.",
        "publication_date": "2024-01-17T17:04:35Z",
        "upvotes": 17
    },
    "2401.09419": {
        "url": "https://arxiv.org/abs/2401.09419",
        "title": "GARField: Group Anything with Radiance Fields",
        "authors": [
            "Chung Min Kim",
            "Mingxuan Wu",
            "Justin Kerr",
            "Ken Goldberg",
            "Matthew Tancik",
            "Angjoo Kanazawa"
        ],
        "abstract": "Grouping is inherently ambiguous due to the multiple levels of granularity in\nwhich one can decompose a scene -- should the wheels of an excavator be\nconsidered separate or part of the whole? We present Group Anything with\nRadiance Fields (GARField), an approach for decomposing 3D scenes into a\nhierarchy of semantically meaningful groups from posed image inputs. To do this\nwe embrace group ambiguity through physical scale: by optimizing a\nscale-conditioned 3D affinity feature field, a point in the world can belong to\ndifferent groups of different sizes. We optimize this field from a set of 2D\nmasks provided by Segment Anything (SAM) in a way that respects coarse-to-fine\nhierarchy, using scale to consistently fuse conflicting masks from different\nviewpoints. From this field we can derive a hierarchy of possible groupings via\nautomatic tree construction or user interaction. We evaluate GARField on a\nvariety of in-the-wild scenes and find it effectively extracts groups at many\nlevels: clusters of objects, objects, and various subparts. GARField inherently\nrepresents multi-view consistent groupings and produces higher fidelity groups\nthan the input SAM masks. GARField's hierarchical grouping could have exciting\ndownstream applications such as 3D asset extraction or dynamic scene\nunderstanding. See the project website at https://www.garfield.studio/",
        "publication_date": "2024-01-17T18:57:53Z",
        "upvotes": 16
    },
    "2401.09084": {
        "url": "https://arxiv.org/abs/2401.09084",
        "title": "UniVG: Towards UNIfied-modal Video Generation",
        "authors": [
            "Ludan Ruan",
            "Lei Tian",
            "Chuanwei Huang",
            "Xu Zhang",
            "Xinyan Xiao"
        ],
        "abstract": "Diffusion based video generation has received extensive attention and\nachieved considerable success within both the academic and industrial\ncommunities. However, current efforts are mainly concentrated on\nsingle-objective or single-task video generation, such as generation driven by\ntext, by image, or by a combination of text and image. This cannot fully meet\nthe needs of real-world application scenarios, as users are likely to input\nimages and text conditions in a flexible manner, either individually or in\ncombination. To address this, we propose a Unified-modal Video Genearation\nsystem that is capable of handling multiple video generation tasks across text\nand image modalities. To this end, we revisit the various video generation\ntasks within our system from the perspective of generative freedom, and\nclassify them into high-freedom and low-freedom video generation categories.\nFor high-freedom video generation, we employ Multi-condition Cross Attention to\ngenerate videos that align with the semantics of the input images or text. For\nlow-freedom video generation, we introduce Biased Gaussian Noise to replace the\npure random Gaussian Noise, which helps to better preserve the content of the\ninput conditions. Our method achieves the lowest Fr\\'echet Video Distance (FVD)\non the public academic benchmark MSR-VTT, surpasses the current open-source\nmethods in human evaluations, and is on par with the current close-source\nmethod Gen2. For more samples, visit https://univg-baidu.github.io.",
        "publication_date": "2024-01-17T09:46:13Z",
        "upvotes": 15
    },
    "2401.09047": {
        "url": "https://arxiv.org/abs/2401.09047",
        "title": "VideoCrafter2: Overcoming Data Limitations for High-Quality Video\n  Diffusion Models",
        "authors": [
            "Haoxin Chen",
            "Yong Zhang",
            "Xiaodong Cun",
            "Menghan Xia",
            "Xintao Wang",
            "Chao Weng",
            "Ying Shan"
        ],
        "abstract": "Text-to-video generation aims to produce a video based on a given prompt.\nRecently, several commercial video models have been able to generate plausible\nvideos with minimal noise, excellent details, and high aesthetic scores.\nHowever, these models rely on large-scale, well-filtered, high-quality videos\nthat are not accessible to the community. Many existing research works, which\ntrain models using the low-quality WebVid-10M dataset, struggle to generate\nhigh-quality videos because the models are optimized to fit WebVid-10M. In this\nwork, we explore the training scheme of video models extended from Stable\nDiffusion and investigate the feasibility of leveraging low-quality videos and\nsynthesized high-quality images to obtain a high-quality video model. We first\nanalyze the connection between the spatial and temporal modules of video models\nand the distribution shift to low-quality videos. We observe that full training\nof all modules results in a stronger coupling between spatial and temporal\nmodules than only training temporal modules. Based on this stronger coupling,\nwe shift the distribution to higher quality without motion degradation by\nfinetuning spatial modules with high-quality images, resulting in a generic\nhigh-quality video model. Evaluations are conducted to demonstrate the\nsuperiority of the proposed method, particularly in picture quality, motion,\nand concept composition.",
        "publication_date": "2024-01-17T08:30:32Z",
        "upvotes": 13
    },
    "2401.08671": {
        "url": "https://arxiv.org/abs/2401.08671",
        "title": "DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and\n  DeepSpeed-Inference",
        "authors": [
            "Connor Holmes",
            "Masahiro Tanaka",
            "Michael Wyatt",
            "Ammar Ahmad Awan",
            "Jeff Rasley",
            "Samyam Rajbhandari",
            "Reza Yazdani Aminabadi",
            "Heyang Qin",
            "Arash Bakhtiari",
            "Lev Kurilenko",
            "Yuxiong He"
        ],
        "abstract": "The deployment and scaling of large language models (LLMs) have become\ncritical as they permeate various applications, demanding high-throughput and\nlow-latency serving systems. Existing frameworks struggle to balance these\nrequirements, especially for workloads with long prompts. This paper introduces\nDeepSpeed-FastGen, a system that employs Dynamic SplitFuse, a novel prompt and\ngeneration composition strategy, to deliver up to 2.3x higher effective\nthroughput, 2x lower latency on average, and up to 3.7x lower (token-level)\ntail latency, compared to state-of-the-art systems like vLLM. We leverage a\nsynergistic combination of DeepSpeed-MII and DeepSpeed-Inference to provide an\nefficient and easy-to-use serving system for LLMs. DeepSpeed-FastGen's advanced\nimplementation supports a range of models and offers both non-persistent and\npersistent deployment options, catering to diverse user scenarios from\ninteractive sessions to long-running applications. We present a detailed\nbenchmarking methodology, analyze the performance through latency-throughput\ncurves, and investigate scalability via load balancing. Our evaluations\ndemonstrate substantial improvements in throughput and latency across various\nmodels and hardware configurations. We discuss our roadmap for future\nenhancements, including broader model support and new hardware backends. The\nDeepSpeed-FastGen code is readily available for community engagement and\ncontribution.",
        "publication_date": "2024-01-09T06:49:40Z",
        "upvotes": 12
    },
    "2401.08740": {
        "url": "https://arxiv.org/abs/2401.08740",
        "title": "SiT: Exploring Flow and Diffusion-based Generative Models with Scalable\n  Interpolant Transformers",
        "authors": [
            "Nanye Ma",
            "Mark Goldstein",
            "Michael S. Albergo",
            "Nicholas M. Boffi",
            "Eric Vanden-Eijnden",
            "Saining Xie"
        ],
        "abstract": "We present Scalable Interpolant Transformers (SiT), a family of generative\nmodels built on the backbone of Diffusion Transformers (DiT). The interpolant\nframework, which allows for connecting two distributions in a more flexible way\nthan standard diffusion models, makes possible a modular study of various\ndesign choices impacting generative models built on dynamical transport: using\ndiscrete vs. continuous time learning, deciding the objective for the model to\nlearn, choosing the interpolant connecting the distributions, and deploying a\ndeterministic or stochastic sampler. By carefully introducing the above\ningredients, SiT surpasses DiT uniformly across model sizes on the conditional\nImageNet 256x256 benchmark using the exact same backbone, number of parameters,\nand GFLOPs. By exploring various diffusion coefficients, which can be tuned\nseparately from learning, SiT achieves an FID-50K score of 2.06.",
        "publication_date": "2024-01-16T18:55:25Z",
        "upvotes": 10
    },
    "2401.09135": {
        "url": "https://arxiv.org/abs/2401.09135",
        "title": "Asynchronous Local-SGD Training for Language Modeling",
        "authors": [
            "Bo Liu",
            "Rachita Chhaparia",
            "Arthur Douillard",
            "Satyen Kale",
            "Andrei A. Rusu",
            "Jiajun Shen",
            "Arthur Szlam",
            "Marc'Aurelio Ranzato"
        ],
        "abstract": "Local stochastic gradient descent (Local-SGD), also referred to as federated\naveraging, is an approach to distributed optimization where each device\nperforms more than one SGD update per communication. This work presents an\nempirical study of {\\it asynchronous} Local-SGD for training language models;\nthat is, each worker updates the global parameters as soon as it has finished\nits SGD steps. We conduct a comprehensive investigation by examining how worker\nhardware heterogeneity, model size, number of workers, and optimizer could\nimpact the learning performance. We find that with naive implementations,\nasynchronous Local-SGD takes more iterations to converge than its synchronous\ncounterpart despite updating the (global) model parameters more frequently. We\nidentify momentum acceleration on the global parameters when worker gradients\nare stale as a key challenge. We propose a novel method that utilizes a delayed\nNesterov momentum update and adjusts the workers' local training steps based on\ntheir computation speed. This approach, evaluated with models up to 150M\nparameters on the C4 dataset, matches the performance of synchronous Local-SGD\nin terms of perplexity per update step, and significantly surpasses it in terms\nof wall clock time.",
        "publication_date": "2024-01-17T11:17:04Z",
        "upvotes": 9
    },
    "2401.09416": {
        "url": "https://arxiv.org/abs/2401.09416",
        "title": "TextureDreamer: Image-guided Texture Synthesis through Geometry-aware\n  Diffusion",
        "authors": [
            "Yu-Ying Yeh",
            "Jia-Bin Huang",
            "Changil Kim",
            "Lei Xiao",
            "Thu Nguyen-Phuoc",
            "Numair Khan",
            "Cheng Zhang",
            "Manmohan Chandraker",
            "Carl S Marshall",
            "Zhao Dong",
            "Zhengqin Li"
        ],
        "abstract": "We present TextureDreamer, a novel image-guided texture synthesis method to\ntransfer relightable textures from a small number of input images (3 to 5) to\ntarget 3D shapes across arbitrary categories. Texture creation is a pivotal\nchallenge in vision and graphics. Industrial companies hire experienced artists\nto manually craft textures for 3D assets. Classical methods require densely\nsampled views and accurately aligned geometry, while learning-based methods are\nconfined to category-specific shapes within the dataset. In contrast,\nTextureDreamer can transfer highly detailed, intricate textures from real-world\nenvironments to arbitrary objects with only a few casually captured images,\npotentially significantly democratizing texture creation. Our core idea,\npersonalized geometry-aware score distillation (PGSD), draws inspiration from\nrecent advancements in diffuse models, including personalized modeling for\ntexture information extraction, variational score distillation for detailed\nappearance synthesis, and explicit geometry guidance with ControlNet. Our\nintegration and several essential modifications substantially improve the\ntexture quality. Experiments on real images spanning different categories show\nthat TextureDreamer can successfully transfer highly realistic, semantic\nmeaningful texture to arbitrary objects, surpassing the visual quality of\nprevious state-of-the-art.",
        "publication_date": "2024-01-17T18:55:49Z",
        "upvotes": 8
    },
    "2401.09048": {
        "url": "https://arxiv.org/abs/2401.09048",
        "title": "Compose and Conquer: Diffusion-Based 3D Depth Aware Composable Image\n  Synthesis",
        "authors": [
            "Jonghyun Lee",
            "Hansam Cho",
            "Youngjoon Yoo",
            "Seoung Bum Kim",
            "Yonghyun Jeong"
        ],
        "abstract": "Addressing the limitations of text as a source of accurate layout\nrepresentation in text-conditional diffusion models, many works incorporate\nadditional signals to condition certain attributes within a generated image.\nAlthough successful, previous works do not account for the specific\nlocalization of said attributes extended into the three dimensional plane. In\nthis context, we present a conditional diffusion model that integrates control\nover three-dimensional object placement with disentangled representations of\nglobal stylistic semantics from multiple exemplar images. Specifically, we\nfirst introduce \\textit{depth disentanglement training} to leverage the\nrelative depth of objects as an estimator, allowing the model to identify the\nabsolute positions of unseen objects through the use of synthetic image\ntriplets. We also introduce \\textit{soft guidance}, a method for imposing\nglobal semantics onto targeted regions without the use of any additional\nlocalization cues. Our integrated framework, \\textsc{Compose and Conquer\n(CnC)}, unifies these techniques to localize multiple conditions in a\ndisentangled manner. We demonstrate that our approach allows perception of\nobjects at varying depths while offering a versatile framework for composing\nlocalized objects with different global semantics. Code:\nhttps://github.com/tomtom1103/compose-and-conquer/",
        "publication_date": "2024-01-17T08:30:47Z",
        "upvotes": 7
    },
    "2401.08937": {
        "url": "https://arxiv.org/abs/2401.08937",
        "title": "ICON: Incremental CONfidence for Joint Pose and Radiance Field\n  Optimization",
        "authors": [
            "Weiyao Wang",
            "Pierre Gleize",
            "Hao Tang",
            "Xingyu Chen",
            "Kevin J Liang",
            "Matt Feiszli"
        ],
        "abstract": "Neural Radiance Fields (NeRF) exhibit remarkable performance for Novel View\nSynthesis (NVS) given a set of 2D images. However, NeRF training requires\naccurate camera pose for each input view, typically obtained by\nStructure-from-Motion (SfM) pipelines. Recent works have attempted to relax\nthis constraint, but they still often rely on decent initial poses which they\ncan refine. Here we aim at removing the requirement for pose initialization. We\npresent Incremental CONfidence (ICON), an optimization procedure for training\nNeRFs from 2D video frames. ICON only assumes smooth camera motion to estimate\ninitial guess for poses. Further, ICON introduces ``confidence\": an adaptive\nmeasure of model quality used to dynamically reweight gradients. ICON relies on\nhigh-confidence poses to learn NeRF, and high-confidence 3D structure (as\nencoded by NeRF) to learn poses. We show that ICON, without prior pose\ninitialization, achieves superior performance in both CO3D and HO3D versus\nmethods which use SfM pose.",
        "publication_date": "2024-01-17T03:18:02Z",
        "upvotes": 5
    },
    "2401.10020": {
        "url": "https://arxiv.org/abs/2401.10020",
        "title": "Self-Rewarding Language Models",
        "authors": [
            "Weizhe Yuan",
            "Richard Yuanzhe Pang",
            "Kyunghyun Cho",
            "Xian Li",
            "Sainbayar Sukhbaatar",
            "Jing Xu",
            "Jason Weston"
        ],
        "abstract": "We posit that to achieve superhuman agents, future models require superhuman\nfeedback in order to provide an adequate training signal. Current approaches\ncommonly train reward models from human preferences, which may then be\nbottlenecked by human performance level, and secondly these separate frozen\nreward models cannot then learn to improve during LLM training. In this work,\nwe study Self-Rewarding Language Models, where the language model itself is\nused via LLM-as-a-Judge prompting to provide its own rewards during training.\nWe show that during Iterative DPO training that not only does instruction\nfollowing ability improve, but also the ability to provide high-quality rewards\nto itself. Fine-tuning Llama 2 70B on three iterations of our approach yields a\nmodel that outperforms many existing systems on the AlpacaEval 2.0 leaderboard,\nincluding Claude 2, Gemini Pro, and GPT-4 0613. While there is much left still\nto explore, this work opens the door to the possibility of models that can\ncontinually improve in both axes.",
        "publication_date": "2024-01-18T14:43:47Z",
        "upvotes": 134
    },
    "2401.10166": {
        "url": "https://arxiv.org/abs/2401.10166",
        "title": "VMamba: Visual State Space Model",
        "authors": [
            "Yue Liu",
            "Yunjie Tian",
            "Yuzhong Zhao",
            "Hongtian Yu",
            "Lingxi Xie",
            "Yaowei Wang",
            "Qixiang Ye",
            "Yunfan Liu"
        ],
        "abstract": "Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) stand as\nthe two most popular foundation models for visual representation learning.\nWhile CNNs exhibit remarkable scalability with linear complexity w.r.t. image\nresolution, ViTs surpass them in fitting capabilities despite contending with\nquadratic complexity. A closer inspection reveals that ViTs achieve superior\nvisual modeling performance through the incorporation of global receptive\nfields and dynamic weights. This observation motivates us to propose a novel\narchitecture that inherits these components while enhancing computational\nefficiency. To this end, we draw inspiration from the recently introduced state\nspace model and propose the Visual State Space Model (VMamba), which achieves\nlinear complexity without sacrificing global receptive fields. To address the\nencountered direction-sensitive issue, we introduce the Cross-Scan Module (CSM)\nto traverse the spatial domain and convert any non-causal visual image into\norder patch sequences. Extensive experimental results substantiate that VMamba\nnot only demonstrates promising capabilities across various visual perception\ntasks, but also exhibits more pronounced advantages over established benchmarks\nas the image resolution increases. Source code has been available at\nhttps://github.com/MzeroMiko/VMamba.",
        "publication_date": "2024-01-18T17:55:39Z",
        "upvotes": 35
    },
    "2401.10061": {
        "url": "https://arxiv.org/abs/2401.10061",
        "title": "DiffusionGPT: LLM-Driven Text-to-Image Generation System",
        "authors": [
            "Jie Qin",
            "Jie Wu",
            "Weifeng Chen",
            "Yuxi Ren",
            "Huixia Li",
            "Hefeng Wu",
            "Xuefeng Xiao",
            "Rui Wang",
            "Shilei Wen"
        ],
        "abstract": "Diffusion models have opened up new avenues for the field of image\ngeneration, resulting in the proliferation of high-quality models shared on\nopen-source platforms. However, a major challenge persists in current\ntext-to-image systems are often unable to handle diverse inputs, or are limited\nto single model results. Current unified attempts often fall into two\northogonal aspects: i) parse Diverse Prompts in input stage; ii) activate\nexpert model to output. To combine the best of both worlds, we propose\nDiffusionGPT, which leverages Large Language Models (LLM) to offer a unified\ngeneration system capable of seamlessly accommodating various types of prompts\nand integrating domain-expert models. DiffusionGPT constructs domain-specific\nTrees for various generative models based on prior knowledge. When provided\nwith an input, the LLM parses the prompt and employs the Trees-of-Thought to\nguide the selection of an appropriate model, thereby relaxing input constraints\nand ensuring exceptional performance across diverse domains. Moreover, we\nintroduce Advantage Databases, where the Tree-of-Thought is enriched with human\nfeedback, aligning the model selection process with human preferences. Through\nextensive experiments and comparisons, we demonstrate the effectiveness of\nDiffusionGPT, showcasing its potential for pushing the boundaries of image\nsynthesis in diverse domains.",
        "publication_date": "2024-01-18T15:30:58Z",
        "upvotes": 26
    },
    "2401.10225": {
        "url": "https://arxiv.org/abs/2401.10225",
        "title": "ChatQA: Building GPT-4 Level Conversational QA Models",
        "authors": [
            "Zihan Liu",
            "Wei Ping",
            "Rajarshi Roy",
            "Peng Xu",
            "Chankyu Lee",
            "Mohammad Shoeybi",
            "Bryan Catanzaro"
        ],
        "abstract": "In this work, we introduce ChatQA, a family of conversational question\nanswering (QA) models that obtain GPT-4 level accuracies. Specifically, we\npropose a two-stage instruction tuning method that can significantly improve\nthe zero-shot conversational QA results from large language models (LLMs). To\nhandle retrieval-augmented generation in conversational QA, we fine-tune a\ndense retriever on a multi-turn QA dataset, which provides comparable results\nto using the state-of-the-art query rewriting model while largely reducing\ndeployment cost. Notably, our ChatQA-70B can outperform GPT-4 in terms of\naverage score on 10 conversational QA datasets (54.14 vs. 53.90), without\nrelying on any synthetic data from OpenAI GPT models.",
        "publication_date": "2024-01-18T18:59:11Z",
        "upvotes": 26
    },
    "2401.09603": {
        "url": "https://arxiv.org/abs/2401.09603",
        "title": "Rethinking FID: Towards a Better Evaluation Metric for Image Generation",
        "authors": [
            "Sadeep Jayasumana",
            "Srikumar Ramalingam",
            "Andreas Veit",
            "Daniel Glasner",
            "Ayan Chakrabarti",
            "Sanjiv Kumar"
        ],
        "abstract": "As with many machine learning problems, the progress of image generation\nmethods hinges on good evaluation metrics. One of the most popular is the\nFrechet Inception Distance (FID). FID estimates the distance between a\ndistribution of Inception-v3 features of real images, and those of images\ngenerated by the algorithm. We highlight important drawbacks of FID:\nInception's poor representation of the rich and varied content generated by\nmodern text-to-image models, incorrect normality assumptions, and poor sample\ncomplexity. We call for a reevaluation of FID's use as the primary quality\nmetric for generated images. We empirically demonstrate that FID contradicts\nhuman raters, it does not reflect gradual improvement of iterative\ntext-to-image models, it does not capture distortion levels, and that it\nproduces inconsistent results when varying the sample size. We also propose an\nalternative new metric, CMMD, based on richer CLIP embeddings and the maximum\nmean discrepancy distance with the Gaussian RBF kernel. It is an unbiased\nestimator that does not make any assumptions on the probability distribution of\nthe embeddings and is sample efficient. Through extensive experiments and\nanalysis, we demonstrate that FID-based evaluations of text-to-image models may\nbe unreliable, and that CMMD offers a more robust and reliable assessment of\nimage quality.",
        "publication_date": "2023-11-30T19:11:01Z",
        "upvotes": 13
    },
    "2401.09865": {
        "url": "https://arxiv.org/abs/2401.09865",
        "title": "Improving fine-grained understanding in image-text pre-training",
        "authors": [
            "Ioana Bica",
            "Anastasija Ili\u0107",
            "Matthias Bauer",
            "Goker Erdogan",
            "Matko Bo\u0161njak",
            "Christos Kaplanis",
            "Alexey A. Gritsenko",
            "Matthias Minderer",
            "Charles Blundell",
            "Razvan Pascanu",
            "Jovana Mitrovi\u0107"
        ],
        "abstract": "We introduce SPARse Fine-grained Contrastive Alignment (SPARC), a simple\nmethod for pretraining more fine-grained multimodal representations from\nimage-text pairs. Given that multiple image patches often correspond to single\nwords, we propose to learn a grouping of image patches for every token in the\ncaption. To achieve this, we use a sparse similarity metric between image\npatches and language tokens and compute for each token a language-grouped\nvision embedding as the weighted average of patches. The token and\nlanguage-grouped vision embeddings are then contrasted through a fine-grained\nsequence-wise loss that only depends on individual samples and does not require\nother batch samples as negatives. This enables more detailed information to be\nlearned in a computationally inexpensive manner. SPARC combines this\nfine-grained loss with a contrastive loss between global image and text\nembeddings to learn representations that simultaneously encode global and local\ninformation. We thoroughly evaluate our proposed method and show improved\nperformance over competing approaches both on image-level tasks relying on\ncoarse-grained information, e.g. classification, as well as region-level tasks\nrelying on fine-grained information, e.g. retrieval, object detection, and\nsegmentation. Moreover, SPARC improves model faithfulness and captioning in\nfoundational vision-language models.",
        "publication_date": "2024-01-18T10:28:45Z",
        "upvotes": 12
    },
    "2401.09985": {
        "url": "https://arxiv.org/abs/2401.09985",
        "title": "WorldDreamer: Towards General World Models for Video Generation via\n  Predicting Masked Tokens",
        "authors": [
            "Xiaofeng Wang",
            "Zheng Zhu",
            "Guan Huang",
            "Boyuan Wang",
            "Xinze Chen",
            "Jiwen Lu"
        ],
        "abstract": "World models play a crucial role in understanding and predicting the dynamics\nof the world, which is essential for video generation. However, existing world\nmodels are confined to specific scenarios such as gaming or driving, limiting\ntheir ability to capture the complexity of general world dynamic environments.\nTherefore, we introduce WorldDreamer, a pioneering world model to foster a\ncomprehensive comprehension of general world physics and motions, which\nsignificantly enhances the capabilities of video generation. Drawing\ninspiration from the success of large language models, WorldDreamer frames\nworld modeling as an unsupervised visual sequence modeling challenge. This is\nachieved by mapping visual inputs to discrete tokens and predicting the masked\nones. During this process, we incorporate multi-modal prompts to facilitate\ninteraction within the world model. Our experiments show that WorldDreamer\nexcels in generating videos across different scenarios, including natural\nscenes and driving environments. WorldDreamer showcases versatility in\nexecuting tasks such as text-to-video conversion, image-tovideo synthesis, and\nvideo editing. These results underscore WorldDreamer's effectiveness in\ncapturing dynamic elements within diverse general world environments.",
        "publication_date": "2024-01-18T14:01:20Z",
        "upvotes": 12
    },
    "2401.10171": {
        "url": "https://arxiv.org/abs/2401.10171",
        "title": "SHINOBI: Shape and Illumination using Neural Object Decomposition via\n  BRDF Optimization In-the-wild",
        "authors": [
            "Andreas Engelhardt",
            "Amit Raj",
            "Mark Boss",
            "Yunzhi Zhang",
            "Abhishek Kar",
            "Yuanzhen Li",
            "Deqing Sun",
            "Ricardo Martin Brualla",
            "Jonathan T. Barron",
            "Hendrik P. A. Lensch",
            "Varun Jampani"
        ],
        "abstract": "We present SHINOBI, an end-to-end framework for the reconstruction of shape,\nmaterial, and illumination from object images captured with varying lighting,\npose, and background. Inverse rendering of an object based on unconstrained\nimage collections is a long-standing challenge in computer vision and graphics\nand requires a joint optimization over shape, radiance, and pose. We show that\nan implicit shape representation based on a multi-resolution hash encoding\nenables faster and robust shape reconstruction with joint camera alignment\noptimization that outperforms prior work. Further, to enable the editing of\nillumination and object reflectance (i.e. material) we jointly optimize BRDF\nand illumination together with the object's shape. Our method is class-agnostic\nand works on in-the-wild image collections of objects to produce relightable 3D\nassets for several use cases such as AR/VR, movies, games, etc. Project page:\nhttps://shinobi.aengelhardt.com Video:\nhttps://www.youtube.com/watch?v=iFENQ6AcYd8&feature=youtu.be",
        "publication_date": "2024-01-18T18:01:19Z",
        "upvotes": 11
    },
    "2401.10032": {
        "url": "https://arxiv.org/abs/2401.10032",
        "title": "FreGrad: Lightweight and Fast Frequency-aware Diffusion Vocoder",
        "authors": [
            "Tan Dat Nguyen",
            "Ji-Hoon Kim",
            "Youngjoon Jang",
            "Jaehun Kim",
            "Joon Son Chung"
        ],
        "abstract": "The goal of this paper is to generate realistic audio with a lightweight and\nfast diffusion-based vocoder, named FreGrad. Our framework consists of the\nfollowing three key components: (1) We employ discrete wavelet transform that\ndecomposes a complicated waveform into sub-band wavelets, which helps FreGrad\nto operate on a simple and concise feature space, (2) We design a\nfrequency-aware dilated convolution that elevates frequency awareness,\nresulting in generating speech with accurate frequency information, and (3) We\nintroduce a bag of tricks that boosts the generation quality of the proposed\nmodel. In our experiments, FreGrad achieves 3.7 times faster training time and\n2.2 times faster inference speed compared to our baseline while reducing the\nmodel size by 0.6 times (only 1.78M parameters) without sacrificing the output\nquality. Audio samples are available at:\nhttps://mm.kaist.ac.kr/projects/FreGrad.",
        "publication_date": "2024-01-18T14:57:25Z",
        "upvotes": 11
    },
    "2401.09962": {
        "url": "https://arxiv.org/abs/2401.09962",
        "title": "CustomVideo: Customizing Text-to-Video Generation with Multiple Subjects",
        "authors": [
            "Zhao Wang",
            "Aoxue Li",
            "Enze Xie",
            "Lingting Zhu",
            "Yong Guo",
            "Qi Dou",
            "Zhenguo Li"
        ],
        "abstract": "Customized text-to-video generation aims to generate high-quality videos\nguided by text prompts and subject references. Current approaches designed for\nsingle subjects suffer from tackling multiple subjects, which is a more\nchallenging and practical scenario. In this work, we aim to promote\nmulti-subject guided text-to-video customization. We propose CustomVideo, a\nnovel framework that can generate identity-preserving videos with the guidance\nof multiple subjects. To be specific, firstly, we encourage the co-occurrence\nof multiple subjects via composing them in a single image. Further, upon a\nbasic text-to-video diffusion model, we design a simple yet effective attention\ncontrol strategy to disentangle different subjects in the latent space of\ndiffusion model. Moreover, to help the model focus on the specific object area,\nwe segment the object from given reference images and provide a corresponding\nobject mask for attention learning. Also, we collect a multi-subject\ntext-to-video generation dataset as a comprehensive benchmark, with 69\nindividual subjects and 57 meaningful pairs. Extensive qualitative,\nquantitative, and user study results demonstrate the superiority of our method,\ncompared with the previous state-of-the-art approaches.",
        "publication_date": "2024-01-18T13:23:51Z",
        "upvotes": 6
    },
    "2401.10891": {
        "url": "https://arxiv.org/abs/2401.10891",
        "title": "Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data",
        "authors": [
            "Lihe Yang",
            "Bingyi Kang",
            "Zilong Huang",
            "Xiaogang Xu",
            "Jiashi Feng",
            "Hengshuang Zhao"
        ],
        "abstract": "This work presents Depth Anything, a highly practical solution for robust\nmonocular depth estimation. Without pursuing novel technical modules, we aim to\nbuild a simple yet powerful foundation model dealing with any images under any\ncircumstances. To this end, we scale up the dataset by designing a data engine\nto collect and automatically annotate large-scale unlabeled data (~62M), which\nsignificantly enlarges the data coverage and thus is able to reduce the\ngeneralization error. We investigate two simple yet effective strategies that\nmake data scaling-up promising. First, a more challenging optimization target\nis created by leveraging data augmentation tools. It compels the model to\nactively seek extra visual knowledge and acquire robust representations.\nSecond, an auxiliary supervision is developed to enforce the model to inherit\nrich semantic priors from pre-trained encoders. We evaluate its zero-shot\ncapabilities extensively, including six public datasets and randomly captured\nphotos. It demonstrates impressive generalization ability. Further, through\nfine-tuning it with metric depth information from NYUv2 and KITTI, new SOTAs\nare set. Our better depth model also results in a better depth-conditioned\nControlNet. Our models are released at\nhttps://github.com/LiheYoung/Depth-Anything.",
        "publication_date": "2024-01-19T18:59:52Z",
        "upvotes": 53
    },
    "2401.10774": {
        "url": "https://arxiv.org/abs/2401.10774",
        "title": "Medusa: Simple LLM Inference Acceleration Framework with Multiple\n  Decoding Heads",
        "authors": [
            "Tianle Cai",
            "Yuhong Li",
            "Zhengyang Geng",
            "Hongwu Peng",
            "Jason D. Lee",
            "Deming Chen",
            "Tri Dao"
        ],
        "abstract": "The inference process in Large Language Models (LLMs) is often limited due to\nthe absence of parallelism in the auto-regressive decoding process, resulting\nin most operations being restricted by the memory bandwidth of accelerators.\nWhile methods such as speculative decoding have been suggested to address this\nissue, their implementation is impeded by the challenges associated with\nacquiring and maintaining a separate draft model. In this paper, we present\nMedusa, an efficient method that augments LLM inference by adding extra\ndecoding heads to predict multiple subsequent tokens in parallel. Using a\ntree-based attention mechanism, Medusa constructs multiple candidate\ncontinuations and verifies them simultaneously in each decoding step. By\nleveraging parallel processing, Medusa introduces only minimal overhead in\nterms of single-step latency while substantially reducing the number of\ndecoding steps required.\n  We present two levels of fine-tuning procedures for Medusa to meet the needs\nof different use cases: Medusa-1: Medusa is directly fine-tuned on top of a\nfrozen backbone LLM, enabling lossless inference acceleration. Medusa-2: Medusa\nis fine-tuned together with the backbone LLM, enabling better prediction\naccuracy of Medusa heads and higher speedup but needing a special training\nrecipe that preserves the backbone model's capabilities.\n  Moreover, we propose several extensions that improve or expand the utility of\nMedusa, including a self-distillation to handle situations where no training\ndata is available and a typical acceptance scheme to boost the acceptance rate\nwhile maintaining generation quality. We evaluate Medusa on models of various\nsizes and training procedures. Our experiments demonstrate that Medusa-1 can\nachieve over 2.2x speedup without compromising generation quality, while\nMedusa-2 further improves the speedup to 2.3-3.6x.",
        "publication_date": "2024-01-19T15:48:40Z",
        "upvotes": 48
    },
    "2401.10241": {
        "url": "https://arxiv.org/abs/2401.10241",
        "title": "Zero Bubble Pipeline Parallelism",
        "authors": [
            "Penghui Qi",
            "Xinyi Wan",
            "Guangxing Huang",
            "Min Lin"
        ],
        "abstract": "Pipeline parallelism is one of the key components for large-scale distributed\ntraining, yet its efficiency suffers from pipeline bubbles which were deemed\ninevitable. In this work, we introduce a scheduling strategy that, to our\nknowledge, is the first to successfully achieve zero pipeline bubbles under\nsynchronous training semantics. The key idea behind this improvement is to\nsplit the backward computation into two parts, one that computes gradient for\nthe input and another that computes for the parameters. Based on this idea, we\nhandcraft novel pipeline schedules that significantly outperform the baseline\nmethods. We further develop an algorithm that automatically finds an optimal\nschedule based on specific model configuration and memory limit. Additionally,\nto truly achieve zero bubble, we introduce a novel technique to bypass\nsynchronizations during the optimizer step. Experimental evaluations show that\nour method outperforms the 1F1B schedule up to 23% in throughput under a\nsimilar memory limit. This number can be further pushed to 31% when the memory\nconstraint is relaxed. We believe our results mark a major step forward in\nharnessing the true potential of pipeline parallelism. We open sourced our\nimplementation based on the popular Megatron-LM repository on\nhttps://github.com/sail-sg/zero-bubble-pipeline-parallelism.",
        "publication_date": "2023-11-30T10:40:34Z",
        "upvotes": 19
    },
    "2401.10889": {
        "url": "https://arxiv.org/abs/2401.10889",
        "title": "Synthesizing Moving People with 3D Control",
        "authors": [
            "Boyi Li",
            "Jathushan Rajasegaran",
            "Yossi Gandelsman",
            "Alexei A. Efros",
            "Jitendra Malik"
        ],
        "abstract": "In this paper, we present a diffusion model-based framework for animating\npeople from a single image for a given target 3D motion sequence. Our approach\nhas two core components: a) learning priors about invisible parts of the human\nbody and clothing, and b) rendering novel body poses with proper clothing and\ntexture. For the first part, we learn an in-filling diffusion model to\nhallucinate unseen parts of a person given a single image. We train this model\non texture map space, which makes it more sample-efficient since it is\ninvariant to pose and viewpoint. Second, we develop a diffusion-based rendering\npipeline, which is controlled by 3D human poses. This produces realistic\nrenderings of novel poses of the person, including clothing, hair, and\nplausible in-filling of unseen regions. This disentangled approach allows our\nmethod to generate a sequence of images that are faithful to the target motion\nin the 3D pose and, to the input image in terms of visual similarity. In\naddition to that, the 3D control allows various synthetic camera trajectories\nto render a person. Our experiments show that our method is resilient in\ngenerating prolonged motions and varied challenging and complex poses compared\nto prior methods. Please check our website for more details:\nhttps://boyiliee.github.io/3DHM.github.io/.",
        "publication_date": "2024-01-19T18:59:11Z",
        "upvotes": 11
    },
    "2401.10822": {
        "url": "https://arxiv.org/abs/2401.10822",
        "title": "ActAnywhere: Subject-Aware Video Background Generation",
        "authors": [
            "Boxiao Pan",
            "Zhan Xu",
            "Chun-Hao Paul Huang",
            "Krishna Kumar Singh",
            "Yang Zhou",
            "Leonidas J. Guibas",
            "Jimei Yang"
        ],
        "abstract": "Generating video background that tailors to foreground subject motion is an\nimportant problem for the movie industry and visual effects community. This\ntask involves synthesizing background that aligns with the motion and\nappearance of the foreground subject, while also complies with the artist's\ncreative intention. We introduce ActAnywhere, a generative model that automates\nthis process which traditionally requires tedious manual efforts. Our model\nleverages the power of large-scale video diffusion models, and is specifically\ntailored for this task. ActAnywhere takes a sequence of foreground subject\nsegmentation as input and an image that describes the desired scene as\ncondition, to produce a coherent video with realistic foreground-background\ninteractions while adhering to the condition frame. We train our model on a\nlarge-scale dataset of human-scene interaction videos. Extensive evaluations\ndemonstrate the superior performance of our model, significantly outperforming\nbaselines. Moreover, we show that ActAnywhere generalizes to diverse\nout-of-distribution samples, including non-human subjects. Please visit our\nproject webpage at https://actanywhere.github.io.",
        "publication_date": "2024-01-19T17:16:16Z",
        "upvotes": 11
    },
    "2401.10838": {
        "url": "https://arxiv.org/abs/2401.10838",
        "title": "Rambler: Supporting Writing With Speech via LLM-Assisted Gist\n  Manipulation",
        "authors": [
            "Susan Lin",
            "Jeremy Warner",
            "J. D. Zamfirescu-Pereira",
            "Matthew G. Lee",
            "Sauhard Jain",
            "Michael Xuelin Huang",
            "Piyawat Lertvittayakumjorn",
            "Shanqing Cai",
            "Shumin Zhai",
            "Bj\u00f6rn Hartmann",
            "Can Liu"
        ],
        "abstract": "Dictation enables efficient text input on mobile devices. However, writing\nwith speech can produce disfluent, wordy, and incoherent text and thus requires\nheavy post-processing. This paper presents Rambler, an LLM-powered graphical\nuser interface that supports gist-level manipulation of dictated text with two\nmain sets of functions: gist extraction and macro revision. Gist extraction\ngenerates keywords and summaries as anchors to support the review and\ninteraction with spoken text. LLM-assisted macro revisions allow users to\nrespeak, split, merge and transform dictated text without specifying precise\nediting locations. Together they pave the way for interactive dictation and\nrevision that help close gaps between spontaneous spoken words and\nwell-structured writing. In a comparative study with 12 participants performing\nverbal composition tasks, Rambler outperformed the baseline of a speech-to-text\neditor + ChatGPT, as it better facilitates iterative revisions with enhanced\nuser control over the content while supporting surprisingly diverse user\nstrategies.",
        "publication_date": "2024-01-19T17:39:56Z",
        "upvotes": 8
    },
    "2401.10404": {
        "url": "https://arxiv.org/abs/2401.10404",
        "title": "Inflation with Diffusion: Efficient Temporal Adaptation for\n  Text-to-Video Super-Resolution",
        "authors": [
            "Xin Yuan",
            "Jinoo Baek",
            "Keyang Xu",
            "Omer Tov",
            "Hongliang Fei"
        ],
        "abstract": "We propose an efficient diffusion-based text-to-video super-resolution (SR)\ntuning approach that leverages the readily learned capacity of pixel level\nimage diffusion model to capture spatial information for video generation. To\naccomplish this goal, we design an efficient architecture by inflating the\nweightings of the text-to-image SR model into our video generation framework.\nAdditionally, we incorporate a temporal adapter to ensure temporal coherence\nacross video frames. We investigate different tuning approaches based on our\ninflated architecture and report trade-offs between computational costs and\nsuper-resolution quality. Empirical evaluation, both quantitative and\nqualitative, on the Shutterstock video dataset, demonstrates that our approach\nis able to perform text-to-video SR generation with good visual quality and\ntemporal consistency. To evaluate temporal coherence, we also present\nvisualizations in video format in\nhttps://drive.google.com/drive/folders/1YVc-KMSJqOrEUdQWVaI-Yfu8Vsfu_1aO?usp=sharing .",
        "publication_date": "2024-01-18T22:25:16Z",
        "upvotes": 8
    },
    "2401.10831": {
        "url": "https://arxiv.org/abs/2401.10831",
        "title": "Understanding Video Transformers via Universal Concept Discovery",
        "authors": [
            "Matthew Kowal",
            "Achal Dave",
            "Rares Ambrus",
            "Adrien Gaidon",
            "Konstantinos G. Derpanis",
            "Pavel Tokmakov"
        ],
        "abstract": "This paper studies the problem of concept-based interpretability of\ntransformer representations for videos. Concretely, we seek to explain the\ndecision-making process of video transformers based on high-level,\nspatiotemporal concepts that are automatically discovered. Prior research on\nconcept-based interpretability has concentrated solely on image-level tasks.\nComparatively, video models deal with the added temporal dimension, increasing\ncomplexity and posing challenges in identifying dynamic concepts over time. In\nthis work, we systematically address these challenges by introducing the first\nVideo Transformer Concept Discovery (VTCD) algorithm. To this end, we propose\nan efficient approach for unsupervised identification of units of video\ntransformer representations - concepts, and ranking their importance to the\noutput of a model. The resulting concepts are highly interpretable, revealing\nspatio-temporal reasoning mechanisms and object-centric representations in\nunstructured video models. Performing this analysis jointly over a diverse set\nof supervised and self-supervised representations, we discover that some of\nthese mechanism are universal in video transformers. Finally, we show that VTCD\ncan be used for fine-grained action recognition and video object segmentation.",
        "publication_date": "2024-01-19T17:27:21Z",
        "upvotes": 6
    },
    "2401.12070": {
        "url": "https://arxiv.org/abs/2401.12070",
        "title": "Spotting LLMs With Binoculars: Zero-Shot Detection of Machine-Generated\n  Text",
        "authors": [
            "Abhimanyu Hans",
            "Avi Schwarzschild",
            "Valeriia Cherepanova",
            "Hamid Kazemi",
            "Aniruddha Saha",
            "Micah Goldblum",
            "Jonas Geiping",
            "Tom Goldstein"
        ],
        "abstract": "Detecting text generated by modern large language models is thought to be\nhard, as both LLMs and humans can exhibit a wide range of complex behaviors.\nHowever, we find that a score based on contrasting two closely related language\nmodels is highly accurate at separating human-generated and machine-generated\ntext. Based on this mechanism, we propose a novel LLM detector that only\nrequires simple calculations using a pair of pre-trained LLMs. The method,\ncalled Binoculars, achieves state-of-the-art accuracy without any training\ndata. It is capable of spotting machine text from a range of modern LLMs\nwithout any model-specific modifications. We comprehensively evaluate\nBinoculars on a number of text sources and in varied situations. Over a wide\nrange of document types, Binoculars detects over 90% of generated samples from\nChatGPT (and other LLMs) at a false positive rate of 0.01%, despite not being\ntrained on any ChatGPT data.",
        "publication_date": "2024-01-22T16:09:47Z",
        "upvotes": 40
    },
    "2401.11708": {
        "url": "https://arxiv.org/abs/2401.11708",
        "title": "Mastering Text-to-Image Diffusion: Recaptioning, Planning, and\n  Generating with Multimodal LLMs",
        "authors": [
            "Ling Yang",
            "Zhaochen Yu",
            "Chenlin Meng",
            "Minkai Xu",
            "Stefano Ermon",
            "Bin Cui"
        ],
        "abstract": "Diffusion models have exhibit exceptional performance in text-to-image\ngeneration and editing. However, existing methods often face challenges when\nhandling complex text prompts that involve multiple objects with multiple\nattributes and relationships. In this paper, we propose a brand new\ntraining-free text-to-image generation/editing framework, namely Recaption,\nPlan and Generate (RPG), harnessing the powerful chain-of-thought reasoning\nability of multimodal LLMs to enhance the compositionality of text-to-image\ndiffusion models. Our approach employs the MLLM as a global planner to\ndecompose the process of generating complex images into multiple simpler\ngeneration tasks within subregions. We propose complementary regional diffusion\nto enable region-wise compositional generation. Furthermore, we integrate\ntext-guided image generation and editing within the proposed RPG in a\nclosed-loop fashion, thereby enhancing generalization ability. Extensive\nexperiments demonstrate our RPG outperforms state-of-the-art text-to-image\ndiffusion models, including DALL-E 3 and SDXL, particularly in multi-category\nobject composition and text-image semantic alignment. Notably, our RPG\nframework exhibits wide compatibility with various MLLM architectures (e.g.,\nMiniGPT-4) and diffusion backbones (e.g., ControlNet). Our code is available\nat: https://github.com/YangLing0818/RPG-DiffusionMaster",
        "publication_date": "2024-01-22T06:16:29Z",
        "upvotes": 26
    },
    "2401.11944": {
        "url": "https://arxiv.org/abs/2401.11944",
        "title": "CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding\n  Benchmark",
        "authors": [
            "Ge Zhang",
            "Xinrun Du",
            "Bei Chen",
            "Yiming Liang",
            "Tongxu Luo",
            "Tianyu Zheng",
            "Kang Zhu",
            "Yuyang Cheng",
            "Chunpu Xu",
            "Shuyue Guo",
            "Haoran Zhang",
            "Xingwei Qu",
            "Junjie Wang",
            "Ruibin Yuan",
            "Yizhi Li",
            "Zekun Wang",
            "Yudong Liu",
            "Yu-Hsuan Tsai",
            "Fengji Zhang",
            "Chenghua Lin",
            "Wenhao Huang",
            "Wenhu Chen",
            "Jie Fu"
        ],
        "abstract": "As the capabilities of large multimodal models (LMMs) continue to advance,\nevaluating the performance of LMMs emerges as an increasing need. Additionally,\nthere is an even larger gap in evaluating the advanced knowledge and reasoning\nabilities of LMMs in non-English contexts such as Chinese. We introduce CMMMU,\na new Chinese Massive Multi-discipline Multimodal Understanding benchmark\ndesigned to evaluate LMMs on tasks demanding college-level subject knowledge\nand deliberate reasoning in a Chinese context. CMMMU is inspired by and\nstrictly follows the annotation and analysis pattern of MMMU.\n  CMMMU includes 12k manually collected multimodal questions from college\nexams, quizzes, and textbooks, covering six core disciplines: Art & Design,\nBusiness, Science, Health & Medicine, Humanities & Social Science, and Tech &\nEngineering, like its companion, MMMU. These questions span 30 subjects and\ncomprise 39 highly heterogeneous image types, such as charts, diagrams, maps,\ntables, music sheets, and chemical structures.\n  CMMMU focuses on complex perception and reasoning with domain-specific\nknowledge in the Chinese context. We evaluate 11 open-source LLMs and one\nproprietary GPT-4V(ision). Even GPT-4V only achieves accuracies of 42%,\nindicating a large space for improvement. CMMMU will boost the community to\nbuild the next-generation LMMs towards expert artificial intelligence and\npromote the democratization of LMMs by providing diverse language contexts.",
        "publication_date": "2024-01-22T13:34:34Z",
        "upvotes": 24
    },
    "2401.12208": {
        "url": "https://arxiv.org/abs/2401.12208",
        "title": "CheXagent: Towards a Foundation Model for Chest X-Ray Interpretation",
        "authors": [
            "Zhihong Chen",
            "Maya Varma",
            "Jean-Benoit Delbrouck",
            "Magdalini Paschali",
            "Louis Blankemeier",
            "Dave Van Veen",
            "Jeya Maria Jose Valanarasu",
            "Alaa Youssef",
            "Joseph Paul Cohen",
            "Eduardo Pontes Reis",
            "Emily B. Tsai",
            "Andrew Johnston",
            "Cameron Olsen",
            "Tanishq Mathew Abraham",
            "Sergios Gatidis",
            "Akshay S. Chaudhari",
            "Curtis Langlotz"
        ],
        "abstract": "Chest X-rays (CXRs) are the most frequently performed imaging test in\nclinical practice. Recent advances in the development of vision-language\nfoundation models (FMs) give rise to the possibility of performing automated\nCXR interpretation, which can assist physicians with clinical decision-making\nand improve patient outcomes. However, developing FMs that can accurately\ninterpret CXRs is challenging due to the (1) limited availability of\nlarge-scale vision-language datasets in the medical image domain, (2) lack of\nvision and language encoders that can capture the complexities of medical data,\nand (3) absence of evaluation frameworks for benchmarking the abilities of FMs\non CXR interpretation. In this work, we address these challenges by first\nintroducing \\emph{CheXinstruct} - a large-scale instruction-tuning dataset\ncurated from 28 publicly-available datasets. We then present \\emph{CheXagent} -\nan instruction-tuned FM capable of analyzing and summarizing CXRs. To build\nCheXagent, we design a clinical large language model (LLM) for parsing\nradiology reports, a vision encoder for representing CXR images, and a network\nto bridge the vision and language modalities. Finally, we introduce\n\\emph{CheXbench} - a novel benchmark designed to systematically evaluate FMs\nacross 8 clinically-relevant CXR interpretation tasks. Extensive quantitative\nevaluations and qualitative reviews with five expert radiologists demonstrate\nthat CheXagent outperforms previously-developed general- and medical-domain FMs\non CheXbench tasks. Furthermore, in an effort to improve model transparency, we\nperform a fairness evaluation across factors of sex, race and age to highlight\npotential performance disparities. Our project is at\n\\url{https://stanford-aimi.github.io/chexagent.html}.",
        "publication_date": "2024-01-22T18:51:07Z",
        "upvotes": 20
    },
    "2401.12168": {
        "url": "https://arxiv.org/abs/2401.12168",
        "title": "SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning\n  Capabilities",
        "authors": [
            "Boyuan Chen",
            "Zhuo Xu",
            "Sean Kirmani",
            "Brian Ichter",
            "Danny Driess",
            "Pete Florence",
            "Dorsa Sadigh",
            "Leonidas Guibas",
            "Fei Xia"
        ],
        "abstract": "Understanding and reasoning about spatial relationships is a fundamental\ncapability for Visual Question Answering (VQA) and robotics. While Vision\nLanguage Models (VLM) have demonstrated remarkable performance in certain VQA\nbenchmarks, they still lack capabilities in 3D spatial reasoning, such as\nrecognizing quantitative relationships of physical objects like distances or\nsize differences. We hypothesize that VLMs' limited spatial reasoning\ncapability is due to the lack of 3D spatial knowledge in training data and aim\nto solve this problem by training VLMs with Internet-scale spatial reasoning\ndata. To this end, we present a system to facilitate this approach. We first\ndevelop an automatic 3D spatial VQA data generation framework that scales up to\n2 billion VQA examples on 10 million real-world images. We then investigate\nvarious factors in the training recipe, including data quality, training\npipeline, and VLM architecture. Our work features the first internet-scale 3D\nspatial reasoning dataset in metric space. By training a VLM on such data, we\nsignificantly enhance its ability on both qualitative and quantitative spatial\nVQA. Finally, we demonstrate that this VLM unlocks novel downstream\napplications in chain-of-thought spatial reasoning and robotics due to its\nquantitative estimation capability. Project website:\nhttps://spatial-vlm.github.io/",
        "publication_date": "2024-01-22T18:01:01Z",
        "upvotes": 20
    },
    "2401.11605": {
        "url": "https://arxiv.org/abs/2401.11605",
        "title": "Scalable High-Resolution Pixel-Space Image Synthesis with Hourglass\n  Diffusion Transformers",
        "authors": [
            "Katherine Crowson",
            "Stefan Andreas Baumann",
            "Alex Birch",
            "Tanishq Mathew Abraham",
            "Daniel Z. Kaplan",
            "Enrico Shippole"
        ],
        "abstract": "We present the Hourglass Diffusion Transformer (HDiT), an image generative\nmodel that exhibits linear scaling with pixel count, supporting training at\nhigh-resolution (e.g. $1024 \\times 1024$) directly in pixel-space. Building on\nthe Transformer architecture, which is known to scale to billions of\nparameters, it bridges the gap between the efficiency of convolutional U-Nets\nand the scalability of Transformers. HDiT trains successfully without typical\nhigh-resolution training techniques such as multiscale architectures, latent\nautoencoders or self-conditioning. We demonstrate that HDiT performs\ncompetitively with existing models on ImageNet $256^2$, and sets a new\nstate-of-the-art for diffusion models on FFHQ-$1024^2$.",
        "publication_date": "2024-01-21T21:49:49Z",
        "upvotes": 19
    },
    "2401.12179": {
        "url": "https://arxiv.org/abs/2401.12179",
        "title": "DITTO: Diffusion Inference-Time T-Optimization for Music Generation",
        "authors": [
            "Zachary Novack",
            "Julian McAuley",
            "Taylor Berg-Kirkpatrick",
            "Nicholas J. Bryan"
        ],
        "abstract": "We propose Diffusion Inference-Time T-Optimization (DITTO), a general-purpose\nframe-work for controlling pre-trained text-to-music diffusion models at\ninference-time via optimizing initial noise latents. Our method can be used to\noptimize through any differentiable feature matching loss to achieve a target\n(stylized) output and leverages gradient checkpointing for memory efficiency.\nWe demonstrate a surprisingly wide-range of applications for music generation\nincluding inpainting, outpainting, and looping as well as intensity, melody,\nand musical structure control - all without ever fine-tuning the underlying\nmodel. When we compare our approach against related training, guidance, and\noptimization-based methods, we find DITTO achieves state-of-the-art performance\non nearly all tasks, including outperforming comparable approaches on\ncontrollability, audio quality, and computational efficiency, thus opening the\ndoor for high-quality, flexible, training-free control of diffusion models.\nSound examples can be found at https://DITTO-Music.github.io/web/.",
        "publication_date": "2024-01-22T18:10:10Z",
        "upvotes": 17
    },
    "2401.12187": {
        "url": "https://arxiv.org/abs/2401.12187",
        "title": "WARM: On the Benefits of Weight Averaged Reward Models",
        "authors": [
            "Alexandre Ram\u00e9",
            "Nino Vieillard",
            "L\u00e9onard Hussenot",
            "Robert Dadashi",
            "Geoffrey Cideron",
            "Olivier Bachem",
            "Johan Ferret"
        ],
        "abstract": "Aligning large language models (LLMs) with human preferences through\nreinforcement learning (RLHF) can lead to reward hacking, where LLMs exploit\nfailures in the reward model (RM) to achieve seemingly high rewards without\nmeeting the underlying objectives. We identify two primary challenges when\ndesigning RMs to mitigate reward hacking: distribution shifts during the RL\nprocess and inconsistencies in human preferences. As a solution, we propose\nWeight Averaged Reward Models (WARM), first fine-tuning multiple RMs, then\naveraging them in the weight space. This strategy follows the observation that\nfine-tuned weights remain linearly mode connected when sharing the same\npre-training. By averaging weights, WARM improves efficiency compared to the\ntraditional ensembling of predictions, while improving reliability under\ndistribution shifts and robustness to preference inconsistencies. Our\nexperiments on summarization tasks, using best-of-N and RL methods, shows that\nWARM improves the overall quality and alignment of LLM predictions; for\nexample, a policy RL fine-tuned with WARM has a 79.4% win rate against a policy\nRL fine-tuned with a single RM.",
        "publication_date": "2024-01-22T18:27:08Z",
        "upvotes": 17
    },
    "2401.11739": {
        "url": "https://arxiv.org/abs/2401.11739",
        "title": "EmerDiff: Emerging Pixel-level Semantic Knowledge in Diffusion Models",
        "authors": [
            "Koichi Namekata",
            "Amirmojtaba Sabour",
            "Sanja Fidler",
            "Seung Wook Kim"
        ],
        "abstract": "Diffusion models have recently received increasing research attention for\ntheir remarkable transfer abilities in semantic segmentation tasks. However,\ngenerating fine-grained segmentation masks with diffusion models often requires\nadditional training on annotated datasets, leaving it unclear to what extent\npre-trained diffusion models alone understand the semantic relations of their\ngenerated images. To address this question, we leverage the semantic knowledge\nextracted from Stable Diffusion (SD) and aim to develop an image segmentor\ncapable of generating fine-grained segmentation maps without any additional\ntraining. The primary difficulty stems from the fact that semantically\nmeaningful feature maps typically exist only in the spatially lower-dimensional\nlayers, which poses a challenge in directly extracting pixel-level semantic\nrelations from these feature maps. To overcome this issue, our framework\nidentifies semantic correspondences between image pixels and spatial locations\nof low-dimensional feature maps by exploiting SD's generation process and\nutilizes them for constructing image-resolution segmentation maps. In extensive\nexperiments, the produced segmentation maps are demonstrated to be well\ndelineated and capture detailed parts of the images, indicating the existence\nof highly accurate pixel-level semantic knowledge in diffusion models.",
        "publication_date": "2024-01-22T07:34:06Z",
        "upvotes": 16
    },
    "2401.11067": {
        "url": "https://arxiv.org/abs/2401.11067",
        "title": "Make-A-Shape: a Ten-Million-scale 3D Shape Model",
        "authors": [
            "Ka-Hei Hui",
            "Aditya Sanghi",
            "Arianna Rampini",
            "Kamal Rahimi Malekshan",
            "Zhengzhe Liu",
            "Hooman Shayani",
            "Chi-Wing Fu"
        ],
        "abstract": "Significant progress has been made in training large generative models for\nnatural language and images. Yet, the advancement of 3D generative models is\nhindered by their substantial resource demands for training, along with\ninefficient, non-compact, and less expressive representations. This paper\nintroduces Make-A-Shape, a new 3D generative model designed for efficient\ntraining on a vast scale, capable of utilizing 10 millions publicly-available\nshapes. Technical-wise, we first innovate a wavelet-tree representation to\ncompactly encode shapes by formulating the subband coefficient filtering scheme\nto efficiently exploit coefficient relations. We then make the representation\ngeneratable by a diffusion model by devising the subband coefficients packing\nscheme to layout the representation in a low-resolution grid. Further, we\nderive the subband adaptive training strategy to train our model to effectively\nlearn to generate coarse and detail wavelet coefficients. Last, we extend our\nframework to be controlled by additional input conditions to enable it to\ngenerate shapes from assorted modalities, e.g., single/multi-view images, point\nclouds, and low-resolution voxels. In our extensive set of experiments, we\ndemonstrate various applications, such as unconditional generation, shape\ncompletion, and conditional generation on a wide range of modalities. Our\napproach not only surpasses the state of the art in delivering high-quality\nresults but also efficiently generates shapes within a few seconds, often\nachieving this in just 2 seconds for most conditions.",
        "publication_date": "2024-01-20T00:21:58Z",
        "upvotes": 15
    },
    "2401.12202": {
        "url": "https://arxiv.org/abs/2401.12202",
        "title": "OK-Robot: What Really Matters in Integrating Open-Knowledge Models for\n  Robotics",
        "authors": [
            "Peiqi Liu",
            "Yaswanth Orru",
            "Jay Vakil",
            "Chris Paxton",
            "Nur Muhammad Mahi Shafiullah",
            "Lerrel Pinto"
        ],
        "abstract": "Remarkable progress has been made in recent years in the fields of vision,\nlanguage, and robotics. We now have vision models capable of recognizing\nobjects based on language queries, navigation systems that can effectively\ncontrol mobile systems, and grasping models that can handle a wide range of\nobjects. Despite these advancements, general-purpose applications of robotics\nstill lag behind, even though they rely on these fundamental capabilities of\nrecognition, navigation, and grasping. In this paper, we adopt a systems-first\napproach to develop a new Open Knowledge-based robotics framework called\nOK-Robot. By combining Vision-Language Models (VLMs) for object detection,\nnavigation primitives for movement, and grasping primitives for object\nmanipulation, OK-Robot offers a integrated solution for pick-and-drop\noperations without requiring any training. To evaluate its performance, we run\nOK-Robot in 10 real-world home environments. The results demonstrate that\nOK-Robot achieves a 58.5% success rate in open-ended pick-and-drop tasks,\nrepresenting a new state-of-the-art in Open Vocabulary Mobile Manipulation\n(OVMM) with nearly 1.8x the performance of prior work. On cleaner, uncluttered\nenvironments, OK-Robot's performance increases to 82%. However, the most\nimportant insight gained from OK-Robot is the critical role of nuanced details\nwhen combining Open Knowledge systems like VLMs with robotic modules. Videos of\nour experiments and code are available on our website:\nhttps://ok-robot.github.io",
        "publication_date": "2024-01-22T18:42:20Z",
        "upvotes": 9
    },
    "2401.11053": {
        "url": "https://arxiv.org/abs/2401.11053",
        "title": "StreamVoice: Streamable Context-Aware Language Modeling for Real-time\n  Zero-Shot Voice Conversion",
        "authors": [
            "Zhichao Wang",
            "Yuanzhe Chen",
            "Xinsheng Wang",
            "Zhuo Chen",
            "Lei Xie",
            "Yuping Wang",
            "Yuxuan Wang"
        ],
        "abstract": "Recent language model (LM) advancements have showcased impressive zero-shot\nvoice conversion (VC) performance. However, existing LM-based VC models usually\napply offline conversion from source semantics to acoustic features, demanding\nthe complete source speech, and limiting their deployment to real-time\napplications. In this paper, we introduce StreamVoice, a novel streaming\nLM-based model for zero-shot VC, facilitating real-time conversion given\narbitrary speaker prompts and source speech. Specifically, to enable streaming\ncapability, StreamVoice employs a fully causal context-aware LM with a\ntemporal-independent acoustic predictor, while alternately processing semantic\nand acoustic features at each time step of autoregression which eliminates the\ndependence on complete source speech. To address the potential performance\ndegradation from the incomplete context in streaming processing, we enhance the\ncontext-awareness of the LM through two strategies: 1) teacher-guided context\nforesight, using a teacher model to summarize the present and future semantic\ncontext during training to guide the model's forecasting for missing context;\n2) semantic masking strategy, promoting acoustic prediction from preceding\ncorrupted semantic and acoustic input, enhancing context-learning ability.\nNotably, StreamVoice is the first LM-based streaming zero-shot VC model without\nany future look-ahead. Experimental results demonstrate StreamVoice's streaming\nconversion capability while maintaining zero-shot performance comparable to\nnon-streaming VC systems.",
        "publication_date": "2024-01-19T23:05:05Z",
        "upvotes": 8
    },
    "2401.11078": {
        "url": "https://arxiv.org/abs/2401.11078",
        "title": "UltrAvatar: A Realistic Animatable 3D Avatar Diffusion Model with\n  Authenticity Guided Textures",
        "authors": [
            "Mingyuan Zhou",
            "Rakib Hyder",
            "Ziwei Xuan",
            "Guojun Qi"
        ],
        "abstract": "Recent advances in 3D avatar generation have gained significant attentions.\nThese breakthroughs aim to produce more realistic animatable avatars, narrowing\nthe gap between virtual and real-world experiences. Most of existing works\nemploy Score Distillation Sampling (SDS) loss, combined with a differentiable\nrenderer and text condition, to guide a diffusion model in generating 3D\navatars. However, SDS often generates oversmoothed results with few facial\ndetails, thereby lacking the diversity compared with ancestral sampling. On the\nother hand, other works generate 3D avatar from a single image, where the\nchallenges of unwanted lighting effects, perspective views, and inferior image\nquality make them difficult to reliably reconstruct the 3D face meshes with the\naligned complete textures. In this paper, we propose a novel 3D avatar\ngeneration approach termed UltrAvatar with enhanced fidelity of geometry, and\nsuperior quality of physically based rendering (PBR) textures without unwanted\nlighting. To this end, the proposed approach presents a diffuse color\nextraction model and an authenticity guided texture diffusion model. The former\nremoves the unwanted lighting effects to reveal true diffuse colors so that the\ngenerated avatars can be rendered under various lighting conditions. The latter\nfollows two gradient-based guidances for generating PBR textures to render\ndiverse face-identity features and details better aligning with 3D mesh\ngeometry. We demonstrate the effectiveness and robustness of the proposed\nmethod, outperforming the state-of-the-art methods by a large margin in the\nexperiments.",
        "publication_date": "2024-01-20T01:55:17Z",
        "upvotes": 6
    },
    "2401.12175": {
        "url": "https://arxiv.org/abs/2401.12175",
        "title": "Template-Free Single-View 3D Human Digitalization with Diffusion-Guided\n  LRM",
        "authors": [
            "Zhenzhen Weng",
            "Jingyuan Liu",
            "Hao Tan",
            "Zhan Xu",
            "Yang Zhou",
            "Serena Yeung-Levy",
            "Jimei Yang"
        ],
        "abstract": "Reconstructing 3D humans from a single image has been extensively\ninvestigated. However, existing approaches often fall short on capturing fine\ngeometry and appearance details, hallucinating occluded parts with plausible\ndetails, and achieving generalization across unseen and in-the-wild datasets.\nWe present Human-LRM, a diffusion-guided feed-forward model that predicts the\nimplicit field of a human from a single image. Leveraging the power of the\nstate-of-the-art reconstruction model (i.e., LRM) and generative model (i.e\nStable Diffusion), our method is able to capture human without any template\nprior, e.g., SMPL, and effectively enhance occluded parts with rich and\nrealistic details. Our approach first uses a single-view LRM model with an\nenhanced geometry decoder to get the triplane NeRF representation. The novel\nview renderings from the triplane NeRF provide strong geometry and color prior,\nfrom which we generate photo-realistic details for the occluded parts using a\ndiffusion model. The generated multiple views then enable reconstruction with\nhigh-quality geometry and appearance, leading to superior overall performance\ncomparing to all existing human reconstruction methods.",
        "publication_date": "2024-01-22T18:08:22Z",
        "upvotes": 4
    },
    "2401.11002": {
        "url": "https://arxiv.org/abs/2401.11002",
        "title": "Fast Registration of Photorealistic Avatars for VR Facial Animation",
        "authors": [
            "Chaitanya Patel",
            "Shaojie Bai",
            "Te-Li Wang",
            "Jason Saragih",
            "Shih-En Wei"
        ],
        "abstract": "Virtual Reality (VR) bares promise of social interactions that can feel more\nimmersive than other media. Key to this is the ability to accurately animate a\nphotorealistic avatar of one's likeness while wearing a VR headset. Although\nhigh quality registration of person-specific avatars to headset-mounted camera\n(HMC) images is possible in an offline setting, the performance of generic\nrealtime models are significantly degraded. Online registration is also\nchallenging due to oblique camera views and differences in modality. In this\nwork, we first show that the domain gap between the avatar and headset-camera\nimages is one of the primary sources of difficulty, where a transformer-based\narchitecture achieves high accuracy on domain-consistent data, but degrades\nwhen the domain-gap is re-introduced. Building on this finding, we develop a\nsystem design that decouples the problem into two parts: 1) an iterative\nrefinement module that takes in-domain inputs, and 2) a generic avatar-guided\nimage-to-image style transfer module that is conditioned on current estimation\nof expression and head pose. These two modules reinforce each other, as image\nstyle transfer becomes easier when close-to-ground-truth examples are shown,\nand better domain-gap removal helps registration. Our system produces\nhigh-quality results efficiently, obviating the need for costly offline\nregistration to generate personalized labels. We validate the accuracy and\nefficiency of our approach through extensive experiments on a commodity\nheadset, demonstrating significant improvements over direct regression methods\nas well as offline registration.",
        "publication_date": "2024-01-19T19:42:38Z",
        "upvotes": 1
    },
    "2401.11985": {
        "url": "https://arxiv.org/abs/2401.11985",
        "title": "Scaling Face Interaction Graph Networks to Real World Scenes",
        "authors": [
            "Tatiana Lopez-Guevara",
            "Yulia Rubanova",
            "William F. Whitney",
            "Tobias Pfaff",
            "Kimberly Stachenfeld",
            "Kelsey R. Allen"
        ],
        "abstract": "Accurately simulating real world object dynamics is essential for various\napplications such as robotics, engineering, graphics, and design. To better\ncapture complex real dynamics such as contact and friction, learned simulators\nbased on graph networks have recently shown great promise. However, applying\nthese learned simulators to real scenes comes with two major challenges: first,\nscaling learned simulators to handle the complexity of real world scenes which\ncan involve hundreds of objects each with complicated 3D shapes, and second,\nhandling inputs from perception rather than 3D state information. Here we\nintroduce a method which substantially reduces the memory required to run\ngraph-based learned simulators. Based on this memory-efficient simulation\nmodel, we then present a perceptual interface in the form of editable NeRFs\nwhich can convert real-world scenes into a structured representation that can\nbe processed by graph network simulator. We show that our method uses\nsubstantially less memory than previous graph-based simulators while retaining\ntheir accuracy, and that the simulators learned in synthetic environments can\nbe applied to real world scenes captured from multiple camera angles. This\npaves the way for expanding the application of learned simulators to settings\nwhere only perceptual information is available at inference time.",
        "publication_date": "2024-01-22T14:38:25Z",
        "upvotes": 1
    },
    "2401.12945": {
        "url": "https://arxiv.org/abs/2401.12945",
        "title": "Lumiere: A Space-Time Diffusion Model for Video Generation",
        "authors": [
            "Omer Bar-Tal",
            "Hila Chefer",
            "Omer Tov",
            "Charles Herrmann",
            "Roni Paiss",
            "Shiran Zada",
            "Ariel Ephrat",
            "Junhwa Hur",
            "Guanghui Liu",
            "Amit Raj",
            "Yuanzhen Li",
            "Michael Rubinstein",
            "Tomer Michaeli",
            "Oliver Wang",
            "Deqing Sun",
            "Tali Dekel",
            "Inbar Mosseri"
        ],
        "abstract": "We introduce Lumiere -- a text-to-video diffusion model designed for\nsynthesizing videos that portray realistic, diverse and coherent motion -- a\npivotal challenge in video synthesis. To this end, we introduce a Space-Time\nU-Net architecture that generates the entire temporal duration of the video at\nonce, through a single pass in the model. This is in contrast to existing video\nmodels which synthesize distant keyframes followed by temporal super-resolution\n-- an approach that inherently makes global temporal consistency difficult to\nachieve. By deploying both spatial and (importantly) temporal down- and\nup-sampling and leveraging a pre-trained text-to-image diffusion model, our\nmodel learns to directly generate a full-frame-rate, low-resolution video by\nprocessing it in multiple space-time scales. We demonstrate state-of-the-art\ntext-to-video generation results, and show that our design easily facilitates a\nwide range of content creation tasks and video editing applications, including\nimage-to-video, video inpainting, and stylized generation.",
        "publication_date": "2024-01-23T18:05:25Z",
        "upvotes": 82
    },
    "2401.12474": {
        "url": "https://arxiv.org/abs/2401.12474",
        "title": "Large Language Models are Superpositions of All Characters: Attaining\n  Arbitrary Role-play via Self-Alignment",
        "authors": [
            "Keming Lu",
            "Bowen Yu",
            "Chang Zhou",
            "Jingren Zhou"
        ],
        "abstract": "Considerable efforts have been invested in augmenting the role-playing\nproficiency of open-source large language models (LLMs) by emulating\nproprietary counterparts. Nevertheless, we posit that LLMs inherently harbor\nrole-play capabilities, owing to the extensive knowledge of characters and\npotential dialogues ingrained in their vast training corpora. Thus, in this\nstudy, we introduce Ditto, a self-alignment method for role-play. Ditto\ncapitalizes on character knowledge, encouraging an instruction-following LLM to\nsimulate role-play dialogues as a variant of reading comprehension. This method\ncreates a role-play training set comprising 4,000 characters, surpassing the\nscale of currently available datasets by tenfold regarding the number of roles.\nSubsequently, we fine-tune the LLM using this self-generated dataset to augment\nits role-playing capabilities. Upon evaluating our meticulously constructed and\nreproducible role-play benchmark and the roleplay subset of MT-Bench, Ditto, in\nvarious parameter scales, consistently maintains a consistent role identity and\nprovides accurate role-specific knowledge in multi-turn role-play\nconversations. Notably, it outperforms all open-source role-play baselines,\nshowcasing performance levels comparable to advanced proprietary chatbots.\nFurthermore, we present the first comprehensive cross-supervision alignment\nexperiment in the role-play domain, revealing that the intrinsic capabilities\nof LLMs confine the knowledge within role-play. Meanwhile, the role-play styles\ncan be easily acquired with the guidance of smaller models. We open-source\nrelated resources at https://github.com/OFA-Sys/Ditto.",
        "publication_date": "2024-01-23T03:56:22Z",
        "upvotes": 32
    },
    "2401.12503": {
        "url": "https://arxiv.org/abs/2401.12503",
        "title": "Small Language Model Meets with Reinforced Vision Vocabulary",
        "authors": [
            "Haoran Wei",
            "Lingyu Kong",
            "Jinyue Chen",
            "Liang Zhao",
            "Zheng Ge",
            "En Yu",
            "Jianjian Sun",
            "Chunrui Han",
            "Xiangyu Zhang"
        ],
        "abstract": "Playing Large Vision Language Models (LVLMs) in 2023 is trendy among the AI\ncommunity. However, the relatively large number of parameters (more than 7B) of\npopular LVLMs makes it difficult to train and deploy on consumer GPUs,\ndiscouraging many researchers with limited resources. Imagine how cool it would\nbe to experience all the features of current LVLMs on an old GTX1080ti (our\nonly game card). Accordingly, we present Vary-toy in this report, a small-size\nVary along with Qwen-1.8B as the base ``large'' language model. In Vary-toy, we\nintroduce an improved vision vocabulary, allowing the model to not only possess\nall features of Vary but also gather more generality. Specifically, we replace\nnegative samples of natural images with positive sample data driven by object\ndetection in the procedure of generating vision vocabulary, more sufficiently\nutilizing the capacity of the vocabulary network and enabling it to efficiently\nencode visual information corresponding to natural objects. For experiments,\nVary-toy can achieve 65.6% ANLS on DocVQA, 59.1% accuracy on ChartQA, 88.1%\naccuracy on RefCOCO, and 29% on MMVet. The code will be publicly available on\nthe homepage.",
        "publication_date": "2024-01-23T05:55:26Z",
        "upvotes": 29
    },
    "2401.12954": {
        "url": "https://arxiv.org/abs/2401.12954",
        "title": "Meta-Prompting: Enhancing Language Models with Task-Agnostic Scaffolding",
        "authors": [
            "Mirac Suzgun",
            "Adam Tauman Kalai"
        ],
        "abstract": "We introduce meta-prompting, an effective scaffolding technique designed to\nenhance the functionality of language models (LMs). This approach transforms a\nsingle LM into a multi-faceted conductor, adept at managing and integrating\nmultiple independent LM queries. By employing high-level instructions,\nmeta-prompting guides the LM to break down complex tasks into smaller, more\nmanageable subtasks. These subtasks are then handled by distinct \"expert\"\ninstances of the same LM, each operating under specific, tailored instructions.\nCentral to this process is the LM itself, in its role as the conductor, which\nensures seamless communication and effective integration of the outputs from\nthese expert models. It additionally employs its inherent critical thinking and\nrobust verification processes to refine and authenticate the end result. This\ncollaborative prompting approach empowers a single LM to simultaneously act as\na comprehensive orchestrator and a panel of diverse experts, significantly\nenhancing its performance across a wide array of tasks. The zero-shot,\ntask-agnostic nature of meta-prompting greatly simplifies user interaction by\nobviating the need for detailed, task-specific instructions. Furthermore, our\nresearch demonstrates the seamless integration of external tools, such as a\nPython interpreter, into the meta-prompting framework, thereby broadening its\napplicability and utility. Through rigorous experimentation with GPT-4, we\nestablish the superiority of meta-prompting over conventional scaffolding\nmethods: When averaged across all tasks, including the Game of 24,\nCheckmate-in-One, and Python Programming Puzzles, meta-prompting, augmented\nwith a Python interpreter functionality, surpasses standard prompting by 17.1%,\nexpert (dynamic) prompting by 17.3%, and multipersona prompting by 15.2%.",
        "publication_date": "2024-01-23T18:22:19Z",
        "upvotes": 28
    },
    "2401.12244": {
        "url": "https://arxiv.org/abs/2401.12244",
        "title": "Large-scale Reinforcement Learning for Diffusion Models",
        "authors": [
            "Yinan Zhang",
            "Eric Tzeng",
            "Yilun Du",
            "Dmitry Kislyuk"
        ],
        "abstract": "Text-to-image diffusion models are a class of deep generative models that\nhave demonstrated an impressive capacity for high-quality image generation.\nHowever, these models are susceptible to implicit biases that arise from\nweb-scale text-image training pairs and may inaccurately model aspects of\nimages we care about. This can result in suboptimal samples, model bias, and\nimages that do not align with human ethics and preferences. In this paper, we\npresent an effective scalable algorithm to improve diffusion models using\nReinforcement Learning (RL) across a diverse set of reward functions, such as\nhuman preference, compositionality, and fairness over millions of images. We\nillustrate how our approach substantially outperforms existing methods for\naligning diffusion models with human preferences. We further illustrate how\nthis substantially improves pretrained Stable Diffusion (SD) models, generating\nsamples that are preferred by humans 80.3% of the time over those from the base\nSD model while simultaneously improving both the composition and diversity of\ngenerated samples.",
        "publication_date": "2024-01-20T08:10:43Z",
        "upvotes": 27
    },
    "2401.12963": {
        "url": "https://arxiv.org/abs/2401.12963",
        "title": "AutoRT: Embodied Foundation Models for Large Scale Orchestration of\n  Robotic Agents",
        "authors": [
            "Michael Ahn",
            "Debidatta Dwibedi",
            "Chelsea Finn",
            "Montse Gonzalez Arenas",
            "Keerthana Gopalakrishnan",
            "Karol Hausman",
            "Brian Ichter",
            "Alex Irpan",
            "Nikhil Joshi",
            "Ryan Julian",
            "Sean Kirmani",
            "Isabel Leal",
            "Edward Lee",
            "Sergey Levine",
            "Yao Lu",
            "Isabel Leal",
            "Sharath Maddineni",
            "Kanishka Rao",
            "Dorsa Sadigh",
            "Pannag Sanketi",
            "Pierre Sermanet",
            "Quan Vuong",
            "Stefan Welker",
            "Fei Xia",
            "Ted Xiao",
            "Peng Xu",
            "Steve Xu",
            "Zhuo Xu"
        ],
        "abstract": "Foundation models that incorporate language, vision, and more recently\nactions have revolutionized the ability to harness internet scale data to\nreason about useful tasks. However, one of the key challenges of training\nembodied foundation models is the lack of data grounded in the physical world.\nIn this paper, we propose AutoRT, a system that leverages existing foundation\nmodels to scale up the deployment of operational robots in completely unseen\nscenarios with minimal human supervision. AutoRT leverages vision-language\nmodels (VLMs) for scene understanding and grounding, and further uses large\nlanguage models (LLMs) for proposing diverse and novel instructions to be\nperformed by a fleet of robots. Guiding data collection by tapping into the\nknowledge of foundation models enables AutoRT to effectively reason about\nautonomy tradeoffs and safety while significantly scaling up data collection\nfor robot learning. We demonstrate AutoRT proposing instructions to over 20\nrobots across multiple buildings and collecting 77k real robot episodes via\nboth teleoperation and autonomous robot policies. We experimentally show that\nsuch \"in-the-wild\" data collected by AutoRT is significantly more diverse, and\nthat AutoRT's use of LLMs allows for instruction following data collection\nrobots that can align to human preferences.",
        "publication_date": "2024-01-23T18:45:54Z",
        "upvotes": 11
    },
    "2401.12522": {
        "url": "https://arxiv.org/abs/2401.12522",
        "title": "BiTA: Bi-Directional Tuning for Lossless Acceleration in Large Language\n  Models",
        "authors": [
            "Feng Lin",
            "Hanling Yi",
            "Hongbin Li",
            "Yifan Yang",
            "Xiaotian Yu",
            "Guangming Lu",
            "Rong Xiao"
        ],
        "abstract": "Large language models (LLMs) commonly employ autoregressive generation during\ninference, leading to high memory bandwidth demand and consequently extended\nlatency. To mitigate this inefficiency, we present Bi-directional Tuning for\nlossless Acceleration (BiTA), an innovative method expediting LLMs via\nstreamlined semi-autoregressive generation and draft verification. Inspired by\nthe concept of prompt tuning, we enhance LLMs with a parameter-efficient design\ncalled bi-directional tuning for the capability in semi-autoregressive\ngeneration. Employing efficient tree-based decoding, the models perform draft\ncandidate generation and verification in parallel, ensuring outputs identical\nto their autoregressive counterparts under greedy sampling. BiTA serves as a\nlightweight plug-in module, seamlessly boosting the inference efficiency of\nexisting LLMs without requiring additional assistance models or incurring\nsignificant extra memory costs. Applying the proposed BiTA, LLaMA-2-70B-Chat\nachieves a 2.7$\\times$ speedup on the MT-Bench benchmark. Extensive experiments\nconfirm our method surpasses state-of-the-art acceleration techniques.",
        "publication_date": "2024-01-23T06:36:49Z",
        "upvotes": 11
    },
    "2401.12246": {
        "url": "https://arxiv.org/abs/2401.12246",
        "title": "Orion-14B: Open-source Multilingual Large Language Models",
        "authors": [
            "Du Chen",
            "Yi Huang",
            "Xiaopu Li",
            "Yongqiang Li",
            "Yongqiang Liu",
            "Haihui Pan",
            "Leichao Xu",
            "Dacheng Zhang",
            "Zhipeng Zhang",
            "Kun Han"
        ],
        "abstract": "In this study, we introduce Orion-14B, a collection of multilingual large\nlanguage models with 14 billion parameters. We utilize a data scheduling\napproach to train a foundational model on a diverse corpus of 2.5 trillion\ntokens, sourced from texts in English, Chinese, Japanese, Korean, and other\nlanguages. Additionally, we fine-tuned a series of models tailored for\nconversational applications and other specific use cases. Our evaluation\nresults demonstrate that Orion-14B achieves state-of-the-art performance across\na broad spectrum of tasks. We make the Orion-14B model family and its\nassociated code publicly accessible https://github.com/OrionStarAI/Orion,\naiming to inspire future research and practical applications in the field.",
        "publication_date": "2024-01-20T12:29:27Z",
        "upvotes": 10
    },
    "2401.12979": {
        "url": "https://arxiv.org/abs/2401.12979",
        "title": "GALA: Generating Animatable Layered Assets from a Single Scan",
        "authors": [
            "Taeksoo Kim",
            "Byungjun Kim",
            "Shunsuke Saito",
            "Hanbyul Joo"
        ],
        "abstract": "We present GALA, a framework that takes as input a single-layer clothed 3D\nhuman mesh and decomposes it into complete multi-layered 3D assets. The outputs\ncan then be combined with other assets to create novel clothed human avatars\nwith any pose. Existing reconstruction approaches often treat clothed humans as\na single-layer of geometry and overlook the inherent compositionality of humans\nwith hairstyles, clothing, and accessories, thereby limiting the utility of the\nmeshes for downstream applications. Decomposing a single-layer mesh into\nseparate layers is a challenging task because it requires the synthesis of\nplausible geometry and texture for the severely occluded regions. Moreover,\neven with successful decomposition, meshes are not normalized in terms of poses\nand body shapes, failing coherent composition with novel identities and poses.\nTo address these challenges, we propose to leverage the general knowledge of a\npretrained 2D diffusion model as geometry and appearance prior for humans and\nother assets. We first separate the input mesh using the 3D surface\nsegmentation extracted from multi-view 2D segmentations. Then we synthesize the\nmissing geometry of different layers in both posed and canonical spaces using a\nnovel pose-guided Score Distillation Sampling (SDS) loss. Once we complete\ninpainting high-fidelity 3D geometry, we also apply the same SDS loss to its\ntexture to obtain the complete appearance including the initially occluded\nregions. Through a series of decomposition steps, we obtain multiple layers of\n3D assets in a shared canonical space normalized in terms of poses and human\nshapes, hence supporting effortless composition to novel identities and\nreanimation with novel poses. Our experiments demonstrate the effectiveness of\nour approach for decomposition, canonicalization, and composition tasks\ncompared to existing solutions.",
        "publication_date": "2024-01-23T18:59:59Z",
        "upvotes": 5
    },
    "2401.12789": {
        "url": "https://arxiv.org/abs/2401.12789",
        "title": "Multilingual and Fully Non-Autoregressive ASR with Large Language Model\n  Fusion: A Comprehensive Study",
        "authors": [
            "W. Ronny Huang",
            "Cyril Allauzen",
            "Tongzhou Chen",
            "Kilol Gupta",
            "Ke Hu",
            "James Qin",
            "Yu Zhang",
            "Yongqiang Wang",
            "Shuo-Yiin Chang",
            "Tara N. Sainath"
        ],
        "abstract": "In the era of large models, the autoregressive nature of decoding often\nresults in latency serving as a significant bottleneck. We propose a\nnon-autoregressive LM-fused ASR system that effectively leverages the\nparallelization capabilities of accelerator hardware. Our approach combines the\nUniversal Speech Model (USM) and the PaLM 2 language model in per-segment\nscoring mode, achieving an average relative WER improvement across all\nlanguages of 10.8% on FLEURS and 3.6% on YouTube captioning. Furthermore, our\ncomprehensive ablation study analyzes key parameters such as LLM size, context\nlength, vocabulary size, fusion methodology. For instance, we explore the\nimpact of LLM size ranging from 128M to 340B parameters on ASR performance.\nThis study provides valuable insights into the factors influencing the\neffectiveness of practical large-scale LM-fused speech recognition systems.",
        "publication_date": "2024-01-23T14:19:01Z",
        "upvotes": 5
    },
    "2401.13627": {
        "url": "https://arxiv.org/abs/2401.13627",
        "title": "Scaling Up to Excellence: Practicing Model Scaling for Photo-Realistic\n  Image Restoration In the Wild",
        "authors": [
            "Fanghua Yu",
            "Jinjin Gu",
            "Zheyuan Li",
            "Jinfan Hu",
            "Xiangtao Kong",
            "Xintao Wang",
            "Jingwen He",
            "Yu Qiao",
            "Chao Dong"
        ],
        "abstract": "We introduce SUPIR (Scaling-UP Image Restoration), a groundbreaking image\nrestoration method that harnesses generative prior and the power of model\nscaling up. Leveraging multi-modal techniques and advanced generative prior,\nSUPIR marks a significant advance in intelligent and realistic image\nrestoration. As a pivotal catalyst within SUPIR, model scaling dramatically\nenhances its capabilities and demonstrates new potential for image restoration.\nWe collect a dataset comprising 20 million high-resolution, high-quality images\nfor model training, each enriched with descriptive text annotations. SUPIR\nprovides the capability to restore images guided by textual prompts, broadening\nits application scope and potential. Moreover, we introduce negative-quality\nprompts to further improve perceptual quality. We also develop a\nrestoration-guided sampling method to suppress the fidelity issue encountered\nin generative-based restoration. Experiments demonstrate SUPIR's exceptional\nrestoration effects and its novel capacity to manipulate restoration through\ntextual prompts.",
        "publication_date": "2024-01-24T17:58:07Z",
        "upvotes": 68
    },
    "2401.13660": {
        "url": "https://arxiv.org/abs/2401.13660",
        "title": "MambaByte: Token-free Selective State Space Model",
        "authors": [
            "Junxiong Wang",
            "Tushaar Gangavarapu",
            "Jing Nathan Yan",
            "Alexander M. Rush"
        ],
        "abstract": "Token-free language models learn directly from raw bytes and remove the\ninductive bias of subword tokenization. Operating on bytes, however, results in\nsignificantly longer sequences. In this setting, standard autoregressive\nTransformers scale poorly as the effective memory required grows with sequence\nlength. The recent development of the Mamba state space model (SSM) offers an\nappealing alternative approach with a fixed-sized memory state and efficient\ndecoding. We propose MambaByte, a token-free adaptation of the Mamba SSM\ntrained autoregressively on byte sequences. In terms of modeling, we show\nMambaByte to be competitive with, and even to outperform, state-of-the-art\nsubword Transformers on language modeling tasks while maintaining the benefits\nof token-free language models, such as robustness to noise. In terms of\nefficiency, we develop an adaptation of speculative decoding with tokenized\ndrafting and byte-level verification. This results in a $2.6\\times$ inference\nspeedup to the standard MambaByte implementation, showing similar decoding\nefficiency as the subword Mamba. These findings establish the viability of SSMs\nin enabling token-free language modeling.",
        "publication_date": "2024-01-24T18:53:53Z",
        "upvotes": 46
    },
    "2401.13601": {
        "url": "https://arxiv.org/abs/2401.13601",
        "title": "MM-LLMs: Recent Advances in MultiModal Large Language Models",
        "authors": [
            "Duzhen Zhang",
            "Yahan Yu",
            "Chenxing Li",
            "Jiahua Dong",
            "Dan Su",
            "Chenhui Chu",
            "Dong Yu"
        ],
        "abstract": "In the past year, MultiModal Large Language Models (MM-LLMs) have undergone\nsubstantial advancements, augmenting off-the-shelf LLMs to support MM inputs or\noutputs via cost-effective training strategies. The resulting models not only\npreserve the inherent reasoning and decision-making capabilities of LLMs but\nalso empower a diverse range of MM tasks. In this paper, we provide a\ncomprehensive survey aimed at facilitating further research of MM-LLMs.\nInitially, we outline general design formulations for model architecture and\ntraining pipeline. Subsequently, we introduce a taxonomy encompassing $122$\nMM-LLMs, each characterized by its specific formulations. Furthermore, we\nreview the performance of selected MM-LLMs on mainstream benchmarks and\nsummarize key training recipes to enhance the potency of MM-LLMs. Finally, we\nexplore promising directions for MM-LLMs while concurrently maintaining a\nreal-time tracking website for the latest developments in the field. We hope\nthat this survey contributes to the ongoing advancement of the MM-LLMs domain.",
        "publication_date": "2024-01-24T17:10:45Z",
        "upvotes": 41
    },
    "2401.13303": {
        "url": "https://arxiv.org/abs/2401.13303",
        "title": "MaLA-500: Massive Language Adaptation of Large Language Models",
        "authors": [
            "Peiqin Lin",
            "Shaoxiong Ji",
            "J\u00f6rg Tiedemann",
            "Andr\u00e9 F. T. Martins",
            "Hinrich Sch\u00fctze"
        ],
        "abstract": "Large language models (LLMs) have advanced the state of the art in natural\nlanguage processing. However, their predominant design for English or a limited\nset of languages creates a substantial gap in their effectiveness for\nlow-resource languages. To bridge this gap, we introduce MaLA-500, a novel\nlarge language model designed to cover an extensive range of 534 languages. To\ntrain MaLA-500, we employ vocabulary extension and continued pretraining on\nLLaMA 2 with Glot500-c. Our intrinsic evaluation demonstrates that MaLA-500 is\nbetter at predicting the given texts of low-resource languages than existing\nmultilingual LLMs. Moreover, the extrinsic evaluation of in-context learning\nshows that MaLA-500 outperforms previous LLMs on SIB200 and Taxi1500 by a\nsignificant margin, i.e., 11.68% and 4.82% marco-average accuracy across\nlanguages. We release MaLA-500 at https://huggingface.co/MaLA-LM",
        "publication_date": "2024-01-24T08:57:39Z",
        "upvotes": 11
    },
    "2401.13160": {
        "url": "https://arxiv.org/abs/2401.13160",
        "title": "SpacTor-T5: Pre-training T5 Models with Span Corruption and Replaced\n  Token Detection",
        "authors": [
            "Ke Ye",
            "Heinrich Jiang",
            "Afshin Rostamizadeh",
            "Ayan Chakrabarti",
            "Giulia DeSalvo",
            "Jean-Fran\u00e7ois Kagy",
            "Lazaros Karydas",
            "Gui Citovsky",
            "Sanjiv Kumar"
        ],
        "abstract": "Pre-training large language models is known to be extremely resource\nintensive and often times inefficient, under-utilizing the information\nencapsulated in the training text sequences. In this paper, we present SpacTor,\na new training procedure consisting of (1) a hybrid objective combining span\ncorruption (SC) and token replacement detection (RTD), and (2) a two-stage\ncurriculum that optimizes the hybrid objective over the initial $\\tau$\niterations, then transitions to standard SC loss. We show empirically that the\neffectiveness of the hybrid objective is tied to the two-stage pre-training\nschedule, and provide extensive analysis on why this is the case. In our\nexperiments with encoder-decoder architectures (T5) on a variety of NLP tasks,\nSpacTor-T5 yields the same downstream performance as standard SC pre-training,\nwhile enabling a 50% reduction in pre-training iterations and 40% reduction in\ntotal FLOPs. Alternatively, given the same amount of computing budget, we find\nthat SpacTor results in significantly improved downstream benchmark\nperformance.",
        "publication_date": "2024-01-24T00:36:13Z",
        "upvotes": 9
    },
    "2401.13388": {
        "url": "https://arxiv.org/abs/2401.13388",
        "title": "UNIMO-G: Unified Image Generation through Multimodal Conditional\n  Diffusion",
        "authors": [
            "Wei Li",
            "Xue Xu",
            "Jiachen Liu",
            "Xinyan Xiao"
        ],
        "abstract": "Existing text-to-image diffusion models primarily generate images from text\nprompts. However, the inherent conciseness of textual descriptions poses\nchallenges in faithfully synthesizing images with intricate details, such as\nspecific entities or scenes. This paper presents UNIMO-G, a simple multimodal\nconditional diffusion framework that operates on multimodal prompts with\ninterleaved textual and visual inputs, which demonstrates a unified ability for\nboth text-driven and subject-driven image generation. UNIMO-G comprises two\ncore components: a Multimodal Large Language Model (MLLM) for encoding\nmultimodal prompts, and a conditional denoising diffusion network for\ngenerating images based on the encoded multimodal input. We leverage a\ntwo-stage training strategy to effectively train the framework: firstly\npre-training on large-scale text-image pairs to develop conditional image\ngeneration capabilities, and then instruction tuning with multimodal prompts to\nachieve unified image generation proficiency. A well-designed data processing\npipeline involving language grounding and image segmentation is employed to\nconstruct multi-modal prompts. UNIMO-G excels in both text-to-image generation\nand zero-shot subject-driven synthesis, and is notably effective in generating\nhigh-fidelity images from complex multimodal prompts involving multiple image\nentities.",
        "publication_date": "2024-01-24T11:36:44Z",
        "upvotes": 9
    },
    "2401.13311": {
        "url": "https://arxiv.org/abs/2401.13311",
        "title": "ConTextual: Evaluating Context-Sensitive Text-Rich Visual Reasoning in\n  Large Multimodal Models",
        "authors": [
            "Rohan Wadhawan",
            "Hritik Bansal",
            "Kai-Wei Chang",
            "Nanyun Peng"
        ],
        "abstract": "Recent advancements in AI have led to the development of large multimodal\nmodels (LMMs) capable of processing complex tasks involving joint reasoning\nover text and visual content in the image (e.g., navigating maps in public\nplaces). This paper introduces ConTextual, a novel benchmark comprising\ninstructions designed explicitly to evaluate LMMs' ability to perform\ncontext-sensitive text-rich visual reasoning. ConTextual emphasizes diverse\nreal-world scenarios (e.g., time-reading, navigation, shopping and more)\ndemanding a deeper understanding of the interactions between textual and visual\nelements. Our findings reveal a significant performance gap of 30.8% between\nthe best-performing LMM, GPT-4V(ision), and human capabilities using human\nevaluation indicating substantial room for improvement in context-sensitive\ntext-rich visual reasoning. Notably, while GPT-4V excelled in abstract\ncategories like meme and quote interpretation, its overall performance still\nlagged behind humans. In addition to human evaluations, we also employed\nautomatic evaluation metrics using GPT-4, uncovering similar trends in\nperformance disparities. We also perform a fine-grained evaluation across\ndiverse visual contexts and provide qualitative analysis which provides a\nrobust framework for future advancements in the LMM design.\nhttps://con-textual.github.io/",
        "publication_date": "2024-01-24T09:07:11Z",
        "upvotes": 8
    },
    "2401.13795": {
        "url": "https://arxiv.org/abs/2401.13795",
        "title": "Diffuse to Choose: Enriching Image Conditioned Inpainting in Latent\n  Diffusion Models for Virtual Try-All",
        "authors": [
            "Mehmet Saygin Seyfioglu",
            "Karim Bouyarmane",
            "Suren Kumar",
            "Amir Tavanaei",
            "Ismail B. Tutar"
        ],
        "abstract": "As online shopping is growing, the ability for buyers to virtually visualize\nproducts in their settings-a phenomenon we define as \"Virtual Try-All\"-has\nbecome crucial. Recent diffusion models inherently contain a world model,\nrendering them suitable for this task within an inpainting context. However,\ntraditional image-conditioned diffusion models often fail to capture the\nfine-grained details of products. In contrast, personalization-driven models\nsuch as DreamPaint are good at preserving the item's details but they are not\noptimized for real-time applications. We present \"Diffuse to Choose,\" a novel\ndiffusion-based image-conditioned inpainting model that efficiently balances\nfast inference with the retention of high-fidelity details in a given reference\nitem while ensuring accurate semantic manipulations in the given scene content.\nOur approach is based on incorporating fine-grained features from the reference\nimage directly into the latent feature maps of the main diffusion model,\nalongside with a perceptual loss to further preserve the reference item's\ndetails. We conduct extensive testing on both in-house and publicly available\ndatasets, and show that Diffuse to Choose is superior to existing zero-shot\ndiffusion inpainting methods as well as few-shot diffusion personalization\nalgorithms like DreamPaint.",
        "publication_date": "2024-01-24T20:25:48Z",
        "upvotes": 62
    },
    "2401.14196": {
        "url": "https://arxiv.org/abs/2401.14196",
        "title": "DeepSeek-Coder: When the Large Language Model Meets Programming -- The\n  Rise of Code Intelligence",
        "authors": [
            "Daya Guo",
            "Qihao Zhu",
            "Dejian Yang",
            "Zhenda Xie",
            "Kai Dong",
            "Wentao Zhang",
            "Guanting Chen",
            "Xiao Bi",
            "Y. Wu",
            "Y. K. Li",
            "Fuli Luo",
            "Yingfei Xiong",
            "Wenfeng Liang"
        ],
        "abstract": "The rapid development of large language models has revolutionized code\nintelligence in software development. However, the predominance of\nclosed-source models has restricted extensive research and development. To\naddress this, we introduce the DeepSeek-Coder series, a range of open-source\ncode models with sizes from 1.3B to 33B, trained from scratch on 2 trillion\ntokens. These models are pre-trained on a high-quality project-level code\ncorpus and employ a fill-in-the-blank task with a 16K window to enhance code\ngeneration and infilling. Our extensive evaluations demonstrate that\nDeepSeek-Coder not only achieves state-of-the-art performance among open-source\ncode models across multiple benchmarks but also surpasses existing\nclosed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models\nare under a permissive license that allows for both research and unrestricted\ncommercial use.",
        "publication_date": "2024-01-25T14:17:53Z",
        "upvotes": 42
    },
    "2401.14391": {
        "url": "https://arxiv.org/abs/2401.14391",
        "title": "Rethinking Patch Dependence for Masked Autoencoders",
        "authors": [
            "Letian Fu",
            "Long Lian",
            "Renhao Wang",
            "Baifeng Shi",
            "Xudong Wang",
            "Adam Yala",
            "Trevor Darrell",
            "Alexei A. Efros",
            "Ken Goldberg"
        ],
        "abstract": "In this work, we re-examine inter-patch dependencies in the decoding\nmechanism of masked autoencoders (MAE). We decompose this decoding mechanism\nfor masked patch reconstruction in MAE into self-attention and cross-attention.\nOur investigations suggest that self-attention between mask patches is not\nessential for learning good representations. To this end, we propose a novel\npretraining framework: Cross-Attention Masked Autoencoders (CrossMAE).\nCrossMAE's decoder leverages only cross-attention between masked and visible\ntokens, with no degradation in downstream performance. This design also enables\ndecoding only a small subset of mask tokens, boosting efficiency. Furthermore,\neach decoder block can now leverage different encoder features, resulting in\nimproved representation learning. CrossMAE matches MAE in performance with 2.5\nto 3.7$\\times$ less decoding compute. It also surpasses MAE on ImageNet\nclassification and COCO instance segmentation under the same compute. Code and\nmodels: https://crossmae.github.io",
        "publication_date": "2024-01-25T18:49:57Z",
        "upvotes": 22
    },
    "2401.13919": {
        "url": "https://arxiv.org/abs/2401.13919",
        "title": "WebVoyager: Building an End-to-End Web Agent with Large Multimodal\n  Models",
        "authors": [
            "Hongliang He",
            "Wenlin Yao",
            "Kaixin Ma",
            "Wenhao Yu",
            "Yong Dai",
            "Hongming Zhang",
            "Zhenzhong Lan",
            "Dong Yu"
        ],
        "abstract": "The rapid advancement of large language models (LLMs) has led to a new era\nmarked by the development of autonomous applications in real-world scenarios,\nwhich drives innovation in creating advanced web agents. Existing web agents\ntypically only handle one input modality and are evaluated only in simplified\nweb simulators or static web snapshots, greatly limiting their applicability in\nreal-world scenarios. To bridge this gap, we introduce WebVoyager, an\ninnovative Large Multimodal Model (LMM) powered web agent that can complete\nuser instructions end-to-end by interacting with real-world websites. Moreover,\nwe establish a new benchmark by compiling real-world tasks from 15 popular\nwebsites and introduce an automatic evaluation protocol leveraging multimodal\nunderstanding abilities of GPT-4V to evaluate open-ended web agents. We show\nthat WebVoyager achieves a 59.1% task success rate on our benchmark,\nsignificantly surpassing the performance of both GPT-4 (All Tools) and the\nWebVoyager (text-only) setups, underscoring the exceptional capability of\nWebVoyager. The proposed automatic evaluation metric achieves 85.3% agreement\nwith human judgment, indicating its effectiveness in providing reliable and\naccurate assessments of web agents.",
        "publication_date": "2024-01-25T03:33:18Z",
        "upvotes": 19
    },
    "2401.14019": {
        "url": "https://arxiv.org/abs/2401.14019",
        "title": "Unitxt: Flexible, Shareable and Reusable Data Preparation and Evaluation\n  for Generative AI",
        "authors": [
            "Elron Bandel",
            "Yotam Perlitz",
            "Elad Venezian",
            "Roni Friedman-Melamed",
            "Ofir Arviv",
            "Matan Orbach",
            "Shachar Don-Yehyia",
            "Dafna Sheinwald",
            "Ariel Gera",
            "Leshem Choshen",
            "Michal Shmueli-Scheuer",
            "Yoav Katz"
        ],
        "abstract": "In the dynamic landscape of generative NLP, traditional text processing\npipelines limit research flexibility and reproducibility, as they are tailored\nto specific dataset, task, and model combinations. The escalating complexity,\ninvolving system prompts, model-specific formats, instructions, and more, calls\nfor a shift to a structured, modular, and customizable solution. Addressing\nthis need, we present Unitxt, an innovative library for customizable textual\ndata preparation and evaluation tailored to generative language models. Unitxt\nnatively integrates with common libraries like HuggingFace and LM-eval-harness\nand deconstructs processing flows into modular components, enabling easy\ncustomization and sharing between practitioners. These components encompass\nmodel-specific formats, task prompts, and many other comprehensive dataset\nprocessing definitions. The Unitxt-Catalog centralizes these components,\nfostering collaboration and exploration in modern textual data workflows.\nBeyond being a tool, Unitxt is a community-driven platform, empowering users to\nbuild, share, and advance their pipelines collaboratively. Join the Unitxt\ncommunity at https://github.com/IBM/unitxt!",
        "publication_date": "2024-01-25T08:57:33Z",
        "upvotes": 18
    },
    "2401.14404": {
        "url": "https://arxiv.org/abs/2401.14404",
        "title": "Deconstructing Denoising Diffusion Models for Self-Supervised Learning",
        "authors": [
            "Xinlei Chen",
            "Zhuang Liu",
            "Saining Xie",
            "Kaiming He"
        ],
        "abstract": "In this study, we examine the representation learning abilities of Denoising\nDiffusion Models (DDM) that were originally purposed for image generation. Our\nphilosophy is to deconstruct a DDM, gradually transforming it into a classical\nDenoising Autoencoder (DAE). This deconstructive procedure allows us to explore\nhow various components of modern DDMs influence self-supervised representation\nlearning. We observe that only a very few modern components are critical for\nlearning good representations, while many others are nonessential. Our study\nultimately arrives at an approach that is highly simplified and to a large\nextent resembles a classical DAE. We hope our study will rekindle interest in a\nfamily of classical methods within the realm of modern self-supervised\nlearning.",
        "publication_date": "2024-01-25T18:59:57Z",
        "upvotes": 16
    },
    "2401.14112": {
        "url": "https://arxiv.org/abs/2401.14112",
        "title": "FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric\n  Algorithm-System Co-Design",
        "authors": [
            "Haojun Xia",
            "Zhen Zheng",
            "Xiaoxia Wu",
            "Shiyang Chen",
            "Zhewei Yao",
            "Stephen Youn",
            "Arash Bakhtiari",
            "Michael Wyatt",
            "Donglin Zhuang",
            "Zhongzhu Zhou",
            "Olatunji Ruwase",
            "Yuxiong He",
            "Shuaiwen Leon Song"
        ],
        "abstract": "Six-bit quantization (FP6) can effectively reduce the size of large language\nmodels (LLMs) and preserve the model quality consistently across varied\napplications. However, existing systems do not provide Tensor Core support for\nFP6 quantization and struggle to achieve practical performance improvements\nduring LLM inference. It is challenging to support FP6 quantization on GPUs due\nto (1) unfriendly memory access of model weights with irregular bit-width and\n(2) high runtime overhead of weight de-quantization. To address these problems,\nwe propose TC-FPx, the first full-stack GPU kernel design scheme with unified\nTensor Core support of float-point weights for various quantization bit-width.\nWe integrate TC-FPx kernel into an existing inference system, providing new\nend-to-end support (called FP6-LLM) for quantized LLM inference, where better\ntrade-offs between inference cost and model quality are achieved. Experiments\nshow that FP6-LLM enables the inference of LLaMA-70b using only a single GPU,\nachieving 1.69x-2.65x higher normalized inference throughput than the FP16\nbaseline. The source code is publicly available at\nhttps://github.com/usyd-fsalab/fp6_llm.",
        "publication_date": "2024-01-25T11:46:38Z",
        "upvotes": 14
    },
    "2401.14405": {
        "url": "https://arxiv.org/abs/2401.14405",
        "title": "Multimodal Pathway: Improve Transformers with Irrelevant Data from Other\n  Modalities",
        "authors": [
            "Yiyuan Zhang",
            "Xiaohan Ding",
            "Kaixiong Gong",
            "Yixiao Ge",
            "Ying Shan",
            "Xiangyu Yue"
        ],
        "abstract": "We propose to improve transformers of a specific modality with irrelevant\ndata from other modalities, e.g., improve an ImageNet model with audio or point\ncloud datasets. We would like to highlight that the data samples of the target\nmodality are irrelevant to the other modalities, which distinguishes our method\nfrom other works utilizing paired (e.g., CLIP) or interleaved data of different\nmodalities. We propose a methodology named Multimodal Pathway - given a target\nmodality and a transformer designed for it, we use an auxiliary transformer\ntrained with data of another modality and construct pathways to connect\ncomponents of the two models so that data of the target modality can be\nprocessed by both models. In this way, we utilize the universal\nsequence-to-sequence modeling abilities of transformers obtained from two\nmodalities. As a concrete implementation, we use a modality-specific tokenizer\nand task-specific head as usual but utilize the transformer blocks of the\nauxiliary model via a proposed method named Cross-Modal Re-parameterization,\nwhich exploits the auxiliary weights without any inference costs. On the image,\npoint cloud, video, and audio recognition tasks, we observe significant and\nconsistent performance improvements with irrelevant data from other modalities.\nThe code and models are available at https://github.com/AILab-CVC/M2PT.",
        "publication_date": "2024-01-25T18:59:58Z",
        "upvotes": 11
    },
    "2401.13974": {
        "url": "https://arxiv.org/abs/2401.13974",
        "title": "BootPIG: Bootstrapping Zero-shot Personalized Image Generation\n  Capabilities in Pretrained Diffusion Models",
        "authors": [
            "Senthil Purushwalkam",
            "Akash Gokul",
            "Shafiq Joty",
            "Nikhil Naik"
        ],
        "abstract": "Recent text-to-image generation models have demonstrated incredible success\nin generating images that faithfully follow input prompts. However, the\nrequirement of using words to describe a desired concept provides limited\ncontrol over the appearance of the generated concepts. In this work, we address\nthis shortcoming by proposing an approach to enable personalization\ncapabilities in existing text-to-image diffusion models. We propose a novel\narchitecture (BootPIG) that allows a user to provide reference images of an\nobject in order to guide the appearance of a concept in the generated images.\n  The proposed BootPIG architecture makes minimal modifications to a pretrained\ntext-to-image diffusion model and utilizes a separate UNet model to steer the\ngenerations toward the desired appearance. We introduce a training procedure\nthat allows us to bootstrap personalization capabilities in the BootPIG\narchitecture using data generated from pretrained text-to-image models, LLM\nchat agents, and image segmentation models. In contrast to existing methods\nthat require several days of pretraining, the BootPIG architecture can be\ntrained in approximately 1 hour. Experiments on the DreamBooth dataset\ndemonstrate that BootPIG outperforms existing zero-shot methods while being\ncomparable with test-time finetuning approaches. Through a user study, we\nvalidate the preference for BootPIG generations over existing methods both in\nmaintaining fidelity to the reference object's appearance and aligning with\ntextual prompts.",
        "publication_date": "2024-01-25T06:18:20Z",
        "upvotes": 11
    },
    "2401.14257": {
        "url": "https://arxiv.org/abs/2401.14257",
        "title": "Sketch2NeRF: Multi-view Sketch-guided Text-to-3D Generation",
        "authors": [
            "Minglin Chen",
            "Weihao Yuan",
            "Yukun Wang",
            "Zhe Sheng",
            "Yisheng He",
            "Zilong Dong",
            "Liefeng Bo",
            "Yulan Guo"
        ],
        "abstract": "Recently, text-to-3D approaches have achieved high-fidelity 3D content\ngeneration using text description. However, the generated objects are\nstochastic and lack fine-grained control. Sketches provide a cheap approach to\nintroduce such fine-grained control. Nevertheless, it is challenging to achieve\nflexible control from these sketches due to their abstraction and ambiguity. In\nthis paper, we present a multi-view sketch-guided text-to-3D generation\nframework (namely, Sketch2NeRF) to add sketch control to 3D generation.\nSpecifically, our method leverages pretrained 2D diffusion models (e.g., Stable\nDiffusion and ControlNet) to supervise the optimization of a 3D scene\nrepresented by a neural radiance field (NeRF). We propose a novel synchronized\ngeneration and reconstruction method to effectively optimize the NeRF. In the\nexperiments, we collected two kinds of multi-view sketch datasets to evaluate\nthe proposed method. We demonstrate that our method can synthesize 3D\nconsistent contents with fine-grained sketch control while being high-fidelity\nto text prompts. Extensive results show that our method achieves\nstate-of-the-art performance in terms of sketch similarity and text alignment.",
        "publication_date": "2024-01-25T15:49:12Z",
        "upvotes": 9
    },
    "2401.14403": {
        "url": "https://arxiv.org/abs/2401.14403",
        "title": "Adaptive Mobile Manipulation for Articulated Objects In the Open World",
        "authors": [
            "Haoyu Xiong",
            "Russell Mendonca",
            "Kenneth Shaw",
            "Deepak Pathak"
        ],
        "abstract": "Deploying robots in open-ended unstructured environments such as homes has\nbeen a long-standing research problem. However, robots are often studied only\nin closed-off lab settings, and prior mobile manipulation work is restricted to\npick-move-place, which is arguably just the tip of the iceberg in this area. In\nthis paper, we introduce Open-World Mobile Manipulation System, a full-stack\napproach to tackle realistic articulated object operation, e.g. real-world\ndoors, cabinets, drawers, and refrigerators in open-ended unstructured\nenvironments. The robot utilizes an adaptive learning framework to initially\nlearns from a small set of data through behavior cloning, followed by learning\nfrom online practice on novel objects that fall outside the training\ndistribution. We also develop a low-cost mobile manipulation hardware platform\ncapable of safe and autonomous online adaptation in unstructured environments\nwith a cost of around 20,000 USD. In our experiments we utilize 20 articulate\nobjects across 4 buildings in the CMU campus. With less than an hour of online\nlearning for each object, the system is able to increase success rate from 50%\nof BC pre-training to 95% using online adaptation. Video results at\nhttps://open-world-mobilemanip.github.io/",
        "publication_date": "2024-01-25T18:59:44Z",
        "upvotes": 8
    },
    "2401.14398": {
        "url": "https://arxiv.org/abs/2401.14398",
        "title": "pix2gestalt: Amodal Segmentation by Synthesizing Wholes",
        "authors": [
            "Ege Ozguroglu",
            "Ruoshi Liu",
            "D\u00eddac Sur\u00eds",
            "Dian Chen",
            "Achal Dave",
            "Pavel Tokmakov",
            "Carl Vondrick"
        ],
        "abstract": "We introduce pix2gestalt, a framework for zero-shot amodal segmentation,\nwhich learns to estimate the shape and appearance of whole objects that are\nonly partially visible behind occlusions. By capitalizing on large-scale\ndiffusion models and transferring their representations to this task, we learn\na conditional diffusion model for reconstructing whole objects in challenging\nzero-shot cases, including examples that break natural and physical priors,\nsuch as art. As training data, we use a synthetically curated dataset\ncontaining occluded objects paired with their whole counterparts. Experiments\nshow that our approach outperforms supervised baselines on established\nbenchmarks. Our model can furthermore be used to significantly improve the\nperformance of existing object recognition and 3D reconstruction methods in the\npresence of occlusions.",
        "publication_date": "2024-01-25T18:57:36Z",
        "upvotes": 8
    },
    "2401.14066": {
        "url": "https://arxiv.org/abs/2401.14066",
        "title": "CreativeSynth: Creative Blending and Synthesis of Visual Arts based on\n  Multimodal Diffusion",
        "authors": [
            "Nisha Huang",
            "Weiming Dong",
            "Yuxin Zhang",
            "Fan Tang",
            "Ronghui Li",
            "Chongyang Ma",
            "Xiu Li",
            "Changsheng Xu"
        ],
        "abstract": "Large-scale text-to-image generative models have made impressive strides,\nshowcasing their ability to synthesize a vast array of high-quality images.\nHowever, adapting these models for artistic image editing presents two\nsignificant challenges. Firstly, users struggle to craft textual prompts that\nmeticulously detail visual elements of the input image. Secondly, prevalent\nmodels, when effecting modifications in specific zones, frequently disrupt the\noverall artistic style, complicating the attainment of cohesive and\naesthetically unified artworks. To surmount these obstacles, we build the\ninnovative unified framework CreativeSynth, which is based on a diffusion model\nwith the ability to coordinate multimodal inputs and multitask in the field of\nartistic image generation. By integrating multimodal features with customized\nattention mechanisms, CreativeSynth facilitates the importation of real-world\nsemantic content into the domain of art through inversion and real-time style\ntransfer. This allows for the precise manipulation of image style and content\nwhile maintaining the integrity of the original model parameters. Rigorous\nqualitative and quantitative evaluations underscore that CreativeSynth excels\nin enhancing artistic images' fidelity and preserves their innate aesthetic\nessence. By bridging the gap between generative models and artistic finesse,\nCreativeSynth becomes a custom digital palette.",
        "publication_date": "2024-01-25T10:42:09Z",
        "upvotes": 7
    },
    "2401.14367": {
        "url": "https://arxiv.org/abs/2401.14367",
        "title": "Genie: Achieving Human Parity in Content-Grounded Datasets Generation",
        "authors": [
            "Asaf Yehudai",
            "Boaz Carmeli",
            "Yosi Mass",
            "Ofir Arviv",
            "Nathaniel Mills",
            "Assaf Toledo",
            "Eyal Shnarch",
            "Leshem Choshen"
        ],
        "abstract": "The lack of high-quality data for content-grounded generation tasks has been\nidentified as a major obstacle to advancing these tasks. To address this gap,\nwe propose Genie, a novel method for automatically generating high-quality\ncontent-grounded data. It consists of three stages: (a) Content Preparation,\n(b) Generation: creating task-specific examples from the content (e.g.,\nquestion-answer pairs or summaries). (c) Filtering mechanism aiming to ensure\nthe quality and faithfulness of the generated data. We showcase this\nmethodology by generating three large-scale synthetic data, making wishes, for\nLong-Form Question-Answering (LFQA), summarization, and information extraction.\nIn a human evaluation, our generated data was found to be natural and of high\nquality. Furthermore, we compare models trained on our data with models trained\non human-written data -- ELI5 and ASQA for LFQA and CNN-DailyMail for\nSummarization. We show that our models are on par with or outperforming models\ntrained on human-generated data and consistently outperforming them in\nfaithfulness. Finally, we applied our method to create LFQA data within the\nmedical domain and compared a model trained on it with models trained on other\ndomains.",
        "publication_date": "2024-01-25T18:14:57Z",
        "upvotes": 6
    },
    "2401.15024": {
        "url": "https://arxiv.org/abs/2401.15024",
        "title": "SliceGPT: Compress Large Language Models by Deleting Rows and Columns",
        "authors": [
            "Saleh Ashkboos",
            "Maximilian L. Croci",
            "Marcelo Gennari do Nascimento",
            "Torsten Hoefler",
            "James Hensman"
        ],
        "abstract": "Large language models have become the cornerstone of natural language\nprocessing, but their use comes with substantial costs in terms of compute and\nmemory resources. Sparsification provides a solution to alleviate these\nresource constraints, and recent works have shown that trained models can be\nsparsified post-hoc. Existing sparsification techniques face challenges as they\nneed additional data structures and offer constrained speedup with current\nhardware. In this paper we present SliceGPT, a new post-training sparsification\nscheme which replaces each weight matrix with a smaller (dense) matrix,\nreducing the embedding dimension of the network. Through extensive\nexperimentation, we show that SliceGPT can remove up to 25% of the model\nparameters (including embeddings) for LLAMA2-70B, OPT 66B and Phi-2 models\nwhile maintaining 99%, 99% and 90% zero-shot task performance of the dense\nmodel respectively. Our sliced models run on fewer GPUs and run faster without\nany additional code optimization: on 24GB consumer GPUs we reduce the total\ncompute for inference on LLAMA2-70B to 64% of that of the dense model; on 40GB\nA100 GPUs we reduce it to 66%. We offer a new insight, computational invariance\nin transformer networks, which enables SliceGPT and we hope it will inspire and\nenable future avenues to reduce memory and computation demands for pre-trained\nmodels. Code is available at:\nhttps://github.com/microsoft/TransformerCompression",
        "publication_date": "2024-01-26T17:35:45Z",
        "upvotes": 62
    },
    "2401.15071": {
        "url": "https://arxiv.org/abs/2401.15071",
        "title": "From GPT-4 to Gemini and Beyond: Assessing the Landscape of MLLMs on\n  Generalizability, Trustworthiness and Causality through Four Modalities",
        "authors": [
            "Chaochao Lu",
            "Chen Qian",
            "Guodong Zheng",
            "Hongxing Fan",
            "Hongzhi Gao",
            "Jie Zhang",
            "Jing Shao",
            "Jingyi Deng",
            "Jinlan Fu",
            "Kexin Huang",
            "Kunchang Li",
            "Lijun Li",
            "Limin Wang",
            "Lu Sheng",
            "Meiqi Chen",
            "Ming Zhang",
            "Qibing Ren",
            "Sirui Chen",
            "Tao Gui",
            "Wanli Ouyang",
            "Yali Wang",
            "Yan Teng",
            "Yaru Wang",
            "Yi Wang",
            "Yinan He",
            "Yingchun Wang",
            "Yixu Wang",
            "Yongting Zhang",
            "Yu Qiao",
            "Yujiong Shen",
            "Yurong Mou",
            "Yuxi Chen",
            "Zaibin Zhang",
            "Zhelun Shi",
            "Zhenfei Yin",
            "Zhipin Wang"
        ],
        "abstract": "Multi-modal Large Language Models (MLLMs) have shown impressive abilities in\ngenerating reasonable responses with respect to multi-modal contents. However,\nthere is still a wide gap between the performance of recent MLLM-based\napplications and the expectation of the broad public, even though the most\npowerful OpenAI's GPT-4 and Google's Gemini have been deployed. This paper\nstrives to enhance understanding of the gap through the lens of a qualitative\nstudy on the generalizability, trustworthiness, and causal reasoning\ncapabilities of recent proprietary and open-source MLLMs across four\nmodalities: ie, text, code, image, and video, ultimately aiming to improve the\ntransparency of MLLMs. We believe these properties are several representative\nfactors that define the reliability of MLLMs, in supporting various downstream\napplications. To be specific, we evaluate the closed-source GPT-4 and Gemini\nand 6 open-source LLMs and MLLMs. Overall we evaluate 230 manually designed\ncases, where the qualitative results are then summarized into 12 scores (ie, 4\nmodalities times 3 properties). In total, we uncover 14 empirical findings that\nare useful to understand the capabilities and limitations of both proprietary\nand open-source MLLMs, towards more reliable downstream multi-modal\napplications.",
        "publication_date": "2024-01-26T18:53:03Z",
        "upvotes": 33
    },
    "2401.14953": {
        "url": "https://arxiv.org/abs/2401.14953",
        "title": "Learning Universal Predictors",
        "authors": [
            "Jordi Grau-Moya",
            "Tim Genewein",
            "Marcus Hutter",
            "Laurent Orseau",
            "Gr\u00e9goire Del\u00e9tang",
            "Elliot Catt",
            "Anian Ruoss",
            "Li Kevin Wenliang",
            "Christopher Mattern",
            "Matthew Aitchison",
            "Joel Veness"
        ],
        "abstract": "Meta-learning has emerged as a powerful approach to train neural networks to\nlearn new tasks quickly from limited data. Broad exposure to different tasks\nleads to versatile representations enabling general problem solving. But, what\nare the limits of meta-learning? In this work, we explore the potential of\namortizing the most powerful universal predictor, namely Solomonoff Induction\n(SI), into neural networks via leveraging meta-learning to its limits. We use\nUniversal Turing Machines (UTMs) to generate training data used to expose\nnetworks to a broad range of patterns. We provide theoretical analysis of the\nUTM data generation processes and meta-training protocols. We conduct\ncomprehensive experiments with neural architectures (e.g. LSTMs, Transformers)\nand algorithmic data generators of varying complexity and universality. Our\nresults suggest that UTM data is a valuable resource for meta-learning, and\nthat it can be used to train neural networks capable of learning universal\nprediction strategies.",
        "publication_date": "2024-01-26T15:37:16Z",
        "upvotes": 18
    },
    "2401.15077": {
        "url": "https://arxiv.org/abs/2401.15077",
        "title": "EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty",
        "authors": [
            "Yuhui Li",
            "Fangyun Wei",
            "Chao Zhang",
            "Hongyang Zhang"
        ],
        "abstract": "Autoregressive decoding makes the inference of Large Language Models (LLMs)\ntime-consuming. In this paper, we reconsider speculative sampling and derive\ntwo key observations. Firstly, autoregression at the feature\n(second-to-top-layer) level is more straightforward than at the token level.\nSecondly, the inherent uncertainty in feature (second-to-top-layer) level\nautoregression constrains its performance. Based on these insights, we\nintroduce EAGLE (Extrapolation Algorithm for Greater Language-model\nEfficiency), a simple yet highly efficient speculative sampling framework. By\nincorporating a token sequence advanced by one time step, EAGLE effectively\nresolves the uncertainty, enabling precise second-to-top-layer feature\nprediction with minimal overhead. We conducted comprehensive evaluations of\nEAGLE, including all models from the Vicuna and LLaMA2-Chat series, the MoE\nmodel Mixtral 8x7B Instruct, and tasks in dialogue, code generation,\nmathematical reasoning, and instruction following. For LLaMA2-Chat 70B, EAGLE\nachieved a latency speedup ratio of 2.7x-3.5x, doubled throughput, while\nmaintaining the distribution of the generated text.",
        "publication_date": "2024-01-26T18:59:01Z",
        "upvotes": 17
    },
    "2401.14688": {
        "url": "https://arxiv.org/abs/2401.14688",
        "title": "Taiyi-Diffusion-XL: Advancing Bilingual Text-to-Image Generation with\n  Large Vision-Language Model Support",
        "authors": [
            "Xiaojun Wu",
            "Dixiang Zhang",
            "Ruyi Gan",
            "Junyu Lu",
            "Ziwei Wu",
            "Renliang Sun",
            "Jiaxing Zhang",
            "Pingjian Zhang",
            "Yan Song"
        ],
        "abstract": "Recent advancements in text-to-image models have significantly enhanced image\ngeneration capabilities, yet a notable gap of open-source models persists in\nbilingual or Chinese language support. To address this need, we present\nTaiyi-Diffusion-XL, a new Chinese and English bilingual text-to-image model\nwhich is developed by extending the capabilities of CLIP and\nStable-Diffusion-XL through a process of bilingual continuous pre-training.\nThis approach includes the efficient expansion of vocabulary by integrating the\nmost frequently used Chinese characters into CLIP's tokenizer and embedding\nlayers, coupled with an absolute position encoding expansion. Additionally, we\nenrich text prompts by large vision-language model, leading to better images\ncaptions and possess higher visual quality. These enhancements are subsequently\napplied to downstream text-to-image models. Our empirical results indicate that\nthe developed CLIP model excels in bilingual image-text retrieval.Furthermore,\nthe bilingual image generation capabilities of Taiyi-Diffusion-XL surpass\nprevious models. This research leads to the development and open-sourcing of\nthe Taiyi-Diffusion-XL model, representing a notable advancement in the field\nof image generation, particularly for Chinese language applications. This\ncontribution is a step forward in addressing the need for more diverse language\nsupport in multimodal research. The model and demonstration are made publicly\navailable at\n\\href{https://huggingface.co/IDEA-CCNL/Taiyi-Stable-Diffusion-XL-3.5B/}{this\nhttps URL}, fostering further research and collaboration in this domain.",
        "publication_date": "2024-01-26T07:17:50Z",
        "upvotes": 11
    },
    "2401.14673": {
        "url": "https://arxiv.org/abs/2401.14673",
        "title": "Generative Expressive Robot Behaviors using Large Language Models",
        "authors": [
            "Karthik Mahadevan",
            "Jonathan Chien",
            "Noah Brown",
            "Zhuo Xu",
            "Carolina Parada",
            "Fei Xia",
            "Andy Zeng",
            "Leila Takayama",
            "Dorsa Sadigh"
        ],
        "abstract": "People employ expressive behaviors to effectively communicate and coordinate\ntheir actions with others, such as nodding to acknowledge a person glancing at\nthem or saying \"excuse me\" to pass people in a busy corridor. We would like\nrobots to also demonstrate expressive behaviors in human-robot interaction.\nPrior work proposes rule-based methods that struggle to scale to new\ncommunication modalities or social situations, while data-driven methods\nrequire specialized datasets for each social situation the robot is used in. We\npropose to leverage the rich social context available from large language\nmodels (LLMs) and their ability to generate motion based on instructions or\nuser preferences, to generate expressive robot motion that is adaptable and\ncomposable, building upon each other. Our approach utilizes few-shot\nchain-of-thought prompting to translate human language instructions into\nparametrized control code using the robot's available and learned skills.\nThrough user studies and simulation experiments, we demonstrate that our\napproach produces behaviors that users found to be competent and easy to\nunderstand. Supplementary material can be found at\nhttps://generative-expressive-motion.github.io/.",
        "publication_date": "2024-01-26T06:34:32Z",
        "upvotes": 4
    },
    "2401.14828": {
        "url": "https://arxiv.org/abs/2401.14828",
        "title": "TIP-Editor: An Accurate 3D Editor Following Both Text-Prompts And\n  Image-Prompts",
        "authors": [
            "Jingyu Zhuang",
            "Di Kang",
            "Yan-Pei Cao",
            "Guanbin Li",
            "Liang Lin",
            "Ying Shan"
        ],
        "abstract": "Text-driven 3D scene editing has gained significant attention owing to its\nconvenience and user-friendliness. However, existing methods still lack\naccurate control of the specified appearance and location of the editing result\ndue to the inherent limitations of the text description. To this end, we\npropose a 3D scene editing framework, TIPEditor, that accepts both text and\nimage prompts and a 3D bounding box to specify the editing region. With the\nimage prompt, users can conveniently specify the detailed appearance/style of\nthe target content in complement to the text description, enabling accurate\ncontrol of the appearance. Specifically, TIP-Editor employs a stepwise 2D\npersonalization strategy to better learn the representation of the existing\nscene and the reference image, in which a localization loss is proposed to\nencourage correct object placement as specified by the bounding box.\nAdditionally, TIPEditor utilizes explicit and flexible 3D Gaussian splatting as\nthe 3D representation to facilitate local editing while keeping the background\nunchanged. Extensive experiments have demonstrated that TIP-Editor conducts\naccurate editing following the text and image prompts in the specified bounding\nbox region, consistently outperforming the baselines in editing quality, and\nthe alignment to the prompts, qualitatively and quantitatively.",
        "publication_date": "2024-01-26T12:57:05Z",
        "upvotes": 4
    },
    "2401.16420": {
        "url": "https://arxiv.org/abs/2401.16420",
        "title": "InternLM-XComposer2: Mastering Free-form Text-Image Composition and\n  Comprehension in Vision-Language Large Model",
        "authors": [
            "Xiaoyi Dong",
            "Pan Zhang",
            "Yuhang Zang",
            "Yuhang Cao",
            "Bin Wang",
            "Linke Ouyang",
            "Xilin Wei",
            "Songyang Zhang",
            "Haodong Duan",
            "Maosong Cao",
            "Wenwei Zhang",
            "Yining Li",
            "Hang Yan",
            "Yang Gao",
            "Xinyue Zhang",
            "Wei Li",
            "Jingwen Li",
            "Kai Chen",
            "Conghui He",
            "Xingcheng Zhang",
            "Yu Qiao",
            "Dahua Lin",
            "Jiaqi Wang"
        ],
        "abstract": "We introduce InternLM-XComposer2, a cutting-edge vision-language model\nexcelling in free-form text-image composition and comprehension. This model\ngoes beyond conventional vision-language understanding, adeptly crafting\ninterleaved text-image content from diverse inputs like outlines, detailed\ntextual specifications, and reference images, enabling highly customizable\ncontent creation. InternLM-XComposer2 proposes a Partial LoRA (PLoRA) approach\nthat applies additional LoRA parameters exclusively to image tokens to preserve\nthe integrity of pre-trained language knowledge, striking a balance between\nprecise vision understanding and text composition with literary talent.\nExperimental results demonstrate the superiority of InternLM-XComposer2 based\non InternLM2-7B in producing high-quality long-text multi-modal content and its\nexceptional vision-language understanding performance across various\nbenchmarks, where it not only significantly outperforms existing multimodal\nmodels but also matches or even surpasses GPT-4V and Gemini Pro in certain\nassessments. This highlights its remarkable proficiency in the realm of\nmultimodal understanding. The InternLM-XComposer2 model series with 7B\nparameters are publicly available at\nhttps://github.com/InternLM/InternLM-XComposer.",
        "publication_date": "2024-01-29T18:59:02Z",
        "upvotes": 53
    },
    "2401.15947": {
        "url": "https://arxiv.org/abs/2401.15947",
        "title": "MoE-LLaVA: Mixture of Experts for Large Vision-Language Models",
        "authors": [
            "Bin Lin",
            "Zhenyu Tang",
            "Yang Ye",
            "Jiaxi Cui",
            "Bin Zhu",
            "Peng Jin",
            "Jinfa Huang",
            "Junwu Zhang",
            "Munan Ning",
            "Li Yuan"
        ],
        "abstract": "Recent advances demonstrate that scaling Large Vision-Language Models (LVLMs)\neffectively improves downstream task performances. However, existing scaling\nmethods enable all model parameters to be active for each token in the\ncalculation, which brings massive training and inferring costs. In this work,\nwe propose a simple yet effective training strategy MoE-Tuning for LVLMs. This\nstrategy innovatively addresses the common issue of performance degradation in\nmulti-modal sparsity learning, consequently constructing a sparse model with an\noutrageous number of parameters but a constant computational cost. Furthermore,\nwe present the MoE-LLaVA, a MoE-based sparse LVLM architecture, which uniquely\nactivates only the top-k experts through routers during deployment, keeping the\nremaining experts inactive. Extensive experiments show the significant\nperformance of MoE-LLaVA in a variety of visual understanding and object\nhallucination benchmarks. Remarkably, with only approximately 3B sparsely\nactivated parameters, MoE-LLaVA demonstrates performance comparable to the\nLLaVA-1.5-7B on various visual understanding datasets and even surpasses the\nLLaVA-1.5-13B in object hallucination benchmark. Through MoE-LLaVA, we aim to\nestablish a baseline for sparse LVLMs and provide valuable insights for future\nresearch in developing more efficient and effective multi-modal learning\nsystems. Code is released at https://github.com/PKU-YuanGroup/MoE-LLaVA.",
        "publication_date": "2024-01-29T08:13:40Z",
        "upvotes": 46
    },
    "2401.16380": {
        "url": "https://arxiv.org/abs/2401.16380",
        "title": "Rephrasing the Web: A Recipe for Compute and Data-Efficient Language\n  Modeling",
        "authors": [
            "Pratyush Maini",
            "Skyler Seto",
            "He Bai",
            "David Grangier",
            "Yizhe Zhang",
            "Navdeep Jaitly"
        ],
        "abstract": "Large language models are trained on massive scrapes of the web, which are\noften unstructured, noisy, and poorly phrased. Current scaling laws show that\nlearning from such data requires an abundance of both compute and data, which\ngrows with the size of the model being trained. This is infeasible both because\nof the large compute costs and duration associated with pre-training, and the\nimpending scarcity of high-quality data on the web. In this work, we propose\nWeb Rephrase Augmented Pre-training ($\\textbf{WRAP}$) that uses an\noff-the-shelf instruction-tuned model prompted to paraphrase documents on the\nweb in specific styles such as \"like Wikipedia\" or in \"question-answer format\"\nto jointly pre-train LLMs on real and synthetic rephrases. First, we show that\nusing WRAP on the C4 dataset, which is naturally noisy, speeds up pre-training\nby $\\sim3x$. At the same pre-training compute budget, it improves perplexity by\nmore than 10% on average across different subsets of the Pile, and improves\nzero-shot question answer accuracy across 13 tasks by more than 2%. Second, we\ninvestigate the impact of the re-phrasing style on the performance of the\nmodel, offering insights into how the composition of the training data can\nimpact the performance of LLMs in OOD settings. Our gains are attributed to the\nfact that re-phrased synthetic data has higher utility than just real data\nbecause it (i) incorporates style diversity that closely reflects downstream\nevaluation style, and (ii) has higher 'quality' than web-scraped data.",
        "publication_date": "2024-01-29T18:19:08Z",
        "upvotes": 43
    },
    "2401.15977": {
        "url": "https://arxiv.org/abs/2401.15977",
        "title": "Motion-I2V: Consistent and Controllable Image-to-Video Generation with\n  Explicit Motion Modeling",
        "authors": [
            "Xiaoyu Shi",
            "Zhaoyang Huang",
            "Fu-Yun Wang",
            "Weikang Bian",
            "Dasong Li",
            "Yi Zhang",
            "Manyuan Zhang",
            "Ka Chun Cheung",
            "Simon See",
            "Hongwei Qin",
            "Jifeng Dai",
            "Hongsheng Li"
        ],
        "abstract": "We introduce Motion-I2V, a novel framework for consistent and controllable\nimage-to-video generation (I2V). In contrast to previous methods that directly\nlearn the complicated image-to-video mapping, Motion-I2V factorizes I2V into\ntwo stages with explicit motion modeling. For the first stage, we propose a\ndiffusion-based motion field predictor, which focuses on deducing the\ntrajectories of the reference image's pixels. For the second stage, we propose\nmotion-augmented temporal attention to enhance the limited 1-D temporal\nattention in video latent diffusion models. This module can effectively\npropagate reference image's feature to synthesized frames with the guidance of\npredicted trajectories from the first stage. Compared with existing methods,\nMotion-I2V can generate more consistent videos even at the presence of large\nmotion and viewpoint variation. By training a sparse trajectory ControlNet for\nthe first stage, Motion-I2V can support users to precisely control motion\ntrajectories and motion regions with sparse trajectory and region annotations.\nThis offers more controllability of the I2V process than solely relying on\ntextual instructions. Additionally, Motion-I2V's second stage naturally\nsupports zero-shot video-to-video translation. Both qualitative and\nquantitative comparisons demonstrate the advantages of Motion-I2V over prior\napproaches in consistent and controllable image-to-video generation. Please see\nour project page at https://xiaoyushi97.github.io/Motion-I2V/.",
        "publication_date": "2024-01-29T09:06:43Z",
        "upvotes": 30
    },
    "2401.15687": {
        "url": "https://arxiv.org/abs/2401.15687",
        "title": "Media2Face: Co-speech Facial Animation Generation With Multi-Modality\n  Guidance",
        "authors": [
            "Qingcheng Zhao",
            "Pengyu Long",
            "Qixuan Zhang",
            "Dafei Qin",
            "Han Liang",
            "Longwen Zhang",
            "Yingliang Zhang",
            "Jingyi Yu",
            "Lan Xu"
        ],
        "abstract": "The synthesis of 3D facial animations from speech has garnered considerable\nattention. Due to the scarcity of high-quality 4D facial data and\nwell-annotated abundant multi-modality labels, previous methods often suffer\nfrom limited realism and a lack of lexible conditioning. We address this\nchallenge through a trilogy. We first introduce Generalized Neural Parametric\nFacial Asset (GNPFA), an efficient variational auto-encoder mapping facial\ngeometry and images to a highly generalized expression latent space, decoupling\nexpressions and identities. Then, we utilize GNPFA to extract high-quality\nexpressions and accurate head poses from a large array of videos. This presents\nthe M2F-D dataset, a large, diverse, and scan-level co-speech 3D facial\nanimation dataset with well-annotated emotional and style labels. Finally, we\npropose Media2Face, a diffusion model in GNPFA latent space for co-speech\nfacial animation generation, accepting rich multi-modality guidances from\naudio, text, and image. Extensive experiments demonstrate that our model not\nonly achieves high fidelity in facial animation synthesis but also broadens the\nscope of expressiveness and style adaptability in 3D facial animation.",
        "publication_date": "2024-01-28T16:17:59Z",
        "upvotes": 19
    },
    "2401.16013": {
        "url": "https://arxiv.org/abs/2401.16013",
        "title": "SERL: A Software Suite for Sample-Efficient Robotic Reinforcement\n  Learning",
        "authors": [
            "Jianlan Luo",
            "Zheyuan Hu",
            "Charles Xu",
            "You Liang Tan",
            "Jacob Berg",
            "Archit Sharma",
            "Stefan Schaal",
            "Chelsea Finn",
            "Abhishek Gupta",
            "Sergey Levine"
        ],
        "abstract": "In recent years, significant progress has been made in the field of robotic\nreinforcement learning (RL), enabling methods that handle complex image\nobservations, train in the real world, and incorporate auxiliary data, such as\ndemonstrations and prior experience. However, despite these advances, robotic\nRL remains hard to use. It is acknowledged among practitioners that the\nparticular implementation details of these algorithms are often just as\nimportant (if not more so) for performance as the choice of algorithm. We posit\nthat a significant challenge to widespread adoption of robotic RL, as well as\nfurther development of robotic RL methods, is the comparative inaccessibility\nof such methods. To address this challenge, we developed a carefully\nimplemented library containing a sample efficient off-policy deep RL method,\ntogether with methods for computing rewards and resetting the environment, a\nhigh-quality controller for a widely-adopted robot, and a number of challenging\nexample tasks. We provide this library as a resource for the community,\ndescribe its design choices, and present experimental results. Perhaps\nsurprisingly, we find that our implementation can achieve very efficient\nlearning, acquiring policies for PCB board assembly, cable routing, and object\nrelocation between 25 to 50 minutes of training per policy on average,\nimproving over state-of-the-art results reported for similar tasks in the\nliterature. These policies achieve perfect or near-perfect success rates,\nextreme robustness even under perturbations, and exhibit emergent recovery and\ncorrection behaviors. We hope that these promising results and our high-quality\nopen-source implementation will provide a tool for the robotics community to\nfacilitate further developments in robotic RL. Our code, documentation, and\nvideos can be found at https://serl-robot.github.io/",
        "publication_date": "2024-01-29T10:01:10Z",
        "upvotes": 16
    },
    "2401.15975": {
        "url": "https://arxiv.org/abs/2401.15975",
        "title": "StableIdentity: Inserting Anybody into Anywhere at First Sight",
        "authors": [
            "Qinghe Wang",
            "Xu Jia",
            "Xiaomin Li",
            "Taiqing Li",
            "Liqian Ma",
            "Yunzhi Zhuge",
            "Huchuan Lu"
        ],
        "abstract": "Recent advances in large pretrained text-to-image models have shown\nunprecedented capabilities for high-quality human-centric generation, however,\ncustomizing face identity is still an intractable problem. Existing methods\ncannot ensure stable identity preservation and flexible editability, even with\nseveral images for each subject during training. In this work, we propose\nStableIdentity, which allows identity-consistent recontextualization with just\none face image. More specifically, we employ a face encoder with an identity\nprior to encode the input face, and then land the face representation into a\nspace with an editable prior, which is constructed from celeb names. By\nincorporating identity prior and editability prior, the learned identity can be\ninjected anywhere with various contexts. In addition, we design a masked\ntwo-phase diffusion loss to boost the pixel-level perception of the input face\nand maintain the diversity of generation. Extensive experiments demonstrate our\nmethod outperforms previous customization methods. In addition, the learned\nidentity can be flexibly combined with the off-the-shelf modules such as\nControlNet. Notably, to the best knowledge, we are the first to directly inject\nthe identity learned from a single image into video/3D generation without\nfinetuning. We believe that the proposed StableIdentity is an important step to\nunify image, video, and 3D customized generation models.",
        "publication_date": "2024-01-29T09:06:15Z",
        "upvotes": 16
    },
    "2401.16158": {
        "url": "https://arxiv.org/abs/2401.16158",
        "title": "Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual\n  Perception",
        "authors": [
            "Junyang Wang",
            "Haiyang Xu",
            "Jiabo Ye",
            "Ming Yan",
            "Weizhou Shen",
            "Ji Zhang",
            "Fei Huang",
            "Jitao Sang"
        ],
        "abstract": "Mobile device agent based on Multimodal Large Language Models (MLLM) is\nbecoming a popular application. In this paper, we introduce Mobile-Agent, an\nautonomous multi-modal mobile device agent. Mobile-Agent first leverages visual\nperception tools to accurately identify and locate both the visual and textual\nelements within the app's front-end interface. Based on the perceived vision\ncontext, it then autonomously plans and decomposes the complex operation task,\nand navigates the mobile Apps through operations step by step. Different from\nprevious solutions that rely on XML files of Apps or mobile system metadata,\nMobile-Agent allows for greater adaptability across diverse mobile operating\nenvironments in a vision-centric way, thereby eliminating the necessity for\nsystem-specific customizations. To assess the performance of Mobile-Agent, we\nintroduced Mobile-Eval, a benchmark for evaluating mobile device operations.\nBased on Mobile-Eval, we conducted a comprehensive evaluation of Mobile-Agent.\nThe experimental results indicate that Mobile-Agent achieved remarkable\naccuracy and completion rates. Even with challenging instructions, such as\nmulti-app operations, Mobile-Agent can still complete the requirements. Code\nand model will be open-sourced at https://github.com/X-PLUG/MobileAgent.",
        "publication_date": "2024-01-29T13:46:37Z",
        "upvotes": 15
    },
    "2401.15688": {
        "url": "https://arxiv.org/abs/2401.15688",
        "title": "Divide and Conquer: Language Models can Plan and Self-Correct for\n  Compositional Text-to-Image Generation",
        "authors": [
            "Zhenyu Wang",
            "Enze Xie",
            "Aoxue Li",
            "Zhongdao Wang",
            "Xihui Liu",
            "Zhenguo Li"
        ],
        "abstract": "Despite significant advancements in text-to-image models for generating\nhigh-quality images, these methods still struggle to ensure the controllability\nof text prompts over images in the context of complex text prompts, especially\nwhen it comes to retaining object attributes and relationships. In this paper,\nwe propose CompAgent, a training-free approach for compositional text-to-image\ngeneration, with a large language model (LLM) agent as its core. The\nfundamental idea underlying CompAgent is premised on a divide-and-conquer\nmethodology. Given a complex text prompt containing multiple concepts including\nobjects, attributes, and relationships, the LLM agent initially decomposes it,\nwhich entails the extraction of individual objects, their associated\nattributes, and the prediction of a coherent scene layout. These individual\nobjects can then be independently conquered. Subsequently, the agent performs\nreasoning by analyzing the text, plans and employs the tools to compose these\nisolated objects. The verification and human feedback mechanism is finally\nincorporated into our agent to further correct the potential attribute errors\nand refine the generated images. Guided by the LLM agent, we propose a\ntuning-free multi-concept customization model and a layout-to-image generation\nmodel as the tools for concept composition, and a local image editing method as\nthe tool to interact with the agent for verification. The scene layout controls\nthe image generation process among these tools to prevent confusion among\nmultiple objects. Extensive experiments demonstrate the superiority of our\napproach for compositional text-to-image generation: CompAgent achieves more\nthan 10\\% improvement on T2I-CompBench, a comprehensive benchmark for\nopen-world compositional T2I generation. The extension to various related tasks\nalso illustrates the flexibility of our CompAgent for potential applications.",
        "publication_date": "2024-01-28T16:18:39Z",
        "upvotes": 10
    },
    "2401.15708": {
        "url": "https://arxiv.org/abs/2401.15708",
        "title": "Object-Driven One-Shot Fine-tuning of Text-to-Image Diffusion with\n  Prototypical Embedding",
        "authors": [
            "Jianxiang Lu",
            "Cong Xie",
            "Hui Guo"
        ],
        "abstract": "As large-scale text-to-image generation models have made remarkable progress\nin the field of text-to-image generation, many fine-tuning methods have been\nproposed. However, these models often struggle with novel objects, especially\nwith one-shot scenarios. Our proposed method aims to address the challenges of\ngeneralizability and fidelity in an object-driven way, using only a single\ninput image and the object-specific regions of interest. To improve\ngeneralizability and mitigate overfitting, in our paradigm, a prototypical\nembedding is initialized based on the object's appearance and its class, before\nfine-tuning the diffusion model. And during fine-tuning, we propose a\nclass-characterizing regularization to preserve prior knowledge of object\nclasses. To further improve fidelity, we introduce object-specific loss, which\ncan also use to implant multiple objects. Overall, our proposed object-driven\nmethod for implanting new objects can integrate seamlessly with existing\nconcepts as well as with high fidelity and generalization. Our method\noutperforms several existing works. The code will be released.",
        "publication_date": "2024-01-28T17:11:42Z",
        "upvotes": 9
    },
    "2401.15914": {
        "url": "https://arxiv.org/abs/2401.15914",
        "title": "Overcoming the Pitfalls of Vision-Language Model Finetuning for OOD\n  Generalization",
        "authors": [
            "Yuhang Zang",
            "Hanlin Goh",
            "Josh Susskind",
            "Chen Huang"
        ],
        "abstract": "Existing vision-language models exhibit strong generalization on a variety of\nvisual domains and tasks. However, such models mainly perform zero-shot\nrecognition in a closed-set manner, and thus struggle to handle open-domain\nvisual concepts by design. There are recent finetuning methods, such as prompt\nlearning, that not only study the discrimination between in-distribution (ID)\nand out-of-distribution (OOD) samples, but also show some improvements in both\nID and OOD accuracies. In this paper, we first demonstrate that vision-language\nmodels, after long enough finetuning but without proper regularization, tend to\noverfit the known classes in the given dataset, with degraded performance on\nunknown classes. Then we propose a novel approach OGEN to address this pitfall,\nwith the main focus on improving the OOD GENeralization of finetuned models.\nSpecifically, a class-conditional feature generator is introduced to synthesize\nOOD features using just the class name of any unknown class. Such synthesized\nfeatures will provide useful knowledge about unknowns and help regularize the\ndecision boundary between ID and OOD data when optimized jointly. Equally\nimportant is our adaptive self-distillation mechanism to regularize our feature\ngeneration model during joint optimization, i.e., adaptively transferring\nknowledge between model states to further prevent overfitting. Experiments\nvalidate that our method yields convincing gains in OOD generalization\nperformance in different settings.",
        "publication_date": "2024-01-29T06:57:48Z",
        "upvotes": 5
    },
    "2401.17268": {
        "url": "https://arxiv.org/abs/2401.17268",
        "title": "Weaver: Foundation Models for Creative Writing",
        "authors": [
            "Tiannan Wang",
            "Jiamin Chen",
            "Qingrui Jia",
            "Shuai Wang",
            "Ruoyu Fang",
            "Huilin Wang",
            "Zhaowei Gao",
            "Chunzhao Xie",
            "Chuou Xu",
            "Jihong Dai",
            "Yibin Liu",
            "Jialong Wu",
            "Shengwei Ding",
            "Long Li",
            "Zhiwei Huang",
            "Xinle Deng",
            "Teng Yu",
            "Gangan Ma",
            "Han Xiao",
            "Zixin Chen",
            "Danjun Xiang",
            "Yunxia Wang",
            "Yuanyuan Zhu",
            "Yi Xiao",
            "Jing Wang",
            "Yiru Wang",
            "Siran Ding",
            "Jiayang Huang",
            "Jiayi Xu",
            "Yilihamu Tayier",
            "Zhenyu Hu",
            "Yuan Gao",
            "Chengfeng Zheng",
            "Yueshu Ye",
            "Yihang Li",
            "Lei Wan",
            "Xinyue Jiang",
            "Yujie Wang",
            "Siyu Cheng",
            "Zhule Song",
            "Xiangru Tang",
            "Xiaohua Xu",
            "Ningyu Zhang",
            "Huajun Chen",
            "Yuchen Eleanor Jiang",
            "Wangchunshu Zhou"
        ],
        "abstract": "This work introduces Weaver, our first family of large language models (LLMs)\ndedicated to content creation. Weaver is pre-trained on a carefully selected\ncorpus that focuses on improving the writing capabilities of large language\nmodels. We then fine-tune Weaver for creative and professional writing purposes\nand align it to the preference of professional writers using a suit of novel\nmethods for instruction data synthesis and LLM alignment, making it able to\nproduce more human-like texts and follow more diverse instructions for content\ncreation. The Weaver family consists of models of Weaver Mini (1.8B), Weaver\nBase (6B), Weaver Pro (14B), and Weaver Ultra (34B) sizes, suitable for\ndifferent applications and can be dynamically dispatched by a routing agent\naccording to query complexity to balance response quality and computation cost.\nEvaluation on a carefully curated benchmark for assessing the writing\ncapabilities of LLMs shows Weaver models of all sizes outperform generalist\nLLMs several times larger than them. Notably, our most-capable Weaver Ultra\nmodel surpasses GPT-4, a state-of-the-art generalist LLM, on various writing\nscenarios, demonstrating the advantage of training specialized LLMs for writing\npurposes. Moreover, Weaver natively supports retrieval-augmented generation\n(RAG) and function calling (tool usage). We present various use cases of these\nabilities for improving AI-assisted writing systems, including integration of\nexternal knowledge bases, tools, or APIs, and providing personalized writing\nassistance. Furthermore, we discuss and summarize a guideline and best\npractices for pre-training and fine-tuning domain-specific LLMs.",
        "publication_date": "2024-01-30T18:58:43Z",
        "upvotes": 39
    },
    "2401.17053": {
        "url": "https://arxiv.org/abs/2401.17053",
        "title": "BlockFusion: Expandable 3D Scene Generation using Latent Tri-plane\n  Extrapolation",
        "authors": [
            "Zhennan Wu",
            "Yang Li",
            "Han Yan",
            "Taizhang Shang",
            "Weixuan Sun",
            "Senbo Wang",
            "Ruikai Cui",
            "Weizhe Liu",
            "Hiroyuki Sato",
            "Hongdong Li",
            "Pan Ji"
        ],
        "abstract": "We present BlockFusion, a diffusion-based model that generates 3D scenes as\nunit blocks and seamlessly incorporates new blocks to extend the scene.\nBlockFusion is trained using datasets of 3D blocks that are randomly cropped\nfrom complete 3D scene meshes. Through per-block fitting, all training blocks\nare converted into the hybrid neural fields: with a tri-plane containing the\ngeometry features, followed by a Multi-layer Perceptron (MLP) for decoding the\nsigned distance values. A variational auto-encoder is employed to compress the\ntri-planes into the latent tri-plane space, on which the denoising diffusion\nprocess is performed. Diffusion applied to the latent representations allows\nfor high-quality and diverse 3D scene generation. To expand a scene during\ngeneration, one needs only to append empty blocks to overlap with the current\nscene and extrapolate existing latent tri-planes to populate new blocks. The\nextrapolation is done by conditioning the generation process with the feature\nsamples from the overlapping tri-planes during the denoising iterations. Latent\ntri-plane extrapolation produces semantically and geometrically meaningful\ntransitions that harmoniously blend with the existing scene. A 2D layout\nconditioning mechanism is used to control the placement and arrangement of\nscene elements. Experimental results indicate that BlockFusion is capable of\ngenerating diverse, geometrically consistent and unbounded large 3D scenes with\nunprecedented high-quality shapes in both indoor and outdoor scenarios.",
        "publication_date": "2024-01-30T14:34:19Z",
        "upvotes": 29
    },
    "2401.17270": {
        "url": "https://arxiv.org/abs/2401.17270",
        "title": "YOLO-World: Real-Time Open-Vocabulary Object Detection",
        "authors": [
            "Tianheng Cheng",
            "Lin Song",
            "Yixiao Ge",
            "Wenyu Liu",
            "Xinggang Wang",
            "Ying Shan"
        ],
        "abstract": "The You Only Look Once (YOLO) series of detectors have established themselves\nas efficient and practical tools. However, their reliance on predefined and\ntrained object categories limits their applicability in open scenarios.\nAddressing this limitation, we introduce YOLO-World, an innovative approach\nthat enhances YOLO with open-vocabulary detection capabilities through\nvision-language modeling and pre-training on large-scale datasets.\nSpecifically, we propose a new Re-parameterizable Vision-Language Path\nAggregation Network (RepVL-PAN) and region-text contrastive loss to facilitate\nthe interaction between visual and linguistic information. Our method excels in\ndetecting a wide range of objects in a zero-shot manner with high efficiency.\nOn the challenging LVIS dataset, YOLO-World achieves 35.4 AP with 52.0 FPS on\nV100, which outperforms many state-of-the-art methods in terms of both accuracy\nand speed. Furthermore, the fine-tuned YOLO-World achieves remarkable\nperformance on several downstream tasks, including object detection and\nopen-vocabulary instance segmentation.",
        "publication_date": "2024-01-30T18:59:38Z",
        "upvotes": 27
    },
    "2401.17093": {
        "url": "https://arxiv.org/abs/2401.17093",
        "title": "StrokeNUWA: Tokenizing Strokes for Vector Graphic Synthesis",
        "authors": [
            "Zecheng Tang",
            "Chenfei Wu",
            "Zekai Zhang",
            "Mingheng Ni",
            "Shengming Yin",
            "Yu Liu",
            "Zhengyuan Yang",
            "Lijuan Wang",
            "Zicheng Liu",
            "Juntao Li",
            "Nan Duan"
        ],
        "abstract": "To leverage LLMs for visual synthesis, traditional methods convert raster\nimage information into discrete grid tokens through specialized visual modules,\nwhile disrupting the model's ability to capture the true semantic\nrepresentation of visual scenes. This paper posits that an alternative\nrepresentation of images, vector graphics, can effectively surmount this\nlimitation by enabling a more natural and semantically coherent segmentation of\nthe image information. Thus, we introduce StrokeNUWA, a pioneering work\nexploring a better visual representation ''stroke tokens'' on vector graphics,\nwhich is inherently visual semantics rich, naturally compatible with LLMs, and\nhighly compressed. Equipped with stroke tokens, StrokeNUWA can significantly\nsurpass traditional LLM-based and optimization-based methods across various\nmetrics in the vector graphic generation task. Besides, StrokeNUWA achieves up\nto a 94x speedup in inference over the speed of prior methods with an\nexceptional SVG code compression ratio of 6.9%.",
        "publication_date": "2024-01-30T15:20:26Z",
        "upvotes": 18
    },
    "2401.16818": {
        "url": "https://arxiv.org/abs/2401.16818",
        "title": "H2O-Danube-1.8B Technical Report",
        "authors": [
            "Philipp Singer",
            "Pascal Pfeiffer",
            "Yauhen Babakhin",
            "Maximilian Jeblick",
            "Nischay Dhankhar",
            "Gabor Fodor",
            "Sri Satish Ambati"
        ],
        "abstract": "We present H2O-Danube-1.8B, a 1.8B language model trained on 1T tokens\nfollowing the core principles of LLama 2 and Mistral. We leverage and refine\nvarious techniques for pre-training large language models. Although our model\nis trained on significantly fewer total tokens compared to reference models of\nsimilar size, it exhibits highly competitive metrics across a multitude of\nbenchmarks. We additionally release a chat model trained with supervised\nfine-tuning followed by direct preference optimization. We make H2O-Danube-1.8B\nopenly available under Apache 2.0 license further democratizing LLMs to a wider\naudience economically.",
        "publication_date": "2024-01-30T08:45:08Z",
        "upvotes": 16
    },
    "2401.17264": {
        "url": "https://arxiv.org/abs/2401.17264",
        "title": "Proactive Detection of Voice Cloning with Localized Watermarking",
        "authors": [
            "Robin San Roman",
            "Pierre Fernandez",
            "Alexandre D\u00e9fossez",
            "Teddy Furon",
            "Tuan Tran",
            "Hady Elsahar"
        ],
        "abstract": "In the rapidly evolving field of speech generative models, there is a\npressing need to ensure audio authenticity against the risks of voice cloning.\nWe present AudioSeal, the first audio watermarking technique designed\nspecifically for localized detection of AI-generated speech. AudioSeal employs\na generator/detector architecture trained jointly with a localization loss to\nenable localized watermark detection up to the sample level, and a novel\nperceptual loss inspired by auditory masking, that enables AudioSeal to achieve\nbetter imperceptibility. AudioSeal achieves state-of-the-art performance in\nterms of robustness to real life audio manipulations and imperceptibility based\non automatic and human evaluation metrics. Additionally, AudioSeal is designed\nwith a fast, single-pass detector, that significantly surpasses existing models\nin speed - achieving detection up to two orders of magnitude faster, making it\nideal for large-scale and real-time applications.",
        "publication_date": "2024-01-30T18:56:22Z",
        "upvotes": 15
    },
    "2401.17256": {
        "url": "https://arxiv.org/abs/2401.17256",
        "title": "Weak-to-Strong Jailbreaking on Large Language Models",
        "authors": [
            "Xuandong Zhao",
            "Xianjun Yang",
            "Tianyu Pang",
            "Chao Du",
            "Lei Li",
            "Yu-Xiang Wang",
            "William Yang Wang"
        ],
        "abstract": "Large language models (LLMs) are vulnerable to jailbreak attacks - resulting\nin harmful, unethical, or biased text generations. However, existing\njailbreaking methods are computationally costly. In this paper, we propose the\nweak-to-strong jailbreaking attack, an efficient method to attack aligned LLMs\nto produce harmful text. Our key intuition is based on the observation that\njailbroken and aligned models only differ in their initial decoding\ndistributions. The weak-to-strong attack's key technical insight is using two\nsmaller models (a safe and an unsafe one) to adversarially modify a\nsignificantly larger safe model's decoding probabilities. We evaluate the\nweak-to-strong attack on 5 diverse LLMs from 3 organizations. The results show\nour method can increase the misalignment rate to over 99% on two datasets with\njust one forward pass per example. Our study exposes an urgent safety issue\nthat needs to be addressed when aligning LLMs. As an initial attempt, we\npropose a defense strategy to protect against such attacks, but creating more\nadvanced defenses remains challenging. The code for replicating the method is\navailable at https://github.com/XuandongZhao/weak-to-strong",
        "publication_date": "2024-01-30T18:48:37Z",
        "upvotes": 14
    },
    "2401.17181": {
        "url": "https://arxiv.org/abs/2401.17181",
        "title": "Transfer Learning for Text Diffusion Models",
        "authors": [
            "Kehang Han",
            "Kathleen Kenealy",
            "Aditya Barua",
            "Noah Fiedel",
            "Noah Constant"
        ],
        "abstract": "In this report, we explore the potential for text diffusion to replace\nautoregressive (AR) decoding for the training and deployment of large language\nmodels (LLMs). We are particularly interested to see whether pretrained AR\nmodels can be transformed into text diffusion models through a lightweight\nadaptation procedure we call ``AR2Diff''. We begin by establishing a strong\nbaseline setup for training text diffusion models. Comparing across multiple\narchitectures and pretraining objectives, we find that training a decoder-only\nmodel with a prefix LM objective is best or near-best across several tasks.\nBuilding on this finding, we test various transfer learning setups for text\ndiffusion models. On machine translation, we find that text diffusion\nunderperforms the standard AR approach. However, on code synthesis and\nextractive QA, we find diffusion models trained from scratch outperform AR\nmodels in many cases. We also observe quality gains from AR2Diff -- adapting AR\nmodels to use diffusion decoding. These results are promising given that text\ndiffusion is relatively underexplored and can be significantly faster than AR\ndecoding for long text generation.",
        "publication_date": "2024-01-30T17:11:56Z",
        "upvotes": 14
    },
    "2401.16861": {
        "url": "https://arxiv.org/abs/2401.16861",
        "title": "Repositioning the Subject within Image",
        "authors": [
            "Yikai Wang",
            "Chenjie Cao",
            "Ke Fan",
            "Qiaole Dong",
            "Yifan Li",
            "Xiangyang Xue",
            "Yanwei Fu"
        ],
        "abstract": "Current image manipulation primarily centers on static manipulation, such as\nreplacing specific regions within an image or altering its overall style. In\nthis paper, we introduce an innovative dynamic manipulation task, subject\nrepositioning. This task involves relocating a user-specified subject to a\ndesired position while preserving the image's fidelity. Our research reveals\nthat the fundamental sub-tasks of subject repositioning, which include filling\nthe void left by the repositioned subject, reconstructing obscured portions of\nthe subject and blending the subject to be consistent with surrounding areas,\ncan be effectively reformulated as a unified, prompt-guided inpainting task.\nConsequently, we can employ a single diffusion generative model to address\nthese sub-tasks using various task prompts learned through our proposed task\ninversion technique. Additionally, we integrate pre-processing and\npost-processing techniques to further enhance the quality of subject\nrepositioning. These elements together form our SEgment-gEnerate-and-bLEnd\n(SEELE) framework. To assess SEELE's effectiveness in subject repositioning, we\nassemble a real-world subject repositioning dataset called ReS. Results of\nSEELE on ReS demonstrate its efficacy.",
        "publication_date": "2024-01-30T10:04:49Z",
        "upvotes": 13
    },
    "2401.16658": {
        "url": "https://arxiv.org/abs/2401.16658",
        "title": "OWSM v3.1: Better and Faster Open Whisper-Style Speech Models based on\n  E-Branchformer",
        "authors": [
            "Yifan Peng",
            "Jinchuan Tian",
            "William Chen",
            "Siddhant Arora",
            "Brian Yan",
            "Yui Sudo",
            "Muhammad Shakeel",
            "Kwanghee Choi",
            "Jiatong Shi",
            "Xuankai Chang",
            "Jee-weon Jung",
            "Shinji Watanabe"
        ],
        "abstract": "Recent studies have advocated for fully open foundation models to promote\ntransparency and open science. As an initial step, the Open Whisper-style\nSpeech Model (OWSM) reproduced OpenAI's Whisper using publicly available data\nand open-source toolkits. With the aim of reproducing Whisper, the previous\nOWSM v1 through v3 models were still based on Transformer, which might lead to\ninferior performance compared to other state-of-the-art speech encoders. In\nthis work, we aim to improve the performance and efficiency of OWSM without\nextra training data. We present E-Branchformer based OWSM v3.1 models at two\nscales, i.e., 100M and 1B. The 1B model is the largest E-Branchformer based\nspeech model that has been made publicly available. It outperforms the previous\nOWSM v3 in a vast majority of evaluation benchmarks, while demonstrating up to\n25% faster inference speed. We publicly release the data preparation scripts,\npre-trained models and training logs.",
        "publication_date": "2024-01-30T01:22:18Z",
        "upvotes": 12
    },
    "2401.16468": {
        "url": "https://arxiv.org/abs/2401.16468",
        "title": "InstructIR: High-Quality Image Restoration Following Human Instructions",
        "authors": [
            "Marcos V. Conde",
            "Gregor Geigle",
            "Radu Timofte"
        ],
        "abstract": "Image restoration is a fundamental problem that involves recovering a\nhigh-quality clean image from its degraded observation. All-In-One image\nrestoration models can effectively restore images from various types and levels\nof degradation using degradation-specific information as prompts to guide the\nrestoration model. In this work, we present the first approach that uses\nhuman-written instructions to guide the image restoration model. Given natural\nlanguage prompts, our model can recover high-quality images from their degraded\ncounterparts, considering multiple degradation types. Our method, InstructIR,\nachieves state-of-the-art results on several restoration tasks including image\ndenoising, deraining, deblurring, dehazing, and (low-light) image enhancement.\nInstructIR improves +1dB over previous all-in-one restoration methods.\nMoreover, our dataset and results represent a novel benchmark for new research\non text-guided image restoration and enhancement. Our code, datasets and models\nare available at: https://github.com/mv-lab/InstructIR",
        "publication_date": "2024-01-29T18:53:33Z",
        "upvotes": 10
    },
    "2401.16467": {
        "url": "https://arxiv.org/abs/2401.16467",
        "title": "ReGAL: Refactoring Programs to Discover Generalizable Abstractions",
        "authors": [
            "Elias Stengel-Eskin",
            "Archiki Prasad",
            "Mohit Bansal"
        ],
        "abstract": "While large language models (LLMs) are increasingly being used for program\nsynthesis, they lack the global view needed to develop useful abstractions;\nthey generally predict programs one at a time, often repeating the same\nfunctionality. Generating redundant code from scratch is both inefficient and\nerror-prone. To address this, we propose Refactoring for Generalizable\nAbstraction Learning (ReGAL), a gradient-free method for learning a library of\nreusable functions via code refactorization, i.e. restructuring code without\nchanging its execution output. ReGAL learns from a small set of existing\nprograms, iteratively verifying and refining its abstractions via execution. We\nfind that the shared function libraries discovered by ReGAL make programs\neasier to predict across diverse domains. On three datasets (LOGO graphics\ngeneration, Date reasoning, and TextCraft, a Minecraft-based text game), both\nopen-source and proprietary LLMs improve in accuracy when predicting programs\nwith ReGAL functions. For CodeLlama-13B, ReGAL results in absolute accuracy\nincreases of 11.5% on graphics, 26.1% on date understanding, and 8.1% on\nTextCraft, outperforming GPT-3.5 in two of three domains. Our analysis reveals\nReGAL's abstractions encapsulate frequently-used subroutines as well as\nenvironment dynamics.",
        "publication_date": "2024-01-29T18:45:30Z",
        "upvotes": 7
    },
    "2401.17221": {
        "url": "https://arxiv.org/abs/2401.17221",
        "title": "MouSi: Poly-Visual-Expert Vision-Language Models",
        "authors": [
            "Xiaoran Fan",
            "Tao Ji",
            "Changhao Jiang",
            "Shuo Li",
            "Senjie Jin",
            "Sirui Song",
            "Junke Wang",
            "Boyang Hong",
            "Lu Chen",
            "Guodong Zheng",
            "Ming Zhang",
            "Caishuang Huang",
            "Rui Zheng",
            "Zhiheng Xi",
            "Yuhao Zhou",
            "Shihan Dou",
            "Junjie Ye",
            "Hang Yan",
            "Tao Gui",
            "Qi Zhang",
            "Xipeng Qiu",
            "Xuanjing Huang",
            "Zuxuan Wu",
            "Yu-Gang Jiang"
        ],
        "abstract": "Current large vision-language models (VLMs) often encounter challenges such\nas insufficient capabilities of a single visual component and excessively long\nvisual tokens. These issues can limit the model's effectiveness in accurately\ninterpreting complex visual information and over-lengthy contextual\ninformation. Addressing these challenges is crucial for enhancing the\nperformance and applicability of VLMs. This paper proposes the use of ensemble\nexperts technique to synergizes the capabilities of individual visual encoders,\nincluding those skilled in image-text matching, OCR, image segmentation, etc.\nThis technique introduces a fusion network to unify the processing of outputs\nfrom different visual experts, while bridging the gap between image encoders\nand pre-trained LLMs. In addition, we explore different positional encoding\nschemes to alleviate the waste of positional encoding caused by lengthy image\nfeature sequences, effectively addressing the issue of position overflow and\nlength limitations. For instance, in our implementation, this technique\nsignificantly reduces the positional occupancy in models like SAM, from a\nsubstantial 4096 to a more efficient and manageable 64 or even down to 1.\nExperimental results demonstrate that VLMs with multiple experts exhibit\nconsistently superior performance over isolated visual encoders and mark a\nsignificant performance boost as more experts are integrated. We have\nopen-sourced the training code used in this report. All of these resources can\nbe found on our project website.",
        "publication_date": "2024-01-30T18:09:11Z",
        "upvotes": 5
    },
    "2401.16677": {
        "url": "https://arxiv.org/abs/2401.16677",
        "title": "T3: Transparent Tracking & Triggering for Fine-grained Overlap of\n  Compute & Collectives",
        "authors": [
            "Suchita Pati",
            "Shaizeen Aga",
            "Mahzabeen Islam",
            "Nuwan Jayasena",
            "Matthew D. Sinclair"
        ],
        "abstract": "Large Language Models increasingly rely on distributed techniques for their\ntraining and inference. These techniques require communication across devices\nwhich can reduce scaling efficiency as the number of devices increases. While\nsome distributed techniques can overlap, and thus, hide this communication with\nindependent computations, techniques such as Tensor Parallelism (TP) inherently\nserialize communication with model execution. One approach to hide this\nserialized communication is to interleave it with the producer operation (of\nthe communicated data) in a fine-grained manner. However, this fine-grained\ninterleaving of communication and computation in software can be difficult.\nFurthermore, as with any concurrent execution, it requires compute and memory\nresources to be shared between computation and communication, causing resource\ncontention that reduces overlapping efficacy.\n  To overcome these challenges, we propose T3 which applies hardware-software\nco-design to transparently overlap serialized communication while minimizing\nresource contention with compute. T3 transparently fuses producer operations\nwith the subsequent communication via a simple configuration of the producer's\noutput address space and requires minor software changes. At the hardware\nlevel, T3 adds a lightweight track and trigger mechanism to orchestrate the\nproducer's compute, and communication. It further uses compute-enhanced\nmemories for communication's attendant compute. As a result, T3 reduces\nresource contention, and efficiently overlaps serialized communication with\ncomputation. For important Transformer models like T-NLG, T3 speeds up\ncommunication-heavy sublayers by 30% geomean (max 47%) and reduces data\nmovement by 22% geomean (max 36%). Furthermore, T3's benefits persist as models\nscale: geomean 29% for sublayers in $\\sim$500-billion parameter models, PALM\nand MT-NLG.",
        "publication_date": "2024-01-30T01:55:34Z",
        "upvotes": 3
    },
    "2401.17377": {
        "url": "https://arxiv.org/abs/2401.17377",
        "title": "Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion\n  Tokens",
        "authors": [
            "Jiacheng Liu",
            "Sewon Min",
            "Luke Zettlemoyer",
            "Yejin Choi",
            "Hannaneh Hajishirzi"
        ],
        "abstract": "Are $n$-gram language models still relevant in this era of neural large\nlanguage models (LLMs)? Our answer is yes, and we showcase their values in both\ntext analysis and improving neural LLMs. This was done by modernizing $n$-gram\nLMs in two aspects. First, we train them at the same data scale as neural LLMs\n-- 5 trillion tokens. This is the largest $n$-gram LM ever built. Second,\nexisting $n$-gram LMs use small $n$ which hinders their performance; we instead\nallow $n$ to be arbitrarily large, by introducing a new $\\infty$-gram LM with\nbackoff. Instead of pre-computing $n$-gram count tables (which would be very\nexpensive), we develop an engine named infini-gram -- powered by suffix arrays\n-- that can compute $\\infty$-gram (as well as $n$-gram with arbitrary $n$)\nprobabilities with millisecond-level latency. The $\\infty$-gram framework and\ninfini-gram engine enable us to conduct many novel and interesting analyses of\nhuman-written and machine-generated text: we find that the $\\infty$-gram LM has\nfairly high accuracy for next-token prediction (47%), and can complement neural\nLLMs to greatly reduce their perplexity. When analyzing machine-generated text,\nwe also observe irregularities in the machine--$\\infty$-gram agreement level\nwith respect to the suffix length, which indicates deficiencies in neural LLM\npretraining and the positional embeddings of Transformers.",
        "publication_date": "2024-01-30T19:03:49Z",
        "upvotes": 31
    },
    "2401.18058": {
        "url": "https://arxiv.org/abs/2401.18058",
        "title": "LongAlign: A Recipe for Long Context Alignment of Large Language Models",
        "authors": [
            "Yushi Bai",
            "Xin Lv",
            "Jiajie Zhang",
            "Yuze He",
            "Ji Qi",
            "Lei Hou",
            "Jie Tang",
            "Yuxiao Dong",
            "Juanzi Li"
        ],
        "abstract": "Extending large language models to effectively handle long contexts requires\ninstruction fine-tuning on input sequences of similar length. To address this,\nwe present LongAlign -- a recipe of the instruction data, training, and\nevaluation for long context alignment. First, we construct a long\ninstruction-following dataset using Self-Instruct. To ensure the data\ndiversity, it covers a broad range of tasks from various long context sources.\nSecond, we adopt the packing and sorted batching strategies to speed up\nsupervised fine-tuning on data with varied length distributions. Additionally,\nwe develop a loss weighting method to balance the contribution to the loss\nacross different sequences during packing training. Third, we introduce the\nLongBench-Chat benchmark for evaluating instruction-following capabilities on\nqueries of 10k-100k in length. Experiments show that LongAlign outperforms\nexisting recipes for LLMs in long context tasks by up to 30\\%, while also\nmaintaining their proficiency in handling short, generic tasks. The code, data,\nand long-aligned models are open-sourced at https://github.com/THUDM/LongAlign.",
        "publication_date": "2024-01-31T18:29:39Z",
        "upvotes": 21
    },
    "2401.17583": {
        "url": "https://arxiv.org/abs/2401.17583",
        "title": "Agile But Safe: Learning Collision-Free High-Speed Legged Locomotion",
        "authors": [
            "Tairan He",
            "Chong Zhang",
            "Wenli Xiao",
            "Guanqi He",
            "Changliu Liu",
            "Guanya Shi"
        ],
        "abstract": "Legged robots navigating cluttered environments must be jointly agile for\nefficient task execution and safe to avoid collisions with obstacles or humans.\nExisting studies either develop conservative controllers (< 1.0 m/s) to ensure\nsafety, or focus on agility without considering potentially fatal collisions.\nThis paper introduces Agile But Safe (ABS), a learning-based control framework\nthat enables agile and collision-free locomotion for quadrupedal robots. ABS\ninvolves an agile policy to execute agile motor skills amidst obstacles and a\nrecovery policy to prevent failures, collaboratively achieving high-speed and\ncollision-free navigation. The policy switch in ABS is governed by a learned\ncontrol-theoretic reach-avoid value network, which also guides the recovery\npolicy as an objective function, thereby safeguarding the robot in a closed\nloop. The training process involves the learning of the agile policy, the\nreach-avoid value network, the recovery policy, and an exteroception\nrepresentation network, all in simulation. These trained modules can be\ndirectly deployed in the real world with onboard sensing and computation,\nleading to high-speed and collision-free navigation in confined indoor and\noutdoor spaces with both static and dynamic obstacles.",
        "publication_date": "2024-01-31T03:58:28Z",
        "upvotes": 21
    },
    "2401.18059": {
        "url": "https://arxiv.org/abs/2401.18059",
        "title": "RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval",
        "authors": [
            "Parth Sarthi",
            "Salman Abdullah",
            "Aditi Tuli",
            "Shubh Khanna",
            "Anna Goldie",
            "Christopher D. Manning"
        ],
        "abstract": "Retrieval-augmented language models can better adapt to changes in world\nstate and incorporate long-tail knowledge. However, most existing methods\nretrieve only short contiguous chunks from a retrieval corpus, limiting\nholistic understanding of the overall document context. We introduce the novel\napproach of recursively embedding, clustering, and summarizing chunks of text,\nconstructing a tree with differing levels of summarization from the bottom up.\nAt inference time, our RAPTOR model retrieves from this tree, integrating\ninformation across lengthy documents at different levels of abstraction.\nControlled experiments show that retrieval with recursive summaries offers\nsignificant improvements over traditional retrieval-augmented LMs on several\ntasks. On question-answering tasks that involve complex, multi-step reasoning,\nwe show state-of-the-art results; for example, by coupling RAPTOR retrieval\nwith the use of GPT-4, we can improve the best performance on the QuALITY\nbenchmark by 20% in absolute accuracy.",
        "publication_date": "2024-01-31T18:30:21Z",
        "upvotes": 20
    },
    "2401.17807": {
        "url": "https://arxiv.org/abs/2401.17807",
        "title": "Advances in 3D Generation: A Survey",
        "authors": [
            "Xiaoyu Li",
            "Qi Zhang",
            "Di Kang",
            "Weihao Cheng",
            "Yiming Gao",
            "Jingbo Zhang",
            "Zhihao Liang",
            "Jing Liao",
            "Yan-Pei Cao",
            "Ying Shan"
        ],
        "abstract": "Generating 3D models lies at the core of computer graphics and has been the\nfocus of decades of research. With the emergence of advanced neural\nrepresentations and generative models, the field of 3D content generation is\ndeveloping rapidly, enabling the creation of increasingly high-quality and\ndiverse 3D models. The rapid growth of this field makes it difficult to stay\nabreast of all recent developments. In this survey, we aim to introduce the\nfundamental methodologies of 3D generation methods and establish a structured\nroadmap, encompassing 3D representation, generation methods, datasets, and\ncorresponding applications. Specifically, we introduce the 3D representations\nthat serve as the backbone for 3D generation. Furthermore, we provide a\ncomprehensive overview of the rapidly growing literature on generation methods,\ncategorized by the type of algorithmic paradigms, including feedforward\ngeneration, optimization-based generation, procedural generation, and\ngenerative novel view synthesis. Lastly, we discuss available datasets,\napplications, and open challenges. We hope this survey will help readers\nexplore this exciting topic and foster further advancements in the field of 3D\ncontent generation.",
        "publication_date": "2024-01-31T13:06:48Z",
        "upvotes": 16
    },
    "2401.17895": {
        "url": "https://arxiv.org/abs/2401.17895",
        "title": "ReplaceAnything3D:Text-Guided 3D Scene Editing with Compositional Neural\n  Radiance Fields",
        "authors": [
            "Edward Bartrum",
            "Thu Nguyen-Phuoc",
            "Chris Xie",
            "Zhengqin Li",
            "Numair Khan",
            "Armen Avetisyan",
            "Douglas Lanman",
            "Lei Xiao"
        ],
        "abstract": "We introduce ReplaceAnything3D model (RAM3D), a novel text-guided 3D scene\nediting method that enables the replacement of specific objects within a scene.\nGiven multi-view images of a scene, a text prompt describing the object to\nreplace, and a text prompt describing the new object, our Erase-and-Replace\napproach can effectively swap objects in the scene with newly generated content\nwhile maintaining 3D consistency across multiple viewpoints. We demonstrate the\nversatility of ReplaceAnything3D by applying it to various realistic 3D scenes,\nshowcasing results of modified foreground objects that are well-integrated with\nthe rest of the scene without affecting its overall integrity.",
        "publication_date": "2024-01-31T15:02:26Z",
        "upvotes": 15
    },
    "2401.17509": {
        "url": "https://arxiv.org/abs/2401.17509",
        "title": "Anything in Any Scene: Photorealistic Video Object Insertion",
        "authors": [
            "Chen Bai",
            "Zeman Shao",
            "Guoxiang Zhang",
            "Di Liang",
            "Jie Yang",
            "Zhuorui Zhang",
            "Yujian Guo",
            "Chengzhang Zhong",
            "Yiqiao Qiu",
            "Zhendong Wang",
            "Yichen Guan",
            "Xiaoyin Zheng",
            "Tao Wang",
            "Cheng Lu"
        ],
        "abstract": "Realistic video simulation has shown significant potential across diverse\napplications, from virtual reality to film production. This is particularly\ntrue for scenarios where capturing videos in real-world settings is either\nimpractical or expensive. Existing approaches in video simulation often fail to\naccurately model the lighting environment, represent the object geometry, or\nachieve high levels of photorealism. In this paper, we propose Anything in Any\nScene, a novel and generic framework for realistic video simulation that\nseamlessly inserts any object into an existing dynamic video with a strong\nemphasis on physical realism. Our proposed general framework encompasses three\nkey processes: 1) integrating a realistic object into a given scene video with\nproper placement to ensure geometric realism; 2) estimating the sky and\nenvironmental lighting distribution and simulating realistic shadows to enhance\nthe light realism; 3) employing a style transfer network that refines the final\nvideo output to maximize photorealism. We experimentally demonstrate that\nAnything in Any Scene framework produces simulated videos of great geometric\nrealism, lighting realism, and photorealism. By significantly mitigating the\nchallenges associated with video data generation, our framework offers an\nefficient and cost-effective solution for acquiring high-quality videos.\nFurthermore, its applications extend well beyond video data augmentation,\nshowing promising potential in virtual reality, video editing, and various\nother video-centric applications. Please check our project website\nhttps://anythinginanyscene.github.io for access to our project code and more\nhigh-resolution video results.",
        "publication_date": "2024-01-30T23:54:43Z",
        "upvotes": 15
    },
    "2401.17464": {
        "url": "https://arxiv.org/abs/2401.17464",
        "title": "Efficient Tool Use with Chain-of-Abstraction Reasoning",
        "authors": [
            "Silin Gao",
            "Jane Dwivedi-Yu",
            "Ping Yu",
            "Xiaoqing Ellen Tan",
            "Ramakanth Pasunuru",
            "Olga Golovneva",
            "Koustuv Sinha",
            "Asli Celikyilmaz",
            "Antoine Bosselut",
            "Tianlu Wang"
        ],
        "abstract": "To achieve faithful reasoning that aligns with human expectations, large\nlanguage models (LLMs) need to ground their reasoning to real-world knowledge\n(e.g., web facts, math and physical rules). Tools help LLMs access this\nexternal knowledge, but there remains challenges for fine-tuning LLM agents\n(e.g., Toolformer) to invoke tools in multi-step reasoning problems, where\ninter-connected tool calls require holistic and efficient tool usage planning.\n  In this work, we propose a new method for LLMs to better leverage tools in\nmulti-step reasoning. Our method, Chain-of-Abstraction (CoA), trains LLMs to\nfirst decode reasoning chains with abstract placeholders, and then call domain\ntools to reify each reasoning chain by filling in specific knowledge. This\nplanning with abstract chains enables LLMs to learn more general reasoning\nstrategies, which are robust to shifts of domain knowledge (e.g., math results)\nrelevant to different reasoning questions. It also allows LLMs to perform\ndecoding and calling of external tools in parallel, which avoids the inference\ndelay caused by waiting for tool responses. In mathematical reasoning and Wiki\nQA domains, we show that our method consistently outperforms previous\nchain-of-thought and tool-augmented baselines on both in-distribution and\nout-of-distribution test sets, with an average ~6% absolute QA accuracy\nimprovement. LLM agents trained with our method also show more efficient tool\nuse, with inference speed being on average ~1.4x faster than baseline\ntool-augmented LLMs.",
        "publication_date": "2024-01-30T21:53:30Z",
        "upvotes": 15
    },
    "2401.17574": {
        "url": "https://arxiv.org/abs/2401.17574",
        "title": "Scavenging Hyena: Distilling Transformers into Long Convolution Models",
        "authors": [
            "Tokiniaina Raharison Ralambomihanta",
            "Shahrad Mohammadzadeh",
            "Mohammad Sami Nur Islam",
            "Wassim Jabbour",
            "Laurence Liang"
        ],
        "abstract": "The rapid evolution of Large Language Models (LLMs), epitomized by\narchitectures like GPT-4, has reshaped the landscape of natural language\nprocessing. This paper introduces a pioneering approach to address the\nefficiency concerns associated with LLM pre-training, proposing the use of\nknowledge distillation for cross-architecture transfer. Leveraging insights\nfrom the efficient Hyena mechanism, our method replaces attention heads in\ntransformer models by Hyena, offering a cost-effective alternative to\ntraditional pre-training while confronting the challenge of processing long\ncontextual information, inherent in quadratic attention mechanisms. Unlike\nconventional compression-focused methods, our technique not only enhances\ninference speed but also surpasses pre-training in terms of both accuracy and\nefficiency. In the era of evolving LLMs, our work contributes to the pursuit of\nsustainable AI solutions, striking a balance between computational power and\nenvironmental impact.",
        "publication_date": "2024-01-31T03:39:07Z",
        "upvotes": 14
    },
    "2401.18075": {
        "url": "https://arxiv.org/abs/2401.18075",
        "title": "CARFF: Conditional Auto-encoded Radiance Field for 3D Scene Forecasting",
        "authors": [
            "Jiezhi Yang",
            "Khushi Desai",
            "Charles Packer",
            "Harshil Bhatia",
            "Nicholas Rhinehart",
            "Rowan McAllister",
            "Joseph Gonzalez"
        ],
        "abstract": "We propose CARFF: Conditional Auto-encoded Radiance Field for 3D Scene\nForecasting, a method for predicting future 3D scenes given past observations,\nsuch as 2D ego-centric images. Our method maps an image to a distribution over\nplausible 3D latent scene configurations using a probabilistic encoder, and\npredicts the evolution of the hypothesized scenes through time. Our latent\nscene representation conditions a global Neural Radiance Field (NeRF) to\nrepresent a 3D scene model, which enables explainable predictions and\nstraightforward downstream applications. This approach extends beyond previous\nneural rendering work by considering complex scenarios of uncertainty in\nenvironmental states and dynamics. We employ a two-stage training of\nPose-Conditional-VAE and NeRF to learn 3D representations. Additionally, we\nauto-regressively predict latent scene representations as a partially\nobservable Markov decision process, utilizing a mixture density network. We\ndemonstrate the utility of our method in realistic scenarios using the CARLA\ndriving simulator, where CARFF can be used to enable efficient trajectory and\ncontingency planning in complex multi-agent autonomous driving scenarios\ninvolving visual occlusions.",
        "publication_date": "2024-01-31T18:56:09Z",
        "upvotes": 7
    },
    "2402.00838": {
        "url": "https://arxiv.org/abs/2402.00838",
        "title": "OLMo: Accelerating the Science of Language Models",
        "authors": [
            "Dirk Groeneveld",
            "Iz Beltagy",
            "Pete Walsh",
            "Akshita Bhagia",
            "Rodney Kinney",
            "Oyvind Tafjord",
            "Ananya Harsh Jha",
            "Hamish Ivison",
            "Ian Magnusson",
            "Yizhong Wang",
            "Shane Arora",
            "David Atkinson",
            "Russell Authur",
            "Khyathi Raghavi Chandu",
            "Arman Cohan",
            "Jennifer Dumas",
            "Yanai Elazar",
            "Yuling Gu",
            "Jack Hessel",
            "Tushar Khot",
            "William Merrill",
            "Jacob Morrison",
            "Niklas Muennighoff",
            "Aakanksha Naik",
            "Crystal Nam",
            "Matthew E. Peters",
            "Valentina Pyatkin",
            "Abhilasha Ravichander",
            "Dustin Schwenk",
            "Saurabh Shah",
            "Will Smith",
            "Emma Strubell",
            "Nishant Subramani",
            "Mitchell Wortsman",
            "Pradeep Dasigi",
            "Nathan Lambert",
            "Kyle Richardson",
            "Luke Zettlemoyer",
            "Jesse Dodge",
            "Kyle Lo",
            "Luca Soldaini",
            "Noah A. Smith",
            "Hannaneh Hajishirzi"
        ],
        "abstract": "Language models (LMs) have become ubiquitous in both NLP research and in\ncommercial product offerings. As their commercial importance has surged, the\nmost powerful models have become closed off, gated behind proprietary\ninterfaces, with important details of their training data, architectures, and\ndevelopment undisclosed. Given the importance of these details in\nscientifically studying these models, including their biases and potential\nrisks, we believe it is essential for the research community to have access to\npowerful, truly open LMs. To this end, this technical report details the first\nrelease of OLMo, a state-of-the-art, truly Open Language Model and its\nframework to build and study the science of language modeling. Unlike most\nprior efforts that have only released model weights and inference code, we\nrelease OLMo and the whole framework, including training data and training and\nevaluation code. We hope this release will empower and strengthen the open\nresearch community and inspire a new wave of innovation.",
        "publication_date": "2024-02-01T18:28:55Z",
        "upvotes": 73
    },
    "2402.00159": {
        "url": "https://arxiv.org/abs/2402.00159",
        "title": "Dolma: an Open Corpus of Three Trillion Tokens for Language Model\n  Pretraining Research",
        "authors": [
            "Luca Soldaini",
            "Rodney Kinney",
            "Akshita Bhagia",
            "Dustin Schwenk",
            "David Atkinson",
            "Russell Authur",
            "Ben Bogin",
            "Khyathi Chandu",
            "Jennifer Dumas",
            "Yanai Elazar",
            "Valentin Hofmann",
            "Ananya Harsh Jha",
            "Sachin Kumar",
            "Li Lucy",
            "Xinxi Lyu",
            "Nathan Lambert",
            "Ian Magnusson",
            "Jacob Morrison",
            "Niklas Muennighoff",
            "Aakanksha Naik",
            "Crystal Nam",
            "Matthew E. Peters",
            "Abhilasha Ravichander",
            "Kyle Richardson",
            "Zejiang Shen",
            "Emma Strubell",
            "Nishant Subramani",
            "Oyvind Tafjord",
            "Pete Walsh",
            "Luke Zettlemoyer",
            "Noah A. Smith",
            "Hannaneh Hajishirzi",
            "Iz Beltagy",
            "Dirk Groeneveld",
            "Jesse Dodge",
            "Kyle Lo"
        ],
        "abstract": "Language models have become a critical technology to tackling a wide range of\nnatural language processing tasks, yet many details about how the\nbest-performing language models were developed are not reported. In particular,\ninformation about their pretraining corpora is seldom discussed: commercial\nlanguage models rarely provide any information about their data; even open\nmodels rarely release datasets they are trained on, or an exact recipe to\nreproduce them. As a result, it is challenging to conduct certain threads of\nlanguage modeling research, such as understanding how training data impacts\nmodel capabilities and shapes their limitations. To facilitate open research on\nlanguage model pretraining, we release Dolma, a three trillion tokens English\ncorpus, built from a diverse mixture of web content, scientific papers, code,\npublic-domain books, social media, and encyclopedic materials. In addition, we\nopen source our data curation toolkit to enable further experimentation and\nreproduction of our work. In this report, we document Dolma, including its\ndesign principles, details about its construction, and a summary of its\ncontents. We interleave this report with analyses and experimental results from\ntraining language models on intermediate states of Dolma to share what we have\nlearned about important data curation practices, including the role of content\nor quality filters, deduplication, and multi-source mixing. Dolma has been used\nto train OLMo, a state-of-the-art, open language model and framework designed\nto build and study the science of language modeling.",
        "publication_date": "2024-01-31T20:29:50Z",
        "upvotes": 55
    },
    "2402.00786": {
        "url": "https://arxiv.org/abs/2402.00786",
        "title": "CroissantLLM: A Truly Bilingual French-English Language Model",
        "authors": [
            "Manuel Faysse",
            "Patrick Fernandes",
            "Nuno M. Guerreiro",
            "Ant\u00f3nio Loison",
            "Duarte M. Alves",
            "Caio Corro",
            "Nicolas Boizard",
            "Jo\u00e3o Alves",
            "Ricardo Rei",
            "Pedro H. Martins",
            "Antoni Bigata Casademunt",
            "Fran\u00e7ois Yvon",
            "Andr\u00e9 F. T. Martins",
            "Gautier Viaud",
            "C\u00e9line Hudelot",
            "Pierre Colombo"
        ],
        "abstract": "We introduce CroissantLLM, a 1.3B language model pretrained on a set of 3T\nEnglish and French tokens, to bring to the research and industrial community a\nhigh-performance, fully open-sourced bilingual model that runs swiftly on\nconsumer-grade local hardware. To that end, we pioneer the approach of training\nan intrinsically bilingual model with a 1:1 English-to-French pretraining data\nratio, a custom tokenizer, and bilingual finetuning datasets. We release the\ntraining dataset, notably containing a French split with manually curated,\nhigh-quality, and varied data sources. To assess performance outside of\nEnglish, we craft a novel benchmark, FrenchBench, consisting of an array of\nclassification and generation tasks, covering various orthogonal aspects of\nmodel performance in the French Language. Additionally, rooted in transparency\nand to foster further Large Language Model research, we release codebases, and\ndozens of checkpoints across various model sizes, training data distributions,\nand training steps, as well as fine-tuned Chat models, and strong translation\nmodels. We evaluate our model through the FMTI framework, and validate 81 % of\nthe transparency criteria, far beyond the scores of even most open initiatives.\nThis work enriches the NLP landscape, breaking away from previous\nEnglish-centric work in order to strengthen our understanding of\nmultilinguality in language models.",
        "publication_date": "2024-02-01T17:17:55Z",
        "upvotes": 22
    },
    "2402.00858": {
        "url": "https://arxiv.org/abs/2402.00858",
        "title": "Can Large Language Models Understand Context?",
        "authors": [
            "Yilun Zhu",
            "Joel Ruben Antony Moniz",
            "Shruti Bhargava",
            "Jiarui Lu",
            "Dhivya Piraviperumal",
            "Site Li",
            "Yuan Zhang",
            "Hong Yu",
            "Bo-Hsiang Tseng"
        ],
        "abstract": "Understanding context is key to understanding human language, an ability\nwhich Large Language Models (LLMs) have been increasingly seen to demonstrate\nto an impressive extent. However, though the evaluation of LLMs encompasses\nvarious domains within the realm of Natural Language Processing, limited\nattention has been paid to probing their linguistic capability of understanding\ncontextual features. This paper introduces a context understanding benchmark by\nadapting existing datasets to suit the evaluation of generative models. This\nbenchmark comprises of four distinct tasks and nine datasets, all featuring\nprompts designed to assess the models' ability to understand context. First, we\nevaluate the performance of LLMs under the in-context learning pretraining\nscenario. Experimental results indicate that pre-trained dense models struggle\nwith understanding more nuanced contextual features when compared to\nstate-of-the-art fine-tuned models. Second, as LLM compression holds growing\nsignificance in both research and real-world applications, we assess the\ncontext understanding of quantized models under in-context-learning settings.\nWe find that 3-bit post-training quantization leads to varying degrees of\nperformance reduction on our benchmark. We conduct an extensive analysis of\nthese scenarios to substantiate our experimental results.",
        "publication_date": "2024-02-01T18:55:29Z",
        "upvotes": 20
    },
    "2402.00396": {
        "url": "https://arxiv.org/abs/2402.00396",
        "title": "Efficient Exploration for LLMs",
        "authors": [
            "Vikranth Dwaracherla",
            "Seyed Mohammad Asghari",
            "Botao Hao",
            "Benjamin Van Roy"
        ],
        "abstract": "We present evidence of substantial benefit from efficient exploration in\ngathering human feedback to improve large language models. In our experiments,\nan agent sequentially generates queries while fitting a reward model to the\nfeedback received. Our best-performing agent generates queries using double\nThompson sampling, with uncertainty represented by an epistemic neural network.\nOur results demonstrate that efficient exploration enables high levels of\nperformance with far fewer queries. Further, both uncertainty estimation and\nthe choice of exploration scheme play critical roles.",
        "publication_date": "2024-02-01T07:32:24Z",
        "upvotes": 18
    },
    "2402.00769": {
        "url": "https://arxiv.org/abs/2402.00769",
        "title": "AnimateLCM: Accelerating the Animation of Personalized Diffusion Models\n  and Adapters with Decoupled Consistency Learning",
        "authors": [
            "Fu-Yun Wang",
            "Zhaoyang Huang",
            "Xiaoyu Shi",
            "Weikang Bian",
            "Guanglu Song",
            "Yu Liu",
            "Hongsheng Li"
        ],
        "abstract": "Video diffusion models has been gaining increasing attention for its ability\nto produce videos that are both coherent and of high fidelity. However, the\niterative denoising process makes it computationally intensive and\ntime-consuming, thus limiting its applications. Inspired by the Consistency\nModel (CM) that distills pretrained image diffusion models to accelerate the\nsampling with minimal steps and its successful extension Latent Consistency\nModel (LCM) on conditional image generation, we propose AnimateLCM, allowing\nfor high-fidelity video generation within minimal steps. Instead of directly\nconducting consistency learning on the raw video dataset, we propose a\ndecoupled consistency learning strategy that decouples the distillation of\nimage generation priors and motion generation priors, which improves the\ntraining efficiency and enhance the generation visual quality. Additionally, to\nenable the combination of plug-and-play adapters in stable diffusion community\nto achieve various functions (e.g., ControlNet for controllable generation). we\npropose an efficient strategy to adapt existing adapters to our distilled\ntext-conditioned video consistency model or train adapters from scratch without\nharming the sampling speed. We validate the proposed strategy in\nimage-conditioned video generation and layout-conditioned video generation, all\nachieving top-performing results. Experimental results validate the\neffectiveness of our proposed method. Code and weights will be made public.\nMore details are available at https://github.com/G-U-N/AnimateLCM.",
        "publication_date": "2024-02-01T16:58:11Z",
        "upvotes": 17
    },
    "2402.00854": {
        "url": "https://arxiv.org/abs/2402.00854",
        "title": "SymbolicAI: A framework for logic-based approaches combining generative\n  models and solvers",
        "authors": [
            "Marius-Constantin Dinu",
            "Claudiu Leoveanu-Condrei",
            "Markus Holzleitner",
            "Werner Zellinger",
            "Sepp Hochreiter"
        ],
        "abstract": "We introduce SymbolicAI, a versatile and modular framework employing a\nlogic-based approach to concept learning and flow management in generative\nprocesses. SymbolicAI enables the seamless integration of generative models\nwith a diverse range of solvers by treating large language models (LLMs) as\nsemantic parsers that execute tasks based on both natural and formal language\ninstructions, thus bridging the gap between symbolic reasoning and generative\nAI. We leverage probabilistic programming principles to tackle complex tasks,\nand utilize differentiable and classical programming paradigms with their\nrespective strengths. The framework introduces a set of polymorphic,\ncompositional, and self-referential operations for data stream manipulation,\naligning LLM outputs with user objectives. As a result, we can transition\nbetween the capabilities of various foundation models endowed with zero- and\nfew-shot learning capabilities and specialized, fine-tuned models or solvers\nproficient in addressing specific problems. In turn, the framework facilitates\nthe creation and evaluation of explainable computational graphs. We conclude by\nintroducing a quality measure and its empirical score for evaluating these\ncomputational graphs, and propose a benchmark that compares various\nstate-of-the-art LLMs across a set of complex workflows. We refer to the\nempirical score as the \"Vector Embedding for Relational Trajectory Evaluation\nthrough Cross-similarity\", or VERTEX score for short. The framework codebase\nand benchmark are linked below.",
        "publication_date": "2024-02-01T18:50:50Z",
        "upvotes": 16
    },
    "2402.00351": {
        "url": "https://arxiv.org/abs/2402.00351",
        "title": "Machine Unlearning for Image-to-Image Generative Models",
        "authors": [
            "Guihong Li",
            "Hsiang Hsu",
            "Chun-Fu Chen",
            "Radu Marculescu"
        ],
        "abstract": "Machine unlearning has emerged as a new paradigm to deliberately forget data\nsamples from a given model in order to adhere to stringent regulations.\nHowever, existing machine unlearning methods have been primarily focused on\nclassification models, leaving the landscape of unlearning for generative\nmodels relatively unexplored. This paper serves as a bridge, addressing the gap\nby providing a unifying framework of machine unlearning for image-to-image\ngenerative models. Within this framework, we propose a\ncomputationally-efficient algorithm, underpinned by rigorous theoretical\nanalysis, that demonstrates negligible performance degradation on the retain\nsamples, while effectively removing the information from the forget samples.\nEmpirical studies on two large-scale datasets, ImageNet-1K and Places-365,\nfurther show that our algorithm does not rely on the availability of the retain\nsamples, which further complies with data retention policy. To our best\nknowledge, this work is the first that represents systemic, theoretical,\nempirical explorations of machine unlearning specifically tailored for\nimage-to-image generative models. Our code is available at\nhttps://github.com/jpmorganchase/l2l-generator-unlearning.",
        "publication_date": "2024-02-01T05:35:25Z",
        "upvotes": 11
    },
    "2402.00867": {
        "url": "https://arxiv.org/abs/2402.00867",
        "title": "AToM: Amortized Text-to-Mesh using 2D Diffusion",
        "authors": [
            "Guocheng Qian",
            "Junli Cao",
            "Aliaksandr Siarohin",
            "Yash Kant",
            "Chaoyang Wang",
            "Michael Vasilkovsky",
            "Hsin-Ying Lee",
            "Yuwei Fang",
            "Ivan Skorokhodov",
            "Peiye Zhuang",
            "Igor Gilitschenski",
            "Jian Ren",
            "Bernard Ghanem",
            "Kfir Aberman",
            "Sergey Tulyakov"
        ],
        "abstract": "We introduce Amortized Text-to-Mesh (AToM), a feed-forward text-to-mesh\nframework optimized across multiple text prompts simultaneously. In contrast to\nexisting text-to-3D methods that often entail time-consuming per-prompt\noptimization and commonly output representations other than polygonal meshes,\nAToM directly generates high-quality textured meshes in less than 1 second with\naround 10 times reduction in the training cost, and generalizes to unseen\nprompts. Our key idea is a novel triplane-based text-to-mesh architecture with\na two-stage amortized optimization strategy that ensures stable training and\nenables scalability. Through extensive experiments on various prompt\nbenchmarks, AToM significantly outperforms state-of-the-art amortized\napproaches with over 4 times higher accuracy (in DF415 dataset) and produces\nmore distinguishable and higher-quality 3D outputs. AToM demonstrates strong\ngeneralizability, offering finegrained 3D assets for unseen interpolated\nprompts without further optimization during inference, unlike per-prompt\nsolutions.",
        "publication_date": "2024-02-01T18:59:56Z",
        "upvotes": 10
    },
    "2402.00742": {
        "url": "https://arxiv.org/abs/2402.00742",
        "title": "Transforming and Combining Rewards for Aligning Large Language Models",
        "authors": [
            "Zihao Wang",
            "Chirag Nagpal",
            "Jonathan Berant",
            "Jacob Eisenstein",
            "Alex D'Amour",
            "Sanmi Koyejo",
            "Victor Veitch"
        ],
        "abstract": "A common approach for aligning language models to human preferences is to\nfirst learn a reward model from preference data, and then use this reward model\nto update the language model. We study two closely related problems that arise\nin this approach. First, any monotone transformation of the reward model\npreserves preference ranking; is there a choice that is ``better'' than others?\nSecond, we often wish to align language models to multiple properties: how\nshould we combine multiple reward models? Using a probabilistic interpretation\nof the alignment procedure, we identify a natural choice for transformation for\n(the common case of) rewards learned from Bradley-Terry preference models. This\nderived transformation has two important properties. First, it emphasizes\nimproving poorly-performing outputs, rather than outputs that already score\nwell. This mitigates both underfitting (where some prompts are not improved)\nand reward hacking (where the model learns to exploit misspecification of the\nreward model). Second, it enables principled aggregation of rewards by linking\nsummation to logical conjunction: the sum of transformed rewards corresponds to\nthe probability that the output is ``good'' in all measured properties, in a\nsense we make precise. Experiments aligning language models to be both helpful\nand harmless using RLHF show substantial improvements over the baseline\n(non-transformed) approach.",
        "publication_date": "2024-02-01T16:39:28Z",
        "upvotes": 10
    },
    "2402.00518": {
        "url": "https://arxiv.org/abs/2402.00518",
        "title": "EE-Tuning: An Economical yet Scalable Solution for Tuning Early-Exit\n  Large Language Models",
        "authors": [
            "Xuchen Pan",
            "Yanxi Chen",
            "Yaliang Li",
            "Bolin Ding",
            "Jingren Zhou"
        ],
        "abstract": "This work introduces EE-Tuning, a lightweight and economical solution to\ntraining/tuning early-exit large language models (LLMs). In contrast to the\ncommon approach of full-parameter pre-training, EE-Tuning augments any\npre-trained (and possibly fine-tuned) standard LLM with additional early-exit\nlayers that are tuned in a parameter-efficient manner, which requires\nsignificantly less computational resources and training data. Our\nimplementation of EE-Tuning achieves outstanding training efficiency via\nextensive performance optimizations, as well as scalability due to its full\ncompatibility with 3D parallelism. Results of systematic experiments validate\nthe efficacy of EE-Tuning, confirming that effective early-exit LLM inference\ncan be achieved with a limited training budget. In hope of making early-exit\nLLMs accessible to the community, we release the source code of our\nimplementation of EE-Tuning at https://github.com/pan-x-c/EE-LLM.",
        "publication_date": "2024-02-01T11:39:04Z",
        "upvotes": 3
    },
    "2402.01093": {
        "url": "https://arxiv.org/abs/2402.01093",
        "title": "Specialized Language Models with Cheap Inference from Limited Domain\n  Data",
        "authors": [
            "David Grangier",
            "Angelos Katharopoulos",
            "Pierre Ablin",
            "Awni Hannun"
        ],
        "abstract": "Large language models have emerged as a versatile tool but are challenging to\napply to tasks lacking large inference budgets and large in-domain training\nsets. This work formalizes these constraints and distinguishes four important\nvariables: the pretraining budget (for training before the target domain is\nknown), the specialization budget (for training after the target domain is\nknown), the inference budget, and the in-domain training set size. Across these\nsettings, we compare different approaches from the machine learning literature.\nLimited by inference cost, we find better alternatives to the standard practice\nof training very large vanilla transformer models. In particular, we show that\nhyper-networks and mixture of experts have better perplexity for large\npretraining budgets, while small models trained on importance sampled datasets\nare attractive for large specialization budgets.",
        "publication_date": "2024-02-02T01:45:18Z",
        "upvotes": 45
    },
    "2402.01391": {
        "url": "https://arxiv.org/abs/2402.01391",
        "title": "StepCoder: Improve Code Generation with Reinforcement Learning from\n  Compiler Feedback",
        "authors": [
            "Shihan Dou",
            "Yan Liu",
            "Haoxiang Jia",
            "Limao Xiong",
            "Enyu Zhou",
            "Wei Shen",
            "Junjie Shan",
            "Caishuang Huang",
            "Xiao Wang",
            "Xiaoran Fan",
            "Zhiheng Xi",
            "Yuhao Zhou",
            "Tao Ji",
            "Rui Zheng",
            "Qi Zhang",
            "Xuanjing Huang",
            "Tao Gui"
        ],
        "abstract": "The advancement of large language models (LLMs) has significantly propelled\nthe field of code generation. Previous work integrated reinforcement learning\n(RL) with compiler feedback for exploring the output space of LLMs to enhance\ncode generation quality. However, the lengthy code generated by LLMs in\nresponse to complex human requirements makes RL exploration a challenge. Also,\nsince the unit tests may not cover the complicated code, optimizing LLMs by\nusing these unexecuted code snippets is ineffective. To tackle these\nchallenges, we introduce StepCoder, a novel RL framework for code generation,\nconsisting of two main components: CCCS addresses the exploration challenge by\nbreaking the long sequences code generation task into a Curriculum of Code\nCompletion Subtasks, while FGO only optimizes the model by masking the\nunexecuted code segments to provide Fine-Grained Optimization. In addition, we\nfurthermore construct the APPS+ dataset for RL training, which is manually\nverified to ensure the correctness of unit tests. Experimental results show\nthat our method improves the ability to explore the output space and\noutperforms state-of-the-art approaches in corresponding benchmarks. Our\ndataset APPS+ and StepCoder are available online.",
        "publication_date": "2024-02-02T13:14:31Z",
        "upvotes": 41
    },
    "2402.01622": {
        "url": "https://arxiv.org/abs/2402.01622",
        "title": "TravelPlanner: A Benchmark for Real-World Planning with Language Agents",
        "authors": [
            "Jian Xie",
            "Kai Zhang",
            "Jiangjie Chen",
            "Tinghui Zhu",
            "Renze Lou",
            "Yuandong Tian",
            "Yanghua Xiao",
            "Yu Su"
        ],
        "abstract": "Planning has been part of the core pursuit for artificial intelligence since\nits conception, but earlier AI agents mostly focused on constrained settings\nbecause many of the cognitive substrates necessary for human-level planning\nhave been lacking. Recently, language agents powered by large language models\n(LLMs) have shown interesting capabilities such as tool use and reasoning. Are\nthese language agents capable of planning in more complex settings that are out\nof the reach of prior AI agents? To advance this investigation, we propose\nTravelPlanner, a new planning benchmark that focuses on travel planning, a\ncommon real-world planning scenario. It provides a rich sandbox environment,\nvarious tools for accessing nearly four million data records, and 1,225\nmeticulously curated planning intents and reference plans. Comprehensive\nevaluations show that the current language agents are not yet capable of\nhandling such complex planning tasks-even GPT-4 only achieves a success rate of\n0.6%. Language agents struggle to stay on task, use the right tools to collect\ninformation, or keep track of multiple constraints. However, we note that the\nmere possibility for language agents to tackle such a complex problem is in\nitself non-trivial progress. TravelPlanner provides a challenging yet\nmeaningful testbed for future language agents.",
        "publication_date": "2024-02-02T18:39:51Z",
        "upvotes": 30
    },
    "2402.01118": {
        "url": "https://arxiv.org/abs/2402.01118",
        "title": "PokeLLMon: A Human-Parity Agent for Pokemon Battles with Large Language\n  Models",
        "authors": [
            "Sihao Hu",
            "Tiansheng Huang",
            "Ling Liu"
        ],
        "abstract": "We introduce PokeLLMon, the first LLM-embodied agent that achieves\nhuman-parity performance in tactical battle games, as demonstrated in Pokemon\nbattles. The design of PokeLLMon incorporates three key strategies: (i)\nIn-context reinforcement learning that instantly consumes text-based feedback\nderived from battles to iteratively refine the policy; (ii) Knowledge-augmented\ngeneration that retrieves external knowledge to counteract hallucination and\nenables the agent to act timely and properly; (iii) Consistent action\ngeneration to mitigate the panic switching phenomenon when the agent faces a\npowerful opponent and wants to elude the battle. We show that online battles\nagainst human demonstrates PokeLLMon's human-like battle strategies and\njust-in-time decision making, achieving 49% of win rate in the Ladder\ncompetitions and 56% of win rate in the invited battles. Our implementation and\nplayable battle logs are available at: https://github.com/git-disl/PokeLLMon.",
        "publication_date": "2024-02-02T03:22:12Z",
        "upvotes": 28
    },
    "2402.01566": {
        "url": "https://arxiv.org/abs/2402.01566",
        "title": "Boximator: Generating Rich and Controllable Motions for Video Synthesis",
        "authors": [
            "Jiawei Wang",
            "Yuchen Zhang",
            "Jiaxin Zou",
            "Yan Zeng",
            "Guoqiang Wei",
            "Liping Yuan",
            "Hang Li"
        ],
        "abstract": "Generating rich and controllable motion is a pivotal challenge in video\nsynthesis. We propose Boximator, a new approach for fine-grained motion\ncontrol. Boximator introduces two constraint types: hard box and soft box.\nUsers select objects in the conditional frame using hard boxes and then use\neither type of boxes to roughly or rigorously define the object's position,\nshape, or motion path in future frames. Boximator functions as a plug-in for\nexisting video diffusion models. Its training process preserves the base\nmodel's knowledge by freezing the original weights and training only the\ncontrol module. To address training challenges, we introduce a novel\nself-tracking technique that greatly simplifies the learning of box-object\ncorrelations. Empirically, Boximator achieves state-of-the-art video quality\n(FVD) scores, improving on two base models, and further enhanced after\nincorporating box constraints. Its robust motion controllability is validated\nby drastic increases in the bounding box alignment metric. Human evaluation\nalso shows that users favor Boximator generation results over the base model.",
        "publication_date": "2024-02-02T16:59:48Z",
        "upvotes": 24
    },
    "2402.01032": {
        "url": "https://arxiv.org/abs/2402.01032",
        "title": "Repeat After Me: Transformers are Better than State Space Models at\n  Copying",
        "authors": [
            "Samy Jelassi",
            "David Brandfonbrener",
            "Sham M. Kakade",
            "Eran Malach"
        ],
        "abstract": "Transformers are the dominant architecture for sequence modeling, but there\nis growing interest in models that use a fixed-size latent state that does not\ndepend on the sequence length, which we refer to as \"generalized state space\nmodels\" (GSSMs). In this paper we show that while GSSMs are promising in terms\nof inference-time efficiency, they are limited compared to transformer models\non tasks that require copying from the input context. We start with a\ntheoretical analysis of the simple task of string copying and prove that a two\nlayer transformer can copy strings of exponential length while GSSMs are\nfundamentally limited by their fixed-size latent state. Empirically, we find\nthat transformers outperform GSSMs in terms of efficiency and generalization on\nsynthetic tasks that require copying the context. Finally, we evaluate\npretrained large language models and find that transformer models dramatically\noutperform state space models at copying and retrieving information from\ncontext. Taken together, these results suggest a fundamental gap between\ntransformers and GSSMs on tasks of practical interest.",
        "publication_date": "2024-02-01T21:44:11Z",
        "upvotes": 22
    },
    "2402.01521": {
        "url": "https://arxiv.org/abs/2402.01521",
        "title": "K-Level Reasoning with Large Language Models",
        "authors": [
            "Yadong Zhang",
            "Shaoguang Mao",
            "Tao Ge",
            "Xun Wang",
            "Yan Xia",
            "Man Lan",
            "Furu Wei"
        ],
        "abstract": "While Large Language Models (LLMs) have demonstrated their proficiency in\ncomplex reasoning tasks, their performance in dynamic, interactive, and\ncompetitive scenarios - such as business strategy and stock market analysis -\nremains underexplored. To bridge this gap, we formally explore the dynamic\nreasoning capabilities of LLMs for decision-making in rapidly evolving\nenvironments. We introduce two game theory-based pilot challenges that mirror\nthe complexities of real-world dynamic decision-making. These challenges are\nwell-defined, enabling clear, controllable, and precise evaluation of LLMs'\ndynamic reasoning abilities. Through extensive experiments, we find that\nexisting reasoning methods tend to falter in dynamic settings that require\nk-level thinking - a key concept not tackled by previous works. To address\nthis, we propose a novel reasoning approach for LLMs, named \"K-Level\nReasoning\". This approach adopts the perspective of rivals to recursively\nemploy k-level thinking based on available historical information, which\nsignificantly improves the prediction accuracy of rivals' subsequent moves and\ninforms more strategic decision-making. This research not only sets a robust\nquantitative benchmark for the assessment of dynamic reasoning but also\nmarkedly enhances the proficiency of LLMs in dynamic contexts.",
        "publication_date": "2024-02-02T16:07:05Z",
        "upvotes": 16
    },
    "2402.01613": {
        "url": "https://arxiv.org/abs/2402.01613",
        "title": "Nomic Embed: Training a Reproducible Long Context Text Embedder",
        "authors": [
            "Zach Nussbaum",
            "John X. Morris",
            "Brandon Duderstadt",
            "Andriy Mulyar"
        ],
        "abstract": "This technical report describes the training of nomic-embed-text-v1, the\nfirst fully reproducible, open-source, open-weights, open-data, 8192 context\nlength English text embedding model that outperforms both OpenAI Ada-002 and\nOpenAI text-embedding-3-small on short and long-context tasks. We release the\ntraining code and model weights under an Apache 2 license. In contrast with\nother open-source models, we release a training data loader with 235 million\ncurated text pairs that allows for the full replication of nomic-embed-text-v1.\nYou can find code and data to replicate the model at\nhttps://github.com/nomic-ai/contrastors",
        "publication_date": "2024-02-02T18:23:18Z",
        "upvotes": 12
    },
    "2402.00892": {
        "url": "https://arxiv.org/abs/2402.00892",
        "title": "EVA-GAN: Enhanced Various Audio Generation via Scalable Generative\n  Adversarial Networks",
        "authors": [
            "Shijia Liao",
            "Shiyi Lan",
            "Arun George Zachariah"
        ],
        "abstract": "The advent of Large Models marks a new era in machine learning, significantly\noutperforming smaller models by leveraging vast datasets to capture and\nsynthesize complex patterns. Despite these advancements, the exploration into\nscaling, especially in the audio generation domain, remains limited, with\nprevious efforts didn't extend into the high-fidelity (HiFi) 44.1kHz domain and\nsuffering from both spectral discontinuities and blurriness in the\nhigh-frequency domain, alongside a lack of robustness against out-of-domain\ndata. These limitations restrict the applicability of models to diverse use\ncases, including music and singing generation. Our work introduces Enhanced\nVarious Audio Generation via Scalable Generative Adversarial Networks\n(EVA-GAN), yields significant improvements over previous state-of-the-art in\nspectral and high-frequency reconstruction and robustness in out-of-domain data\nperformance, enabling the generation of HiFi audios by employing an extensive\ndataset of 36,000 hours of 44.1kHz audio, a context-aware module, a\nHuman-In-The-Loop artifact measurement toolkit, and expands the model to\napproximately 200 million parameters. Demonstrations of our work are available\nat https://double-blind-eva-gan.cc.",
        "publication_date": "2024-01-31T03:31:03Z",
        "upvotes": 9
    },
    "2402.03300": {
        "url": "https://arxiv.org/abs/2402.03300",
        "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open\n  Language Models",
        "authors": [
            "Zhihong Shao",
            "Peiyi Wang",
            "Qihao Zhu",
            "Runxin Xu",
            "Junxiao Song",
            "Mingchuan Zhang",
            "Y. K. Li",
            "Y. Wu",
            "Daya Guo"
        ],
        "abstract": "Mathematical reasoning poses a significant challenge for language models due\nto its complex and structured nature. In this paper, we introduce DeepSeekMath\n7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B\nmath-related tokens sourced from Common Crawl, together with natural language\nand code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the\ncompetition-level MATH benchmark without relying on external toolkits and\nvoting techniques, approaching the performance level of Gemini-Ultra and GPT-4.\nSelf-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH.\nThe mathematical reasoning capability of DeepSeekMath is attributed to two key\nfactors: First, we harness the significant potential of publicly available web\ndata through a meticulously engineered data selection pipeline. Second, we\nintroduce Group Relative Policy Optimization (GRPO), a variant of Proximal\nPolicy Optimization (PPO), that enhances mathematical reasoning abilities while\nconcurrently optimizing the memory usage of PPO.",
        "publication_date": "2024-02-05T18:55:32Z",
        "upvotes": 60
    },
    "2402.03286": {
        "url": "https://arxiv.org/abs/2402.03286",
        "title": "Training-Free Consistent Text-to-Image Generation",
        "authors": [
            "Yoad Tewel",
            "Omri Kaduri",
            "Rinon Gal",
            "Yoni Kasten",
            "Lior Wolf",
            "Gal Chechik",
            "Yuval Atzmon"
        ],
        "abstract": "Text-to-image models offer a new level of creative flexibility by allowing\nusers to guide the image generation process through natural language. However,\nusing these models to consistently portray the same subject across diverse\nprompts remains challenging. Existing approaches fine-tune the model to teach\nit new words that describe specific user-provided subjects or add image\nconditioning to the model. These methods require lengthy per-subject\noptimization or large-scale pre-training. Moreover, they struggle to align\ngenerated images with text prompts and face difficulties in portraying multiple\nsubjects. Here, we present ConsiStory, a training-free approach that enables\nconsistent subject generation by sharing the internal activations of the\npretrained model. We introduce a subject-driven shared attention block and\ncorrespondence-based feature injection to promote subject consistency between\nimages. Additionally, we develop strategies to encourage layout diversity while\nmaintaining subject consistency. We compare ConsiStory to a range of baselines,\nand demonstrate state-of-the-art performance on subject consistency and text\nalignment, without requiring a single optimization step. Finally, ConsiStory\ncan naturally extend to multi-subject scenarios, and even enable training-free\npersonalization for common objects.",
        "publication_date": "2024-02-05T18:42:34Z",
        "upvotes": 60
    },
    "2402.01739": {
        "url": "https://arxiv.org/abs/2402.01739",
        "title": "OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models",
        "authors": [
            "Fuzhao Xue",
            "Zian Zheng",
            "Yao Fu",
            "Jinjie Ni",
            "Zangwei Zheng",
            "Wangchunshu Zhou",
            "Yang You"
        ],
        "abstract": "To help the open-source community have a better understanding of\nMixture-of-Experts (MoE) based large language models (LLMs), we train and\nrelease OpenMoE, a series of fully open-sourced and reproducible decoder-only\nMoE LLMs, ranging from 650M to 34B parameters and trained on up to over 1T\ntokens. Our investigation confirms that MoE-based LLMs can offer a more\nfavorable cost-effectiveness trade-off than dense LLMs, highlighting the\npotential effectiveness for future LLM development.\n  One more important contribution of this study is an in-depth analysis of the\nrouting mechanisms within our OpenMoE models, leading to three significant\nfindings: Context-Independent Specialization, Early Routing Learning, and\nDrop-towards-the-End. We discovered that routing decisions in MoE models are\npredominantly based on token IDs, with minimal context relevance. The\ntoken-to-expert assignments are determined early in the pre-training phase and\nremain largely unchanged. This imperfect routing can result in performance\ndegradation, particularly in sequential tasks like multi-turn conversations,\nwhere tokens appearing later in a sequence are more likely to be dropped.\nFinally, we rethink our design based on the above-mentioned observations and\nanalysis. To facilitate future MoE LLM development, we propose potential\nstrategies for mitigating the issues we found and further improving\noff-the-shelf MoE LLM designs.",
        "publication_date": "2024-01-29T12:05:02Z",
        "upvotes": 26
    },
    "2402.01771": {
        "url": "https://arxiv.org/abs/2402.01771",
        "title": "BlackMamba: Mixture of Experts for State-Space Models",
        "authors": [
            "Quentin Anthony",
            "Yury Tokpanov",
            "Paolo Glorioso",
            "Beren Millidge"
        ],
        "abstract": "State-space models (SSMs) have recently demonstrated competitive performance\nto transformers at large-scale language modeling benchmarks while achieving\nlinear time and memory complexity as a function of sequence length. Mamba, a\nrecently released SSM model, shows impressive performance in both language\nmodeling and long sequence processing tasks. Simultaneously, mixture-of-expert\n(MoE) models have shown remarkable performance while significantly reducing the\ncompute and latency costs of inference at the expense of a larger memory\nfootprint. In this paper, we present BlackMamba, a novel architecture that\ncombines the Mamba SSM with MoE to obtain the benefits of both. We demonstrate\nthat BlackMamba performs competitively against both Mamba and transformer\nbaselines, and outperforms in inference and training FLOPs. We fully train and\nopen-source 340M/1.5B and 630M/2.8B BlackMamba models on 300B tokens of a\ncustom dataset. We show that BlackMamba inherits and combines both of the\nbenefits of SSM and MoE architectures, combining linear-complexity generation\nfrom SSM with cheap and fast inference from MoE. We release all weights,\ncheckpoints, and inference code open-source. Inference code at:\nhttps://github.com/Zyphra/BlackMamba",
        "publication_date": "2024-02-01T07:15:58Z",
        "upvotes": 22
    },
    "2402.01878": {
        "url": "https://arxiv.org/abs/2402.01878",
        "title": "LiPO: Listwise Preference Optimization through Learning-to-Rank",
        "authors": [
            "Tianqi Liu",
            "Zhen Qin",
            "Junru Wu",
            "Jiaming Shen",
            "Misha Khalman",
            "Rishabh Joshi",
            "Yao Zhao",
            "Mohammad Saleh",
            "Simon Baumgartner",
            "Jialu Liu",
            "Peter J. Liu",
            "Xuanhui Wang"
        ],
        "abstract": "Aligning language models (LMs) with curated human feedback is critical to\ncontrol their behaviors in real-world applications. Several recent policy\noptimization methods, such as DPO and SLiC, serve as promising alternatives to\nthe traditional Reinforcement Learning from Human Feedback (RLHF) approach. In\npractice, human feedback often comes in a format of a ranked list over multiple\nresponses to amortize the cost of reading prompt. Multiple responses can also\nbe ranked by reward models or AI feedback. There lacks such a study on directly\nfitting upon a list of responses. In this work, we formulate the LM alignment\nas a listwise ranking problem and describe the Listwise Preference Optimization\n(LiPO) framework, where the policy can potentially learn more effectively from\na ranked list of plausible responses given the prompt. This view draws an\nexplicit connection to Learning-to-Rank (LTR), where most existing preference\noptimization work can be mapped to existing ranking objectives, especially\npairwise ones. Following this connection, we provide an examination of ranking\nobjectives that are not well studied for LM alignment withDPO and SLiC as\nspecial cases when list size is two. In particular, we highlight a specific\nmethod, LiPO-{\\lambda}, which leverages a state-of-the-art listwise ranking\nobjective and weights each preference pair in a more advanced manner. We show\nthat LiPO-{\\lambda} can outperform DPO and SLiC by a clear margin on two\npreference alignment tasks.",
        "publication_date": "2024-02-02T20:08:10Z",
        "upvotes": 19
    },
    "2402.01761": {
        "url": "https://arxiv.org/abs/2402.01761",
        "title": "Rethinking Interpretability in the Era of Large Language Models",
        "authors": [
            "Chandan Singh",
            "Jeevana Priya Inala",
            "Michel Galley",
            "Rich Caruana",
            "Jianfeng Gao"
        ],
        "abstract": "Interpretable machine learning has exploded as an area of interest over the\nlast decade, sparked by the rise of increasingly large datasets and deep neural\nnetworks. Simultaneously, large language models (LLMs) have demonstrated\nremarkable capabilities across a wide array of tasks, offering a chance to\nrethink opportunities in interpretable machine learning. Notably, the\ncapability to explain in natural language allows LLMs to expand the scale and\ncomplexity of patterns that can be given to a human. However, these new\ncapabilities raise new challenges, such as hallucinated explanations and\nimmense computational costs.\n  In this position paper, we start by reviewing existing methods to evaluate\nthe emerging field of LLM interpretation (both interpreting LLMs and using LLMs\nfor explanation). We contend that, despite their limitations, LLMs hold the\nopportunity to redefine interpretability with a more ambitious scope across\nmany applications, including in auditing LLMs themselves. We highlight two\nemerging research priorities for LLM interpretation: using LLMs to directly\nanalyze new datasets and to generate interactive explanations.",
        "publication_date": "2024-01-30T17:38:54Z",
        "upvotes": 18
    },
    "2402.03162": {
        "url": "https://arxiv.org/abs/2402.03162",
        "title": "Direct-a-Video: Customized Video Generation with User-Directed Camera\n  Movement and Object Motion",
        "authors": [
            "Shiyuan Yang",
            "Liang Hou",
            "Haibin Huang",
            "Chongyang Ma",
            "Pengfei Wan",
            "Di Zhang",
            "Xiaodong Chen",
            "Jing Liao"
        ],
        "abstract": "Recent text-to-video diffusion models have achieved impressive progress. In\npractice, users often desire the ability to control object motion and camera\nmovement independently for customized video creation. However, current methods\nlack the focus on separately controlling object motion and camera movement in a\ndecoupled manner, which limits the controllability and flexibility of\ntext-to-video models. In this paper, we introduce Direct-a-Video, a system that\nallows users to independently specify motions for one or multiple objects\nand/or camera movements, as if directing a video. We propose a simple yet\neffective strategy for the decoupled control of object motion and camera\nmovement. Object motion is controlled through spatial cross-attention\nmodulation using the model's inherent priors, requiring no additional\noptimization. For camera movement, we introduce new temporal cross-attention\nlayers to interpret quantitative camera movement parameters. We further employ\nan augmentation-based approach to train these layers in a self-supervised\nmanner on a small-scale dataset, eliminating the need for explicit motion\nannotation. Both components operate independently, allowing individual or\ncombined control, and can generalize to open-domain scenarios. Extensive\nexperiments demonstrate the superiority and effectiveness of our method.\nProject page: https://direct-a-video.github.io/.",
        "publication_date": "2024-02-05T16:30:57Z",
        "upvotes": 17
    },
    "2402.03040": {
        "url": "https://arxiv.org/abs/2402.03040",
        "title": "InteractiveVideo: User-Centric Controllable Video Generation with\n  Synergistic Multimodal Instructions",
        "authors": [
            "Yiyuan Zhang",
            "Yuhao Kang",
            "Zhixin Zhang",
            "Xiaohan Ding",
            "Sanyuan Zhao",
            "Xiangyu Yue"
        ],
        "abstract": "We introduce $\\textit{InteractiveVideo}$, a user-centric framework for video\ngeneration. Different from traditional generative approaches that operate based\non user-provided images or text, our framework is designed for dynamic\ninteraction, allowing users to instruct the generative model through various\nintuitive mechanisms during the whole generation process, e.g. text and image\nprompts, painting, drag-and-drop, etc. We propose a Synergistic Multimodal\nInstruction mechanism, designed to seamlessly integrate users' multimodal\ninstructions into generative models, thus facilitating a cooperative and\nresponsive interaction between user inputs and the generative process. This\napproach enables iterative and fine-grained refinement of the generation result\nthrough precise and effective user instructions. With\n$\\textit{InteractiveVideo}$, users are given the flexibility to meticulously\ntailor key aspects of a video. They can paint the reference image, edit\nsemantics, and adjust video motions until their requirements are fully met.\nCode, models, and demo are available at\nhttps://github.com/invictus717/InteractiveVideo",
        "publication_date": "2024-02-05T14:24:46Z",
        "upvotes": 16
    },
    "2402.03310": {
        "url": "https://arxiv.org/abs/2402.03310",
        "title": "V-IRL: Grounding Virtual Intelligence in Real Life",
        "authors": [
            "Jihan Yang",
            "Runyu Ding",
            "Ellis Brown",
            "Xiaojuan Qi",
            "Saining Xie"
        ],
        "abstract": "There is a sensory gulf between the Earth that humans inhabit and the digital\nrealms in which modern AI agents are created. To develop AI agents that can\nsense, think, and act as flexibly as humans in real-world settings, it is\nimperative to bridge the realism gap between the digital and physical worlds.\nHow can we embody agents in an environment as rich and diverse as the one we\ninhabit, without the constraints imposed by real hardware and control? Towards\nthis end, we introduce V-IRL: a platform that enables agents to scalably\ninteract with the real world in a virtual yet realistic environment. Our\nplatform serves as a playground for developing agents that can accomplish\nvarious practical tasks and as a vast testbed for measuring progress in\ncapabilities spanning perception, decision-making, and interaction with\nreal-world data across the entire globe.",
        "publication_date": "2024-02-05T18:59:36Z",
        "upvotes": 14
    },
    "2402.03161": {
        "url": "https://arxiv.org/abs/2402.03161",
        "title": "Video-LaVIT: Unified Video-Language Pre-training with Decoupled\n  Visual-Motional Tokenization",
        "authors": [
            "Yang Jin",
            "Zhicheng Sun",
            "Kun Xu",
            "Kun Xu",
            "Liwei Chen",
            "Hao Jiang",
            "Quzhe Huang",
            "Chengru Song",
            "Yuliang Liu",
            "Di Zhang",
            "Yang Song",
            "Kun Gai",
            "Yadong Mu"
        ],
        "abstract": "In light of recent advances in multimodal Large Language Models (LLMs), there\nis increasing attention to scaling them from image-text data to more\ninformative real-world videos. Compared to static images, video poses unique\nchallenges for effective large-scale pre-training due to the modeling of its\nspatiotemporal dynamics. In this paper, we address such limitations in\nvideo-language pre-training with an efficient video decomposition that\nrepresents each video as keyframes and temporal motions. These are then adapted\nto an LLM using well-designed tokenizers that discretize visual and temporal\ninformation as a few tokens, thus enabling unified generative pre-training of\nvideos, images, and text. At inference, the generated tokens from the LLM are\ncarefully recovered to the original continuous pixel space to create various\nvideo content. Our proposed framework is both capable of comprehending and\ngenerating image and video content, as demonstrated by its competitive\nperformance across 13 multimodal benchmarks in image and video understanding\nand generation. Our code and models will be available at\nhttps://video-lavit.github.io.",
        "publication_date": "2024-02-05T16:30:49Z",
        "upvotes": 13
    },
    "2402.02791": {
        "url": "https://arxiv.org/abs/2402.02791",
        "title": "Rethinking Optimization and Architecture for Tiny Language Models",
        "authors": [
            "Yehui Tang",
            "Fangcheng Liu",
            "Yunsheng Ni",
            "Yuchuan Tian",
            "Zheyuan Bai",
            "Yi-Qi Hu",
            "Sichao Liu",
            "Shangling Jui",
            "Kai Han",
            "Yunhe Wang"
        ],
        "abstract": "The power of large language models (LLMs) has been demonstrated through\nnumerous data and computing resources. However, the application of language\nmodels on mobile devices is facing huge challenge on the computation and memory\ncosts, that is, tiny language models with high performance are urgently\nrequired. Limited by the highly complex training process, there are many\ndetails for optimizing language models that are seldom studied carefully. In\nthis study, based on a tiny language model with 1B parameters, we carefully\ndesign a series of empirical study to analyze the effect of each component.\nThree perspectives are mainly discussed, \\ie, neural architecture, parameter\ninitialization, and optimization strategy. Several design formulas are\nempirically proved especially effective for tiny language models, including\ntokenizer compression, architecture tweaking, parameter inheritance and\nmultiple-round training. Then we train PanGu-$\\pi$-1B Pro and PanGu-$\\pi$-1.5B\nPro on 1.6T multilingual corpora, following the established formulas.\nExperimental results demonstrate the improved optimization and architecture\nyield a notable average improvement of 8.87 on benchmark evaluation sets for\nPanGu-$\\pi$-1B Pro. Besides, PanGu-$\\pi$-1.5B Pro surpasses a range of SOTA\nmodels with larger model sizes, validating its superior performance. The code\nis available at https://github.com/YuchuanTian/RethinkTinyLM.",
        "publication_date": "2024-02-05T07:59:38Z",
        "upvotes": 12
    },
    "2402.01935": {
        "url": "https://arxiv.org/abs/2402.01935",
        "title": "Code Representation Learning At Scale",
        "authors": [
            "Dejiao Zhang",
            "Wasi Ahmad",
            "Ming Tan",
            "Hantian Ding",
            "Ramesh Nallapati",
            "Dan Roth",
            "Xiaofei Ma",
            "Bing Xiang"
        ],
        "abstract": "Recent studies have shown that code language models at scale demonstrate\nsignificant performance gains on downstream tasks, i.e., code generation.\nHowever, most of the existing works on code representation learning train\nmodels at a hundred million parameter scale using very limited pretraining\ncorpora. In this work, we fuel code representation learning with a vast amount\nof code data via a two-stage pretraining scheme. We first train the encoders\nvia a mix that leverages both randomness in masking language modeling and the\nstructure aspect of programming language. We then enhance the representations\nvia contrastive learning with hard negative and hard positive constructed in an\nunsupervised manner. We establish an off-the-shelf encoder model that\npersistently outperforms the existing models on a wide variety of downstream\ntasks by large margins. To comprehend the factors contributing to successful\ncode representation learning, we conduct detailed ablations and share our\nfindings on (i) a customized and effective token-level denoising scheme for\nsource code; (ii) the importance of hard negatives and hard positives; (iii)\nhow the proposed bimodal contrastive learning boost the cross-lingual semantic\nsearch performance; and (iv) how the pretraining schemes decide the downstream\ntask performance scales with the model size.",
        "publication_date": "2024-02-02T22:19:15Z",
        "upvotes": 12
    },
    "2402.01831": {
        "url": "https://arxiv.org/abs/2402.01831",
        "title": "Audio Flamingo: A Novel Audio Language Model with Few-Shot Learning and\n  Dialogue Abilities",
        "authors": [
            "Zhifeng Kong",
            "Arushi Goel",
            "Rohan Badlani",
            "Wei Ping",
            "Rafael Valle",
            "Bryan Catanzaro"
        ],
        "abstract": "Augmenting large language models (LLMs) to understand audio -- including\nnon-speech sounds and non-verbal speech -- is critically important for diverse\nreal-world applications of LLMs. In this paper, we propose Audio Flamingo, a\nnovel audio language model with 1) strong audio understanding abilities, 2) the\nability to quickly adapt to unseen tasks via in-context learning and retrieval,\nand 3) strong multi-turn dialogue abilities. We introduce a series of training\ntechniques, architecture design, and data strategies to enhance our model with\nthese abilities. Extensive evaluations across various audio understanding tasks\nconfirm the efficacy of our method, setting new state-of-the-art benchmarks.\nOur demo website is: \\url{https://audioflamingo.github.io/}.",
        "publication_date": "2024-02-02T18:58:34Z",
        "upvotes": 11
    },
    "2402.02834": {
        "url": "https://arxiv.org/abs/2402.02834",
        "title": "Shortened LLaMA: A Simple Depth Pruning for Large Language Models",
        "authors": [
            "Bo-Kyeong Kim",
            "Geonmin Kim",
            "Tae-Ho Kim",
            "Thibault Castells",
            "Shinkook Choi",
            "Junho Shin",
            "Hyoung-Kyu Song"
        ],
        "abstract": "Structured pruning of modern large language models (LLMs) has emerged as a\nway of decreasing their high computational needs. Width pruning reduces the\nsize of projection weight matrices (e.g., by removing attention heads) while\nmaintaining the number of layers. Depth pruning, in contrast, removes entire\nlayers or blocks, while keeping the size of the remaining weights unchanged.\nMost current research focuses on either width-only or a blend of width and\ndepth pruning, with little comparative analysis between the two units (width\nvs. depth) concerning their impact on LLM inference efficiency. In this work,\nwe show that a simple depth pruning approach can compete with recent width\npruning methods in terms of zero-shot task performance. Our pruning method\nboosts inference speeds, especially under memory-constrained conditions that\nrequire limited batch sizes for running LLMs, where width pruning is\nineffective. We hope this work can help deploy LLMs on local and edge devices.",
        "publication_date": "2024-02-05T09:44:49Z",
        "upvotes": 10
    },
    "2402.02583": {
        "url": "https://arxiv.org/abs/2402.02583",
        "title": "DiffEditor: Boosting Accuracy and Flexibility on Diffusion-based Image\n  Editing",
        "authors": [
            "Chong Mou",
            "Xintao Wang",
            "Jiechong Song",
            "Ying Shan",
            "Jian Zhang"
        ],
        "abstract": "Large-scale Text-to-Image (T2I) diffusion models have revolutionized image\ngeneration over the last few years. Although owning diverse and high-quality\ngeneration capabilities, translating these abilities to fine-grained image\nediting remains challenging. In this paper, we propose DiffEditor to rectify\ntwo weaknesses in existing diffusion-based image editing: (1) in complex\nscenarios, editing results often lack editing accuracy and exhibit unexpected\nartifacts; (2) lack of flexibility to harmonize editing operations, e.g.,\nimagine new content. In our solution, we introduce image prompts in\nfine-grained image editing, cooperating with the text prompt to better describe\nthe editing content. To increase the flexibility while maintaining content\nconsistency, we locally combine stochastic differential equation (SDE) into the\nordinary differential equation (ODE) sampling. In addition, we incorporate\nregional score-based gradient guidance and a time travel strategy into the\ndiffusion sampling, further improving the editing quality. Extensive\nexperiments demonstrate that our method can efficiently achieve\nstate-of-the-art performance on various fine-grained image editing tasks,\nincluding editing within a single image (e.g., object moving, resizing, and\ncontent dragging) and across images (e.g., appearance replacing and object\npasting). Our source code is released at\nhttps://github.com/MC-E/DragonDiffusion.",
        "publication_date": "2024-02-04T18:50:29Z",
        "upvotes": 7
    },
    "2402.03620": {
        "url": "https://arxiv.org/abs/2402.03620",
        "title": "Self-Discover: Large Language Models Self-Compose Reasoning Structures",
        "authors": [
            "Pei Zhou",
            "Jay Pujara",
            "Xiang Ren",
            "Xinyun Chen",
            "Heng-Tze Cheng",
            "Quoc V. Le",
            "Ed H. Chi",
            "Denny Zhou",
            "Swaroop Mishra",
            "Huaixiu Steven Zheng"
        ],
        "abstract": "We introduce SELF-DISCOVER, a general framework for LLMs to self-discover the\ntask-intrinsic reasoning structures to tackle complex reasoning problems that\nare challenging for typical prompting methods. Core to the framework is a\nself-discovery process where LLMs select multiple atomic reasoning modules such\nas critical thinking and step-by-step thinking, and compose them into an\nexplicit reasoning structure for LLMs to follow during decoding. SELF-DISCOVER\nsubstantially improves GPT-4 and PaLM 2's performance on challenging reasoning\nbenchmarks such as BigBench-Hard, grounded agent reasoning, and MATH, by as\nmuch as 32% compared to Chain of Thought (CoT). Furthermore, SELF-DISCOVER\noutperforms inference-intensive methods such as CoT-Self-Consistency by more\nthan 20%, while requiring 10-40x fewer inference compute. Finally, we show that\nthe self-discovered reasoning structures are universally applicable across\nmodel families: from PaLM 2-L to GPT-4, and from GPT-4 to Llama2, and share\ncommonalities with human reasoning patterns.",
        "publication_date": "2024-02-06T01:13:53Z",
        "upvotes": 99
    },
    "2402.04248": {
        "url": "https://arxiv.org/abs/2402.04248",
        "title": "Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning\n  Tasks",
        "authors": [
            "Jongho Park",
            "Jaeseung Park",
            "Zheyang Xiong",
            "Nayoung Lee",
            "Jaewoong Cho",
            "Samet Oymak",
            "Kangwook Lee",
            "Dimitris Papailiopoulos"
        ],
        "abstract": "State-space models (SSMs), such as Mamba Gu & Dao (2034), have been proposed\nas alternatives to Transformer networks in language modeling, by incorporating\ngating, convolutions, and input-dependent token selection to mitigate the\nquadratic cost of multi-head attention. Although SSMs exhibit competitive\nperformance, their in-context learning (ICL) capabilities, a remarkable\nemergent property of modern language models that enables task execution without\nparameter optimization, remain underexplored compared to Transformers. In this\nstudy, we evaluate the ICL performance of SSMs, focusing on Mamba, against\nTransformer models across various tasks. Our results show that SSMs perform\ncomparably to Transformers in standard regression ICL tasks, while\noutperforming them in tasks like sparse parity learning. However, SSMs fall\nshort in tasks involving non-standard retrieval functionality. To address these\nlimitations, we introduce a hybrid model, \\variant, that combines Mamba with\nattention blocks, surpassing individual models in tasks where they struggle\nindependently. Our findings suggest that hybrid architectures offer promising\navenues for enhancing ICL in language models.",
        "publication_date": "2024-02-06T18:56:35Z",
        "upvotes": 23
    },
    "2402.04252": {
        "url": "https://arxiv.org/abs/2402.04252",
        "title": "EVA-CLIP-18B: Scaling CLIP to 18 Billion Parameters",
        "authors": [
            "Quan Sun",
            "Jinsheng Wang",
            "Qiying Yu",
            "Yufeng Cui",
            "Fan Zhang",
            "Xiaosong Zhang",
            "Xinlong Wang"
        ],
        "abstract": "Scaling up contrastive language-image pretraining (CLIP) is critical for\nempowering both vision and multimodal models. We present EVA-CLIP-18B, the\nlargest and most powerful open-source CLIP model to date, with 18-billion\nparameters. With only 6-billion training samples seen, EVA-CLIP-18B achieves an\nexceptional 80.7% zero-shot top-1 accuracy averaged across 27 widely recognized\nimage classification benchmarks, outperforming its forerunner EVA-CLIP\n(5-billion parameters) and other open-source CLIP models by a large margin.\nRemarkably, we observe a consistent performance improvement with the model size\nscaling of EVA-CLIP, despite maintaining a constant training dataset of\n2-billion image-text pairs from LAION-2B and COYO-700M. This dataset is openly\navailable and much smaller than the in-house datasets (e.g., DFN-5B, WebLI-10B)\nemployed in other state-of-the-art CLIP models. EVA-CLIP-18B demonstrates the\npotential of EVA-style weak-to-strong visual model scaling. With our model\nweights made publicly available, we hope to facilitate future research in\nvision and multimodal foundation models.",
        "publication_date": "2024-02-06T18:59:48Z",
        "upvotes": 20
    },
    "2402.04229": {
        "url": "https://arxiv.org/abs/2402.04229",
        "title": "MusicRL: Aligning Music Generation to Human Preferences",
        "authors": [
            "Geoffrey Cideron",
            "Sertan Girgin",
            "Mauro Verzetti",
            "Damien Vincent",
            "Matej Kastelic",
            "Zal\u00e1n Borsos",
            "Brian McWilliams",
            "Victor Ungureanu",
            "Olivier Bachem",
            "Olivier Pietquin",
            "Matthieu Geist",
            "L\u00e9onard Hussenot",
            "Neil Zeghidour",
            "Andrea Agostinelli"
        ],
        "abstract": "We propose MusicRL, the first music generation system finetuned from human\nfeedback. Appreciation of text-to-music models is particularly subjective since\nthe concept of musicality as well as the specific intention behind a caption\nare user-dependent (e.g. a caption such as \"upbeat work-out music\" can map to a\nretro guitar solo or a techno pop beat). Not only this makes supervised\ntraining of such models challenging, but it also calls for integrating\ncontinuous human feedback in their post-deployment finetuning. MusicRL is a\npretrained autoregressive MusicLM (Agostinelli et al., 2023) model of discrete\naudio tokens finetuned with reinforcement learning to maximise sequence-level\nrewards. We design reward functions related specifically to text-adherence and\naudio quality with the help from selected raters, and use those to finetune\nMusicLM into MusicRL-R. We deploy MusicLM to users and collect a substantial\ndataset comprising 300,000 pairwise preferences. Using Reinforcement Learning\nfrom Human Feedback (RLHF), we train MusicRL-U, the first text-to-music model\nthat incorporates human feedback at scale. Human evaluations show that both\nMusicRL-R and MusicRL-U are preferred to the baseline. Ultimately, MusicRL-RU\ncombines the two approaches and results in the best model according to human\nraters. Ablation studies shed light on the musical attributes influencing human\npreferences, indicating that text adherence and quality only account for a part\nof it. This underscores the prevalence of subjectivity in musical appreciation\nand calls for further involvement of human listeners in the finetuning of music\ngeneration models.",
        "publication_date": "2024-02-06T18:36:52Z",
        "upvotes": 16
    },
    "2402.04177": {
        "url": "https://arxiv.org/abs/2402.04177",
        "title": "Scaling Laws for Downstream Task Performance of Large Language Models",
        "authors": [
            "Berivan Isik",
            "Natalia Ponomareva",
            "Hussein Hazimeh",
            "Dimitris Paparas",
            "Sergei Vassilvitskii",
            "Sanmi Koyejo"
        ],
        "abstract": "Scaling laws provide important insights that can guide the design of large\nlanguage models (LLMs). Existing work has primarily focused on studying scaling\nlaws for pretraining (upstream) loss. However, in transfer learning settings,\nin which LLMs are pretrained on an unsupervised dataset and then finetuned on a\ndownstream task, we often also care about the downstream performance. In this\nwork, we study the scaling behavior in a transfer learning setting, where LLMs\nare finetuned for machine translation tasks. Specifically, we investigate how\nthe choice of the pretraining data and its size affect downstream performance\n(translation quality) as judged by two metrics: downstream cross-entropy and\nBLEU score. Our experiments indicate that the size of the finetuning dataset\nand the distribution alignment between the pretraining and downstream data\nsignificantly influence the scaling behavior. With sufficient alignment, both\ndownstream cross-entropy and BLEU score improve monotonically with more\npretraining data. In such cases, we show that it is possible to predict the\ndownstream BLEU score with good accuracy using a log-law. However, there are\nalso cases where moderate misalignment causes the BLEU score to fluctuate or\nget worse with more pretraining, whereas downstream cross-entropy monotonically\nimproves. By analyzing these observations, we provide new practical insights\nfor choosing appropriate pretraining data.",
        "publication_date": "2024-02-06T17:31:20Z",
        "upvotes": 16
    },
    "2402.03766": {
        "url": "https://arxiv.org/abs/2402.03766",
        "title": "MobileVLM V2: Faster and Stronger Baseline for Vision Language Model",
        "authors": [
            "Xiangxiang Chu",
            "Limeng Qiao",
            "Xinyu Zhang",
            "Shuang Xu",
            "Fei Wei",
            "Yang Yang",
            "Xiaofei Sun",
            "Yiming Hu",
            "Xinyang Lin",
            "Bo Zhang",
            "Chunhua Shen"
        ],
        "abstract": "We introduce MobileVLM V2, a family of significantly improved vision language\nmodels upon MobileVLM, which proves that a delicate orchestration of novel\narchitectural design, an improved training scheme tailored for mobile VLMs, and\nrich high-quality dataset curation can substantially benefit VLMs' performance.\nSpecifically, MobileVLM V2 1.7B achieves better or on-par performance on\nstandard VLM benchmarks compared with much larger VLMs at the 3B scale.\nNotably, our 3B model outperforms a large variety of VLMs at the 7B+ scale. Our\nmodels will be released at https://github.com/Meituan-AutoML/MobileVLM .",
        "publication_date": "2024-02-06T07:16:36Z",
        "upvotes": 9
    },
    "2402.03749": {
        "url": "https://arxiv.org/abs/2402.03749",
        "title": "Vision Superalignment: Weak-to-Strong Generalization for Vision\n  Foundation Models",
        "authors": [
            "Jianyuan Guo",
            "Hanting Chen",
            "Chengcheng Wang",
            "Kai Han",
            "Chang Xu",
            "Yunhe Wang"
        ],
        "abstract": "Recent advancements in large language models have sparked interest in their\nextraordinary and near-superhuman capabilities, leading researchers to explore\nmethods for evaluating and optimizing these abilities, which is called\nsuperalignment. In this context, our paper delves into the realm of vision\nfoundation models, focusing on the concept of weak-to-strong generalization,\nwhich involves using a weaker model to supervise a stronger one, aiming to\nenhance the latter's capabilities beyond the former's limits. We introduce a\nnovel and adaptively adjustable loss function for weak-to-strong supervision.\nOur comprehensive experiments span various scenarios, including few-shot\nlearning, transfer learning, noisy label learning, and common knowledge\ndistillation settings. The results are striking: our approach not only exceeds\nthe performance benchmarks set by strong-to-strong generalization but also\nsurpasses the outcomes of fine-tuning strong models with whole datasets. This\ncompelling evidence underscores the significant potential of weak-to-strong\ngeneralization, showcasing its capability to substantially elevate the\nperformance of vision foundation models. The code is available at\nhttps://github.com/ggjy/vision_weak_to_strong.",
        "publication_date": "2024-02-06T06:30:34Z",
        "upvotes": 9
    },
    "2402.04141": {
        "url": "https://arxiv.org/abs/2402.04141",
        "title": "Multi-line AI-assisted Code Authoring",
        "authors": [
            "Omer Dunay",
            "Daniel Cheng",
            "Adam Tait",
            "Parth Thakkar",
            "Peter C Rigby",
            "Andy Chiu",
            "Imad Ahmad",
            "Arun Ganesan",
            "Chandra Maddila",
            "Vijayaraghavan Murali",
            "Ali Tayyebi",
            "Nachiappan Nagappan"
        ],
        "abstract": "CodeCompose is an AI-assisted code authoring tool powered by large language\nmodels (LLMs) that provides inline suggestions to 10's of thousands of\ndevelopers at Meta. In this paper, we present how we scaled the product from\ndisplaying single-line suggestions to multi-line suggestions. This evolution\nrequired us to overcome several unique challenges in improving the usability of\nthese suggestions for developers.\n  First, we discuss how multi-line suggestions can have a 'jarring' effect, as\nthe LLM's suggestions constantly move around the developer's existing code,\nwhich would otherwise result in decreased productivity and satisfaction.\n  Second, multi-line suggestions take significantly longer to generate; hence\nwe present several innovative investments we made to reduce the perceived\nlatency for users. These model-hosting optimizations sped up multi-line\nsuggestion latency by 2.5x.\n  Finally, we conduct experiments on 10's of thousands of engineers to\nunderstand how multi-line suggestions impact the user experience and contrast\nthis with single-line suggestions. Our experiments reveal that (i) multi-line\nsuggestions account for 42% of total characters accepted (despite only\naccounting for 16% for displayed suggestions) (ii) multi-line suggestions\nalmost doubled the percentage of keystrokes saved for users from 9% to 17%.\nMulti-line CodeCompose has been rolled out to all engineers at Meta, and less\nthan 1% of engineers have opted out of multi-line suggestions.",
        "publication_date": "2024-02-06T16:48:50Z",
        "upvotes": 8
    },
    "2402.03570": {
        "url": "https://arxiv.org/abs/2402.03570",
        "title": "Diffusion World Model",
        "authors": [
            "Zihan Ding",
            "Amy Zhang",
            "Yuandong Tian",
            "Qinqing Zheng"
        ],
        "abstract": "We introduce Diffusion World Model (DWM), a conditional diffusion model\ncapable of predicting multistep future states and rewards concurrently. As\nopposed to traditional one-step dynamics models, DWM offers long-horizon\npredictions in a single forward pass, eliminating the need for recursive\nqueries. We integrate DWM into model-based value estimation, where the\nshort-term return is simulated by future trajectories sampled from DWM. In the\ncontext of offline reinforcement learning, DWM can be viewed as a conservative\nvalue regularization through generative modeling. Alternatively, it can be seen\nas a data source that enables offline Q-learning with synthetic data. Our\nexperiments on the D4RL dataset confirm the robustness of DWM to long-horizon\nsimulation. In terms of absolute performance, DWM significantly surpasses\none-step dynamics models with a $44\\%$ performance gain, and achieves\nstate-of-the-art performance.",
        "publication_date": "2024-02-05T22:43:57Z",
        "upvotes": 7
    },
    "2402.03944": {
        "url": "https://arxiv.org/abs/2402.03944",
        "title": "IMUSIC: IMU-based Facial Expression Capture",
        "authors": [
            "Youjia Wang",
            "Yiwen Wu",
            "Ruiqian Li",
            "Hengan Zhou",
            "Hongyang Lin",
            "Yingwenqi Jiang",
            "Yingsheng Zhu",
            "Guanpeng Long",
            "Jingya Wang",
            "Lan Xu",
            "Jingyi Yu"
        ],
        "abstract": "For facial motion capture and analysis, the dominated solutions are generally\nbased on visual cues, which cannot protect privacy and are vulnerable to\nocclusions. Inertial measurement units (IMUs) serve as potential rescues yet\nare mainly adopted for full-body motion capture. In this paper, we propose\nIMUSIC to fill the gap, a novel path for facial expression capture using purely\nIMU signals, significantly distant from previous visual solutions.The key\ndesign in our IMUSIC is a trilogy. We first design micro-IMUs to suit facial\ncapture, companion with an anatomy-driven IMU placement scheme. Then, we\ncontribute a novel IMU-ARKit dataset, which provides rich paired IMU/visual\nsignals for diverse facial expressions and performances. Such unique\nmulti-modality brings huge potential for future directions like IMU-based\nfacial behavior analysis. Moreover, utilizing IMU-ARKit, we introduce a strong\nbaseline approach to accurately predict facial blendshape parameters from\npurely IMU signals. Specifically, we tailor a Transformer diffusion model with\na two-stage training strategy for this novel tracking task. The IMUSIC\nframework empowers us to perform accurate facial capture in scenarios where\nvisual methods falter and simultaneously safeguard user privacy. We conduct\nextensive experiments about both the IMU configuration and technical components\nto validate the effectiveness of our IMUSIC approach. Notably, IMUSIC enables\nvarious potential and novel applications, i.e., privacy-protecting facial\ncapture, hybrid capture against occlusions, or detecting minute facial\nmovements that are often invisible through visual cues. We will release our\ndataset and implementations to enrich more possibilities of facial capture and\nanalysis in our community.",
        "publication_date": "2024-02-03T14:27:18Z",
        "upvotes": 5
    },
    "2402.03908": {
        "url": "https://arxiv.org/abs/2402.03908",
        "title": "EscherNet: A Generative Model for Scalable View Synthesis",
        "authors": [
            "Xin Kong",
            "Shikun Liu",
            "Xiaoyang Lyu",
            "Marwan Taher",
            "Xiaojuan Qi",
            "Andrew J. Davison"
        ],
        "abstract": "We introduce EscherNet, a multi-view conditioned diffusion model for view\nsynthesis. EscherNet learns implicit and generative 3D representations coupled\nwith a specialised camera positional encoding, allowing precise and continuous\nrelative control of the camera transformation between an arbitrary number of\nreference and target views. EscherNet offers exceptional generality,\nflexibility, and scalability in view synthesis -- it can generate more than 100\nconsistent target views simultaneously on a single consumer-grade GPU, despite\nbeing trained with a fixed number of 3 reference views to 3 target views. As a\nresult, EscherNet not only addresses zero-shot novel view synthesis, but also\nnaturally unifies single- and multi-image 3D reconstruction, combining these\ndiverse tasks into a single, cohesive framework. Our extensive experiments\ndemonstrate that EscherNet achieves state-of-the-art performance in multiple\nbenchmarks, even when compared to methods specifically tailored for each\nindividual problem. This remarkable versatility opens up new directions for\ndesigning scalable neural architectures for 3D vision. Project page:\nhttps://kxhit.github.io/EscherNet.",
        "publication_date": "2024-02-06T11:21:58Z",
        "upvotes": 5
    },
    "2402.04236": {
        "url": "https://arxiv.org/abs/2402.04236",
        "title": "CogCoM: Train Large Vision-Language Models Diving into Details through\n  Chain of Manipulations",
        "authors": [
            "Ji Qi",
            "Ming Ding",
            "Weihan Wang",
            "Yushi Bai",
            "Qingsong Lv",
            "Wenyi Hong",
            "Bin Xu",
            "Lei Hou",
            "Juanzi Li",
            "Yuxiao Dong",
            "Jie Tang"
        ],
        "abstract": "Vision-Language Models (VLMs) have demonstrated their widespread viability\nthanks to extensive training in aligning visual instructions to answers.\nHowever, this conclusive alignment leads models to ignore critical visual\nreasoning, and further result in failures on meticulous visual problems and\nunfaithful responses. In this paper, we propose Chain of Manipulations, a\nmechanism that enables VLMs to solve problems with a series of manipulations,\nwhere each manipulation refers to an operation on the visual input, either from\nintrinsic abilities (e.g., grounding) acquired through prior training or from\nimitating human-like behaviors (e.g., zoom in). This mechanism encourages VLMs\nto generate faithful responses with evidential visual reasoning, and permits\nusers to trace error causes in the interpretable paths. We thus train CogCoM, a\ngeneral 17B VLM with a memory-based compatible architecture endowed this\nreasoning mechanism. Experiments show that our model achieves the\nstate-of-the-art performance across 8 benchmarks from 3 categories, and a\nlimited number of training steps with the data swiftly gains a competitive\nperformance. The code and data are publicly available at\nhttps://github.com/THUDM/CogCoM.",
        "publication_date": "2024-02-06T18:43:48Z",
        "upvotes": 5
    },
    "2402.04494": {
        "url": "https://arxiv.org/abs/2402.04494",
        "title": "Grandmaster-Level Chess Without Search",
        "authors": [
            "Anian Ruoss",
            "Gr\u00e9goire Del\u00e9tang",
            "Sourabh Medapati",
            "Jordi Grau-Moya",
            "Li Kevin Wenliang",
            "Elliot Catt",
            "John Reid",
            "Tim Genewein"
        ],
        "abstract": "The recent breakthrough successes in machine learning are mainly attributed\nto scale: namely large-scale attention-based architectures and datasets of\nunprecedented scale. This paper investigates the impact of training at scale\nfor chess. Unlike traditional chess engines that rely on complex heuristics,\nexplicit search, or a combination of both, we train a 270M parameter\ntransformer model with supervised learning on a dataset of 10 million chess\ngames. We annotate each board in the dataset with action-values provided by the\npowerful Stockfish 16 engine, leading to roughly 15 billion data points. Our\nlargest model reaches a Lichess blitz Elo of 2895 against humans, and\nsuccessfully solves a series of challenging chess puzzles, without any\ndomain-specific tweaks or explicit search algorithms. We also show that our\nmodel outperforms AlphaZero's policy and value networks (without MCTS) and\nGPT-3.5-turbo-instruct. A systematic investigation of model and dataset size\nshows that strong chess performance only arises at sufficient scale. To\nvalidate our results, we perform an extensive series of ablations of design\nchoices and hyperparameters.",
        "publication_date": "2024-02-07T00:36:24Z",
        "upvotes": 61
    },
    "2402.04291": {
        "url": "https://arxiv.org/abs/2402.04291",
        "title": "BiLLM: Pushing the Limit of Post-Training Quantization for LLMs",
        "authors": [
            "Wei Huang",
            "Yangdong Liu",
            "Haotong Qin",
            "Ying Li",
            "Shiming Zhang",
            "Xianglong Liu",
            "Michele Magno",
            "Xiaojuan Qi"
        ],
        "abstract": "Pretrained large language models (LLMs) exhibit exceptional general language\nprocessing capabilities but come with significant demands on memory and\ncomputational resources. As a powerful compression technology, binarization can\nextremely reduce model weights to a mere 1 bit, lowering the expensive\ncomputation and memory requirements. However, existing quantization techniques\nfall short of maintaining LLM performance under ultra-low bit-widths. In\nresponse to this challenge, we present BiLLM, a groundbreaking 1-bit\npost-training quantization scheme tailored for pretrained LLMs. Based on the\nweight distribution of LLMs, BiLLM first identifies and structurally selects\nsalient weights, and minimizes the compression loss through an effective binary\nresidual approximation strategy. Moreover, considering the bell-shaped\ndistribution of the non-salient weights, we propose an optimal splitting search\nto group and binarize them accurately. BiLLM achieving for the first time\nhigh-accuracy inference (e.g. 8.41 perplexity on LLaMA2-70B) with only 1.08-bit\nweights across various LLMs families and evaluation metrics, outperforms SOTA\nquantization methods of LLM by significant margins. Moreover, BiLLM enables the\nbinarization process of the LLM with 7 billion weights within 0.5 hours on a\nsingle GPU, demonstrating satisfactory time efficiency.",
        "publication_date": "2024-02-06T09:26:34Z",
        "upvotes": 48
    },
    "2402.04615": {
        "url": "https://arxiv.org/abs/2402.04615",
        "title": "ScreenAI: A Vision-Language Model for UI and Infographics Understanding",
        "authors": [
            "Gilles Baechler",
            "Srinivas Sunkara",
            "Maria Wang",
            "Fedir Zubach",
            "Hassan Mansoor",
            "Vincent Etter",
            "Victor C\u0103rbune",
            "Jason Lin",
            "Jindong Chen",
            "Abhanshu Sharma"
        ],
        "abstract": "Screen user interfaces (UIs) and infographics, sharing similar visual\nlanguage and design principles, play important roles in human communication and\nhuman-machine interaction. We introduce ScreenAI, a vision-language model that\nspecializes in UI and infographics understanding. Our model improves upon the\nPaLI architecture with the flexible patching strategy of pix2struct and is\ntrained on a unique mixture of datasets. At the heart of this mixture is a\nnovel screen annotation task in which the model has to identify the type and\nlocation of UI elements. We use these text annotations to describe screens to\nLarge Language Models and automatically generate question-answering (QA), UI\nnavigation, and summarization training datasets at scale. We run ablation\nstudies to demonstrate the impact of these design choices. At only 5B\nparameters, ScreenAI achieves new state-of-the-artresults on UI- and\ninfographics-based tasks (Multi-page DocVQA, WebSRC, MoTIF and Widget\nCaptioning), and new best-in-class performance on others (Chart QA, DocVQA, and\nInfographicVQA) compared to models of similar size. Finally, we release three\nnew datasets: one focused on the screen annotation task and two others focused\non question answering.",
        "publication_date": "2024-02-07T06:42:33Z",
        "upvotes": 28
    },
    "2402.04792": {
        "url": "https://arxiv.org/abs/2402.04792",
        "title": "Direct Language Model Alignment from Online AI Feedback",
        "authors": [
            "Shangmin Guo",
            "Biao Zhang",
            "Tianlin Liu",
            "Tianqi Liu",
            "Misha Khalman",
            "Felipe Llinares",
            "Alexandre Rame",
            "Thomas Mesnard",
            "Yao Zhao",
            "Bilal Piot",
            "Johan Ferret",
            "Mathieu Blondel"
        ],
        "abstract": "Direct alignment from preferences (DAP) methods, such as DPO, have recently\nemerged as efficient alternatives to reinforcement learning from human feedback\n(RLHF), that do not require a separate reward model. However, the preference\ndatasets used in DAP methods are usually collected ahead of training and never\nupdated, thus the feedback is purely offline. Moreover, responses in these\ndatasets are often sampled from a language model distinct from the one being\naligned, and since the model evolves over training, the alignment phase is\ninevitably off-policy. In this study, we posit that online feedback is key and\nimproves DAP methods. Our method, online AI feedback (OAIF), uses an LLM as\nannotator: on each training iteration, we sample two responses from the current\nmodel and prompt the LLM annotator to choose which one is preferred, thus\nproviding online feedback. Despite its simplicity, we demonstrate via human\nevaluation in several tasks that OAIF outperforms both offline DAP and RLHF\nmethods. We further show that the feedback leveraged in OAIF is easily\ncontrollable, via instruction prompts to the LLM annotator.",
        "publication_date": "2024-02-07T12:31:13Z",
        "upvotes": 24
    },
    "2402.05054": {
        "url": "https://arxiv.org/abs/2402.05054",
        "title": "LGM: Large Multi-View Gaussian Model for High-Resolution 3D Content\n  Creation",
        "authors": [
            "Jiaxiang Tang",
            "Zhaoxi Chen",
            "Xiaokang Chen",
            "Tengfei Wang",
            "Gang Zeng",
            "Ziwei Liu"
        ],
        "abstract": "3D content creation has achieved significant progress in terms of both\nquality and speed. Although current feed-forward models can produce 3D objects\nin seconds, their resolution is constrained by the intensive computation\nrequired during training. In this paper, we introduce Large Multi-View Gaussian\nModel (LGM), a novel framework designed to generate high-resolution 3D models\nfrom text prompts or single-view images. Our key insights are two-fold: 1) 3D\nRepresentation: We propose multi-view Gaussian features as an efficient yet\npowerful representation, which can then be fused together for differentiable\nrendering. 2) 3D Backbone: We present an asymmetric U-Net as a high-throughput\nbackbone operating on multi-view images, which can be produced from text or\nsingle-view image input by leveraging multi-view diffusion models. Extensive\nexperiments demonstrate the high fidelity and efficiency of our approach.\nNotably, we maintain the fast speed to generate 3D objects within 5 seconds\nwhile boosting the training resolution to 512, thereby achieving\nhigh-resolution 3D content generation.",
        "publication_date": "2024-02-07T17:57:03Z",
        "upvotes": 23
    },
    "2402.04324": {
        "url": "https://arxiv.org/abs/2402.04324",
        "title": "ConsistI2V: Enhancing Visual Consistency for Image-to-Video Generation",
        "authors": [
            "Weiming Ren",
            "Harry Yang",
            "Ge Zhang",
            "Cong Wei",
            "Xinrun Du",
            "Stephen Huang",
            "Wenhu Chen"
        ],
        "abstract": "Image-to-video (I2V) generation aims to use the initial frame (alongside a\ntext prompt) to create a video sequence. A grand challenge in I2V generation is\nto maintain visual consistency throughout the video: existing methods often\nstruggle to preserve the integrity of the subject, background, and style from\nthe first frame, as well as ensure a fluid and logical progression within the\nvideo narrative. To mitigate these issues, we propose ConsistI2V, a\ndiffusion-based method to enhance visual consistency for I2V generation.\nSpecifically, we introduce (1) spatiotemporal attention over the first frame to\nmaintain spatial and motion consistency, (2) noise initialization from the\nlow-frequency band of the first frame to enhance layout consistency. These two\napproaches enable ConsistI2V to generate highly consistent videos. We also\nextend the proposed approaches to show their potential to improve consistency\nin auto-regressive long video generation and camera motion control. To verify\nthe effectiveness of our method, we propose I2V-Bench, a comprehensive\nevaluation benchmark for I2V generation. Our automatic and human evaluation\nresults demonstrate the superiority of ConsistI2V over existing methods.",
        "publication_date": "2024-02-06T19:08:18Z",
        "upvotes": 22
    },
    "2402.05008": {
        "url": "https://arxiv.org/abs/2402.05008",
        "title": "EfficientViT-SAM: Accelerated Segment Anything Model Without Performance\n  Loss",
        "authors": [
            "Zhuoyang Zhang",
            "Han Cai",
            "Song Han"
        ],
        "abstract": "We present EfficientViT-SAM, a new family of accelerated segment anything\nmodels. We retain SAM's lightweight prompt encoder and mask decoder while\nreplacing the heavy image encoder with EfficientViT. For the training, we begin\nwith the knowledge distillation from the SAM-ViT-H image encoder to\nEfficientViT. Subsequently, we conduct end-to-end training on the SA-1B\ndataset. Benefiting from EfficientViT's efficiency and capacity,\nEfficientViT-SAM delivers 48.9x measured TensorRT speedup on A100 GPU over\nSAM-ViT-H without sacrificing performance. Our code and pre-trained models are\nreleased at https://github.com/mit-han-lab/efficientvit.",
        "publication_date": "2024-02-07T16:28:36Z",
        "upvotes": 18
    },
    "2402.05099": {
        "url": "https://arxiv.org/abs/2402.05099",
        "title": "Hydragen: High-Throughput LLM Inference with Shared Prefixes",
        "authors": [
            "Jordan Juravsky",
            "Bradley Brown",
            "Ryan Ehrlich",
            "Daniel Y. Fu",
            "Christopher R\u00e9",
            "Azalia Mirhoseini"
        ],
        "abstract": "Transformer-based large language models (LLMs) are now deployed to hundreds\nof millions of users. LLM inference is commonly performed on batches of\nsequences that share a prefix, such as few-shot examples or a chatbot system\nprompt. Decoding in this large-batch setting can be bottlenecked by the\nattention operation, which reads large key-value (KV) caches from memory and\ncomputes inefficient matrix-vector products for every sequence in the batch. In\nthis work, we introduce Hydragen, a hardware-aware exact implementation of\nattention with shared prefixes. Hydragen computes attention over the shared\nprefix and unique suffixes separately. This decomposition enables efficient\nprefix attention by batching queries together across sequences, reducing\nredundant memory reads and enabling the use of hardware-friendly matrix\nmultiplications. Our method can improve end-to-end LLM throughput by up to 32x\nagainst competitive baselines, with speedup growing with the batch size and\nshared prefix length. Hydragen also enables the use of very long shared\ncontexts: with a high batch size, increasing the prefix length from 1K to 16K\ntokens decreases Hydragen throughput by less than 15%, while the throughput of\nbaselines drops by over 90%. Hydragen generalizes beyond simple prefix-suffix\ndecomposition and can be applied to tree-based prompt sharing patterns,\nallowing us to further reduce inference time on competitive programming\nproblems by 55%.",
        "publication_date": "2024-02-07T18:53:01Z",
        "upvotes": 14
    },
    "2402.04858": {
        "url": "https://arxiv.org/abs/2402.04858",
        "title": "CodeIt: Self-Improving Language Models with Prioritized Hindsight Replay",
        "authors": [
            "Natasha Butt",
            "Blazej Manczak",
            "Auke Wiggers",
            "Corrado Rainone",
            "David Zhang",
            "Micha\u00ebl Defferrard",
            "Taco Cohen"
        ],
        "abstract": "Large language models are increasingly solving tasks that are commonly\nbelieved to require human-level reasoning ability. However, these models still\nperform very poorly on benchmarks of general intelligence such as the\nAbstraction and Reasoning Corpus (ARC). In this paper, we approach ARC as a\nprogramming-by-examples problem, and introduce a novel and scalable method for\nlanguage model self-improvement called Code Iteration (CodeIt). Our method\niterates between 1) program sampling and hindsight relabeling, and 2) learning\nfrom prioritized experience replay. By relabeling the goal of an episode (i.e.,\nthe target program output given input) to the realized output produced by the\nsampled program, our method effectively deals with the extreme sparsity of\nrewards in program synthesis. Applying CodeIt to the ARC dataset, we\ndemonstrate that prioritized hindsight replay, along with pre-training and\ndata-augmentation, leads to successful inter-task generalization. CodeIt is the\nfirst neuro-symbolic approach that scales to the full ARC evaluation dataset.\nOur method solves 15% of ARC evaluation tasks, achieving state-of-the-art\nperformance and outperforming existing neural and symbolic baselines.",
        "publication_date": "2024-02-07T13:55:27Z",
        "upvotes": 13
    },
    "2402.04347": {
        "url": "https://arxiv.org/abs/2402.04347",
        "title": "The Hedgehog & the Porcupine: Expressive Linear Attentions with Softmax\n  Mimicry",
        "authors": [
            "Michael Zhang",
            "Kush Bhatia",
            "Hermann Kumbong",
            "Christopher R\u00e9"
        ],
        "abstract": "Linear attentions have shown potential for improving Transformer efficiency,\nreducing attention's quadratic complexity to linear in sequence length. This\nholds exciting promise for (1) training linear Transformers from scratch, (2)\n\"finetuned-conversion\" of task-specific Transformers into linear versions that\nrecover task performance, and (3) \"pretrained-conversion\" of Transformers such\nas large language models into linear versions finetunable on downstream tasks.\nHowever, linear attentions often underperform standard softmax attention in\nquality. To close this performance gap, we find prior linear attentions lack\nkey properties of softmax attention tied to good performance: low-entropy (or\n\"spiky\") weights and dot-product monotonicity. We further observe surprisingly\nsimple feature maps that retain these properties and match softmax performance,\nbut are inefficient to compute in linear attention. We thus propose Hedgehog, a\nlearnable linear attention that retains the spiky and monotonic properties of\nsoftmax attention while maintaining linear complexity. Hedgehog uses simple\ntrainable MLPs to produce attention weights mimicking softmax attention.\nExperiments show Hedgehog recovers over 99% of standard Transformer quality in\ntrain-from-scratch and finetuned-conversion settings, outperforming prior\nlinear attentions up to 6 perplexity points on WikiText-103 with causal GPTs,\nand up to 8.7 GLUE score points on finetuned bidirectional BERTs. Hedgehog also\nenables pretrained-conversion. Converting a pretrained GPT-2 into a linear\nattention variant achieves state-of-the-art 16.7 perplexity on WikiText-103 for\n125M subquadratic decoder models. We finally turn a pretrained Llama-2 7B into\na viable linear attention Llama. With low-rank adaptation, Hedgehog-Llama2 7B\nachieves 28.1 higher ROUGE-1 points over the base standard attention model,\nwhere prior linear attentions lead to 16.5 point drops.",
        "publication_date": "2024-02-06T19:31:26Z",
        "upvotes": 11
    },
    "2402.04379": {
        "url": "https://arxiv.org/abs/2402.04379",
        "title": "Fine-Tuned Language Models Generate Stable Inorganic Materials as Text",
        "authors": [
            "Nate Gruver",
            "Anuroop Sriram",
            "Andrea Madotto",
            "Andrew Gordon Wilson",
            "C. Lawrence Zitnick",
            "Zachary Ulissi"
        ],
        "abstract": "We propose fine-tuning large language models for generation of stable\nmaterials. While unorthodox, fine-tuning large language models on text-encoded\natomistic data is simple to implement yet reliable, with around 90% of sampled\nstructures obeying physical constraints on atom positions and charges. Using\nenergy above hull calculations from both learned ML potentials and\ngold-standard DFT calculations, we show that our strongest model (fine-tuned\nLLaMA-2 70B) can generate materials predicted to be metastable at about twice\nthe rate (49% vs 28%) of CDVAE, a competing diffusion model. Because of text\nprompting's inherent flexibility, our models can simultaneously be used for\nunconditional generation of stable material, infilling of partial structures\nand text-conditional generation. Finally, we show that language models' ability\nto capture key symmetries of crystal structures improves with model scale,\nsuggesting that the biases of pretrained LLMs are surprisingly well-suited for\natomistic data.",
        "publication_date": "2024-02-06T20:35:28Z",
        "upvotes": 7
    },
    "2402.04825": {
        "url": "https://arxiv.org/abs/2402.04825",
        "title": "Fast Timing-Conditioned Latent Audio Diffusion",
        "authors": [
            "Zach Evans",
            "CJ Carr",
            "Josiah Taylor",
            "Scott H. Hawley",
            "Jordi Pons"
        ],
        "abstract": "Generating long-form 44.1kHz stereo audio from text prompts can be\ncomputationally demanding. Further, most previous works do not tackle that\nmusic and sound effects naturally vary in their duration. Our research focuses\non the efficient generation of long-form, variable-length stereo music and\nsounds at 44.1kHz using text prompts with a generative model. Stable Audio is\nbased on latent diffusion, with its latent defined by a fully-convolutional\nvariational autoencoder. It is conditioned on text prompts as well as timing\nembeddings, allowing for fine control over both the content and length of the\ngenerated music and sounds. Stable Audio is capable of rendering stereo signals\nof up to 95 sec at 44.1kHz in 8 sec on an A100 GPU. Despite its compute\nefficiency and fast inference, it is one of the best in two public\ntext-to-music and -audio benchmarks and, differently from state-of-the-art\nmodels, can generate music with structure and stereo sounds.",
        "publication_date": "2024-02-07T13:23:25Z",
        "upvotes": 6
    },
    "2402.04925": {
        "url": "https://arxiv.org/abs/2402.04925",
        "title": "TP-Aware Dequantization",
        "authors": [
            "Adnan Hoque",
            "Mudhakar Srivatsa",
            "Chih-Chieh Yang",
            "Raghu Ganti"
        ],
        "abstract": "In this paper, we present a novel method that reduces model inference latency\nduring distributed deployment of Large Language Models (LLMs). Our contribution\nis an optimized inference deployment scheme that address the current\nlimitations of state-of-the-art quantization kernels when used in conjunction\nwith Tensor Parallel (TP). Our method preserves data locality in GPU memory\naccess patterns and exploits a priori knowledge of TP to reduce global\ncommunication. We demonstrate an up to 1.81x speedup over existing methods for\nLlama-70B and up to 1.78x speedup for IBM WatsonX's Granite-20B MLP layer\nproblem sizes on A100 and H100 NVIDIA DGX Systems for a variety of TP settings.",
        "publication_date": "2024-01-15T08:01:40Z",
        "upvotes": 3
    },
    "2402.04744": {
        "url": "https://arxiv.org/abs/2402.04744",
        "title": "Progressive Gradient Flow for Robust N:M Sparsity Training in\n  Transformers",
        "authors": [
            "Abhimanyu Rajeshkumar Bambhaniya",
            "Amir Yazdanbakhsh",
            "Suvinay Subramanian",
            "Sheng-Chun Kao",
            "Shivani Agrawal",
            "Utku Evci",
            "Tushar Krishna"
        ],
        "abstract": "N:M Structured sparsity has garnered significant interest as a result of\nrelatively modest overhead and improved efficiency. Additionally, this form of\nsparsity holds considerable appeal for reducing the memory footprint owing to\ntheir modest representation overhead. There have been efforts to develop\ntraining recipes for N:M structured sparsity, they primarily focus on\nlow-sparsity regions ($\\sim$50\\%). Nonetheless, performance of models trained\nusing these approaches tends to decline when confronted with high-sparsity\nregions ($>$80\\%). In this work, we study the effectiveness of existing sparse\ntraining recipes at \\textit{high-sparsity regions} and argue that these methods\nfail to sustain the model quality on par with low-sparsity regions. We\ndemonstrate that the significant factor contributing to this disparity is the\npresence of elevated levels of induced noise in the gradient magnitudes. To\nmitigate this undesirable effect, we employ decay mechanisms to progressively\nrestrict the flow of gradients towards pruned elements. Our approach improves\nthe model quality by up to 2$\\%$ and 5$\\%$ in vision and language models at\nhigh sparsity regime, respectively. We also evaluate the trade-off between\nmodel accuracy and training compute cost in terms of FLOPs. At iso-training\nFLOPs, our method yields better performance compared to conventional sparse\ntraining recipes, exhibiting an accuracy improvement of up to 2$\\%$. The source\ncode is available at\nhttps://github.com/abhibambhaniya/progressive_gradient_flow_nm_sparsity.",
        "publication_date": "2024-02-07T10:55:59Z",
        "upvotes": 1
    },
    "2402.05120": {
        "url": "https://arxiv.org/abs/2402.05120",
        "title": "More Agents Is All You Need",
        "authors": [
            "Junyou Li",
            "Qin Zhang",
            "Yangbin Yu",
            "Qiang Fu",
            "Deheng Ye"
        ],
        "abstract": "We find that, simply via a sampling-and-voting method, the performance of\nlarge language models (LLMs) scales with the number of agents instantiated.\nAlso, this method is orthogonal to existing complicated methods to further\nenhance LLMs, while the degree of enhancement is correlated to the task\ndifficulty. We conduct comprehensive experiments on a wide range of LLM\nbenchmarks to verify the presence of our finding, and to study the properties\nthat can facilitate its occurrence. Our code is publicly available at:\n\\url{https://anonymous.4open.science/r/more_agent_is_all_you_need}.",
        "publication_date": "2024-02-03T05:55:24Z",
        "upvotes": 42
    },
    "2402.05930": {
        "url": "https://arxiv.org/abs/2402.05930",
        "title": "WebLINX: Real-World Website Navigation with Multi-Turn Dialogue",
        "authors": [
            "Xing Han L\u00f9",
            "Zden\u011bk Kasner",
            "Siva Reddy"
        ],
        "abstract": "We propose the problem of conversational web navigation, where a digital\nagent controls a web browser and follows user instructions to solve real-world\ntasks in a multi-turn dialogue fashion. To support this problem, we introduce\nWEBLINX - a large-scale benchmark of 100K interactions across 2300 expert\ndemonstrations of conversational web navigation. Our benchmark covers a broad\nrange of patterns on over 150 real-world websites and can be used to train and\nevaluate agents in diverse scenarios. Due to the magnitude of information\npresent, Large Language Models (LLMs) cannot process entire web pages in\nreal-time. To solve this bottleneck, we design a retrieval-inspired model that\nefficiently prunes HTML pages by ranking relevant elements. We use the selected\nelements, along with screenshots and action history, to assess a variety of\nmodels for their ability to replicate human behavior when navigating the web.\nOur experiments span from small text-only to proprietary multimodal LLMs. We\nfind that smaller finetuned decoders surpass the best zero-shot LLMs (including\nGPT-4V), but also larger finetuned multimodal models which were explicitly\npretrained on screenshots. However, all finetuned models struggle to generalize\nto unseen websites. Our findings highlight the need for large multimodal models\nthat can generalize to novel settings. Our code, data and models are available\nfor research: https://mcgill-nlp.github.io/weblinx",
        "publication_date": "2024-02-08T18:58:02Z",
        "upvotes": 34
    },
    "2402.05929": {
        "url": "https://arxiv.org/abs/2402.05929",
        "title": "An Interactive Agent Foundation Model",
        "authors": [
            "Zane Durante",
            "Bidipta Sarkar",
            "Ran Gong",
            "Rohan Taori",
            "Yusuke Noda",
            "Paul Tang",
            "Ehsan Adeli",
            "Shrinidhi Kowshika Lakshmikanth",
            "Kevin Schulman",
            "Arnold Milstein",
            "Demetri Terzopoulos",
            "Ade Famoti",
            "Noboru Kuno",
            "Ashley Llorens",
            "Hoi Vo",
            "Katsu Ikeuchi",
            "Li Fei-Fei",
            "Jianfeng Gao",
            "Naoki Wake",
            "Qiuyuan Huang"
        ],
        "abstract": "The development of artificial intelligence systems is transitioning from\ncreating static, task-specific models to dynamic, agent-based systems capable\nof performing well in a wide range of applications. We propose an Interactive\nAgent Foundation Model that uses a novel multi-task agent training paradigm for\ntraining AI agents across a wide range of domains, datasets, and tasks. Our\ntraining paradigm unifies diverse pre-training strategies, including visual\nmasked auto-encoders, language modeling, and next-action prediction, enabling a\nversatile and adaptable AI framework. We demonstrate the performance of our\nframework across three separate domains -- Robotics, Gaming AI, and Healthcare.\nOur model demonstrates its ability to generate meaningful and contextually\nrelevant outputs in each area. The strength of our approach lies in its\ngenerality, leveraging a variety of data sources such as robotics sequences,\ngameplay data, large-scale video datasets, and textual information for\neffective multimodal and multi-task learning. Our approach provides a promising\navenue for developing generalist, action-taking, multimodal systems.",
        "publication_date": "2024-02-08T18:58:02Z",
        "upvotes": 24
    },
    "2402.05140": {
        "url": "https://arxiv.org/abs/2402.05140",
        "title": "Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains",
        "authors": [
            "Junhong Shen",
            "Neil Tenenholtz",
            "James Brian Hall",
            "David Alvarez-Melis",
            "Nicolo Fusi"
        ],
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable proficiency in\nunderstanding and generating natural language. However, their capabilities wane\nin highly specialized domains underrepresented in the pretraining corpus, such\nas physical and biomedical sciences. This work explores how to repurpose\ngeneral LLMs into effective task solvers for specialized domains. We introduce\na novel, model-agnostic framework for learning custom input tags, which are\nparameterized as continuous vectors appended to the LLM's embedding layer, to\ncondition the LLM. We design two types of input tags: domain tags are used to\ndelimit specialized representations (e.g., chemical formulas) and provide\ndomain-relevant context; function tags are used to represent specific functions\n(e.g., predicting molecular properties) and compress function-solving\ninstructions. We develop a three-stage protocol to learn these tags using\nauxiliary data and domain knowledge. By explicitly disentangling task domains\nfrom task functions, our method enables zero-shot generalization to unseen\nproblems through diverse combinations of the input tags. It also boosts LLM's\nperformance in various specialized domains, such as predicting protein or\nchemical properties and modeling drug-target interactions, outperforming expert\nmodels tailored to these tasks.",
        "publication_date": "2024-02-06T20:11:54Z",
        "upvotes": 18
    },
    "2402.05672": {
        "url": "https://arxiv.org/abs/2402.05672",
        "title": "Multilingual E5 Text Embeddings: A Technical Report",
        "authors": [
            "Liang Wang",
            "Nan Yang",
            "Xiaolong Huang",
            "Linjun Yang",
            "Rangan Majumder",
            "Furu Wei"
        ],
        "abstract": "This technical report presents the training methodology and evaluation\nresults of the open-source multilingual E5 text embedding models, released in\nmid-2023. Three embedding models of different sizes (small / base / large) are\nprovided, offering a balance between the inference efficiency and embedding\nquality. The training procedure adheres to the English E5 model recipe,\ninvolving contrastive pre-training on 1 billion multilingual text pairs,\nfollowed by fine-tuning on a combination of labeled datasets. Additionally, we\nintroduce a new instruction-tuned embedding model, whose performance is on par\nwith state-of-the-art, English-only models of similar sizes. Information\nregarding the model release can be found at\nhttps://github.com/microsoft/unilm/tree/master/e5 .",
        "publication_date": "2024-02-08T13:47:50Z",
        "upvotes": 15
    },
    "2402.05195": {
        "url": "https://arxiv.org/abs/2402.05195",
        "title": "$\u03bb$-ECLIPSE: Multi-Concept Personalized Text-to-Image Diffusion\n  Models by Leveraging CLIP Latent Space",
        "authors": [
            "Maitreya Patel",
            "Sangmin Jung",
            "Chitta Baral",
            "Yezhou Yang"
        ],
        "abstract": "Despite the recent advances in personalized text-to-image (P-T2I) generative\nmodels, subject-driven T2I remains challenging. The primary bottlenecks include\n1) Intensive training resource requirements, 2) Hyper-parameter sensitivity\nleading to inconsistent outputs, and 3) Balancing the intricacies of novel\nvisual concept and composition alignment. We start by re-iterating the core\nphilosophy of T2I diffusion models to address the above limitations.\nPredominantly, contemporary subject-driven T2I approaches hinge on Latent\nDiffusion Models (LDMs), which facilitate T2I mapping through cross-attention\nlayers. While LDMs offer distinct advantages, P-T2I methods' reliance on the\nlatent space of these diffusion models significantly escalates resource\ndemands, leading to inconsistent results and necessitating numerous iterations\nfor a single desired image. Recently, ECLIPSE has demonstrated a more\nresource-efficient pathway for training UnCLIP-based T2I models, circumventing\nthe need for diffusion text-to-image priors. Building on this, we introduce\n$\\lambda$-ECLIPSE. Our method illustrates that effective P-T2I does not\nnecessarily depend on the latent space of diffusion models. $\\lambda$-ECLIPSE\nachieves single, multi-subject, and edge-guided T2I personalization with just\n34M parameters and is trained on a mere 74 GPU hours using 1.6M image-text\ninterleaved data. Through extensive experiments, we also establish that\n$\\lambda$-ECLIPSE surpasses existing baselines in composition alignment while\npreserving concept alignment performance, even with significantly lower\nresource utilization.",
        "publication_date": "2024-02-07T19:07:10Z",
        "upvotes": 15
    },
    "2402.05403": {
        "url": "https://arxiv.org/abs/2402.05403",
        "title": "In-Context Principle Learning from Mistakes",
        "authors": [
            "Tianjun Zhang",
            "Aman Madaan",
            "Luyu Gao",
            "Steven Zheng",
            "Swaroop Mishra",
            "Yiming Yang",
            "Niket Tandon",
            "Uri Alon"
        ],
        "abstract": "In-context learning (ICL, also known as few-shot prompting) has been the\nstandard method of adapting LLMs to downstream tasks, by learning from a few\ninput-output examples. Nonetheless, all ICL-based approaches only learn from\ncorrect input-output pairs. In this paper, we revisit this paradigm, by\nlearning more from the few given input-output examples. We introduce Learning\nPrinciples (LEAP): First, we intentionally induce the model to make mistakes on\nthese few examples; then we reflect on these mistakes, and learn explicit\ntask-specific \"principles\" from them, which help solve similar problems and\navoid common mistakes; finally, we prompt the model to answer unseen test\nquestions using the original few-shot examples and these learned general\nprinciples. We evaluate LEAP on a wide range of benchmarks, including multi-hop\nquestion answering (Hotpot QA), textual QA (DROP), Big-Bench Hard reasoning,\nand math problems (GSM8K and MATH); in all these benchmarks, LEAP improves the\nstrongest available LLMs such as GPT-3.5-turbo, GPT-4, GPT-4 turbo and\nClaude-2.1. For example, LEAP improves over the standard few-shot prompting\nusing GPT-4 by 7.5% in DROP, and by 3.3% in HotpotQA. Importantly, LEAP does\nnot require any more input or examples than the standard few-shot prompting\nsettings.",
        "publication_date": "2024-02-08T04:42:29Z",
        "upvotes": 12
    },
    "2402.05935": {
        "url": "https://arxiv.org/abs/2402.05935",
        "title": "SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large\n  Language Models",
        "authors": [
            "Peng Gao",
            "Renrui Zhang",
            "Chris Liu",
            "Longtian Qiu",
            "Siyuan Huang",
            "Weifeng Lin",
            "Shitian Zhao",
            "Shijie Geng",
            "Ziyi Lin",
            "Peng Jin",
            "Kaipeng Zhang",
            "Wenqi Shao",
            "Chao Xu",
            "Conghui He",
            "Junjun He",
            "Hao Shao",
            "Pan Lu",
            "Hongsheng Li",
            "Yu Qiao"
        ],
        "abstract": "We propose SPHINX-X, an extensive Multimodality Large Language Model (MLLM)\nseries developed upon SPHINX. To improve the architecture and training\nefficiency, we modify the SPHINX framework by removing redundant visual\nencoders, bypassing fully-padded sub-images with skip tokens, and simplifying\nmulti-stage training into a one-stage all-in-one paradigm. To fully unleash the\npotential of MLLMs, we assemble a comprehensive multi-domain and multimodal\ndataset covering publicly available resources in language, vision, and\nvision-language tasks. We further enrich this collection with our curated OCR\nintensive and Set-of-Mark datasets, extending the diversity and generality. By\ntraining over different base LLMs including TinyLlama1.1B, InternLM2-7B,\nLLaMA2-13B, and Mixtral8x7B, we obtain a spectrum of MLLMs that vary in\nparameter size and multilingual capabilities. Comprehensive benchmarking\nreveals a strong correlation between the multi-modal performance with the data\nand parameter scales. Code and models are released at\nhttps://github.com/Alpha-VLLM/LLaMA2-Accessory",
        "publication_date": "2024-02-08T18:59:48Z",
        "upvotes": 11
    },
    "2402.05937": {
        "url": "https://arxiv.org/abs/2402.05937",
        "title": "InstaGen: Enhancing Object Detection by Training on Synthetic Dataset",
        "authors": [
            "Chengjian Feng",
            "Yujie Zhong",
            "Zequn Jie",
            "Weidi Xie",
            "Lin Ma"
        ],
        "abstract": "In this paper, we introduce a novel paradigm to enhance the ability of object\ndetector, e.g., expanding categories or improving detection performance, by\ntraining on synthetic dataset generated from diffusion models. Specifically, we\nintegrate an instance-level grounding head into a pre-trained, generative\ndiffusion model, to augment it with the ability of localising arbitrary\ninstances in the generated images. The grounding head is trained to align the\ntext embedding of category names with the regional visual feature of the\ndiffusion model, using supervision from an off-the-shelf object detector, and a\nnovel self-training scheme on (novel) categories not covered by the detector.\nThis enhanced version of diffusion model, termed as InstaGen, can serve as a\ndata synthesizer for object detection. We conduct thorough experiments to show\nthat, object detector can be enhanced while training on the synthetic dataset\nfrom InstaGen, demonstrating superior performance over existing\nstate-of-the-art methods in open-vocabulary (+4.5 AP) and data-sparse (+1.2 to\n5.2 AP) scenarios.",
        "publication_date": "2024-02-08T18:59:53Z",
        "upvotes": 8
    },
    "2402.05861": {
        "url": "https://arxiv.org/abs/2402.05861",
        "title": "Memory Consolidation Enables Long-Context Video Understanding",
        "authors": [
            "Ivana Bala\u017eevi\u0107",
            "Yuge Shi",
            "Pinelopi Papalampidi",
            "Rahma Chaabouni",
            "Skanda Koppula",
            "Olivier J. H\u00e9naff"
        ],
        "abstract": "Most transformer-based video encoders are limited to short temporal contexts\ndue to their quadratic complexity. While various attempts have been made to\nextend this context, this has often come at the cost of both conceptual and\ncomputational complexity. We propose to instead re-purpose existing pre-trained\nvideo transformers by simply fine-tuning them to attend to memories derived\nnon-parametrically from past activations. By leveraging redundancy reduction,\nour memory-consolidated vision transformer (MC-ViT) effortlessly extends its\ncontext far into the past and exhibits excellent scaling behavior when learning\nfrom longer videos. In doing so, MC-ViT sets a new state-of-the-art in\nlong-context video understanding on EgoSchema, Perception Test, and Diving48,\noutperforming methods that benefit from orders of magnitude more parameters.",
        "publication_date": "2024-02-08T17:50:22Z",
        "upvotes": 7
    },
    "2402.05755": {
        "url": "https://arxiv.org/abs/2402.05755",
        "title": "SpiRit-LM: Interleaved Spoken and Written Language Model",
        "authors": [
            "Tu Anh Nguyen",
            "Benjamin Muller",
            "Bokai Yu",
            "Marta R. Costa-jussa",
            "Maha Elbayad",
            "Sravya Popuri",
            "Paul-Ambroise Duquenne",
            "Robin Algayres",
            "Ruslan Mavlyutov",
            "Itai Gat",
            "Gabriel Synnaeve",
            "Juan Pino",
            "Benoit Sagot",
            "Emmanuel Dupoux"
        ],
        "abstract": "We introduce SPIRIT-LM, a foundation multimodal language model that freely\nmixes text and speech. Our model is based on a pretrained text language model\nthat we extend to the speech modality by continuously training it on text and\nspeech units. Speech and text sequences are concatenated as a single set of\ntokens, and trained with a word-level interleaving method using a small\nautomatically-curated speech-text parallel corpus. SPIRIT-LM comes in two\nversions: a BASE version that uses speech semantic units and an EXPRESSIVE\nversion that models expressivity using pitch and style units in addition to the\nsemantic units. For both versions, the text is encoded with subword BPE tokens.\nThe resulting model displays both the semantic abilities of text models and the\nexpressive abilities of speech models. Additionally, we demonstrate that\nSPIRIT-LM is able to learn new tasks in a few-shot fashion across modalities\n(i.e. ASR, TTS, Speech Classification).",
        "publication_date": "2024-02-08T15:39:32Z",
        "upvotes": 6
    },
    "2402.05468": {
        "url": "https://arxiv.org/abs/2402.05468",
        "title": "Implicit Diffusion: Efficient Optimization through Stochastic Sampling",
        "authors": [
            "Pierre Marion",
            "Anna Korba",
            "Peter Bartlett",
            "Mathieu Blondel",
            "Valentin De Bortoli",
            "Arnaud Doucet",
            "Felipe Llinares-L\u00f3pez",
            "Courtney Paquette",
            "Quentin Berthet"
        ],
        "abstract": "We present a new algorithm to optimize distributions defined implicitly by\nparameterized stochastic diffusions. Doing so allows us to modify the outcome\ndistribution of sampling processes by optimizing over their parameters. We\nintroduce a general framework for first-order optimization of these processes,\nthat performs jointly, in a single loop, optimization and sampling steps. This\napproach is inspired by recent advances in bilevel optimization and automatic\nimplicit differentiation, leveraging the point of view of sampling as\noptimization over the space of probability distributions. We provide\ntheoretical guarantees on the performance of our method, as well as\nexperimental results demonstrating its effectiveness in real-world settings.",
        "publication_date": "2024-02-08T08:00:11Z",
        "upvotes": 5
    },
    "2402.05546": {
        "url": "https://arxiv.org/abs/2402.05546",
        "title": "Offline Actor-Critic Reinforcement Learning Scales to Large Models",
        "authors": [
            "Jost Tobias Springenberg",
            "Abbas Abdolmaleki",
            "Jingwei Zhang",
            "Oliver Groth",
            "Michael Bloesch",
            "Thomas Lampe",
            "Philemon Brakel",
            "Sarah Bechtle",
            "Steven Kapturowski",
            "Roland Hafner",
            "Nicolas Heess",
            "Martin Riedmiller"
        ],
        "abstract": "We show that offline actor-critic reinforcement learning can scale to large\nmodels - such as transformers - and follows similar scaling laws as supervised\nlearning. We find that offline actor-critic algorithms can outperform strong,\nsupervised, behavioral cloning baselines for multi-task training on a large\ndataset containing both sub-optimal and expert behavior on 132 continuous\ncontrol tasks. We introduce a Perceiver-based actor-critic model and elucidate\nthe key model features needed to make offline RL work with self- and\ncross-attention modules. Overall, we find that: i) simple offline actor critic\nalgorithms are a natural choice for gradually moving away from the currently\npredominant paradigm of behavioral cloning, and ii) via offline RL it is\npossible to learn multi-task policies that master many domains simultaneously,\nincluding real robotics tasks, from sub-optimal demonstrations or\nself-generated data.",
        "publication_date": "2024-02-08T10:29:46Z",
        "upvotes": 4
    },
    "2402.05472": {
        "url": "https://arxiv.org/abs/2402.05472",
        "title": "Question Aware Vision Transformer for Multimodal Reasoning",
        "authors": [
            "Roy Ganz",
            "Yair Kittenplon",
            "Aviad Aberdam",
            "Elad Ben Avraham",
            "Oren Nuriel",
            "Shai Mazor",
            "Ron Litman"
        ],
        "abstract": "Vision-Language (VL) models have gained significant research focus, enabling\nremarkable advances in multimodal reasoning. These architectures typically\ncomprise a vision encoder, a Large Language Model (LLM), and a projection\nmodule that aligns visual features with the LLM's representation space. Despite\ntheir success, a critical limitation persists: the vision encoding process\nremains decoupled from user queries, often in the form of image-related\nquestions. Consequently, the resulting visual features may not be optimally\nattuned to the query-specific elements of the image. To address this, we\nintroduce QA-ViT, a Question Aware Vision Transformer approach for multimodal\nreasoning, which embeds question awareness directly within the vision encoder.\nThis integration results in dynamic visual features focusing on relevant image\naspects to the posed question. QA-ViT is model-agnostic and can be incorporated\nefficiently into any VL architecture. Extensive experiments demonstrate the\neffectiveness of applying our method to various multimodal architectures,\nleading to consistent improvement across diverse tasks and showcasing its\npotential for enhancing visual and scene-text understanding.",
        "publication_date": "2024-02-08T08:03:39Z",
        "upvotes": 4
    },
    "2402.05932": {
        "url": "https://arxiv.org/abs/2402.05932",
        "title": "Driving Everywhere with Large Language Model Policy Adaptation",
        "authors": [
            "Boyi Li",
            "Yue Wang",
            "Jiageng Mao",
            "Boris Ivanovic",
            "Sushant Veer",
            "Karen Leung",
            "Marco Pavone"
        ],
        "abstract": "Adapting driving behavior to new environments, customs, and laws is a\nlong-standing problem in autonomous driving, precluding the widespread\ndeployment of autonomous vehicles (AVs). In this paper, we present LLaDA, a\nsimple yet powerful tool that enables human drivers and autonomous vehicles\nalike to drive everywhere by adapting their tasks and motion plans to traffic\nrules in new locations. LLaDA achieves this by leveraging the impressive\nzero-shot generalizability of large language models (LLMs) in interpreting the\ntraffic rules in the local driver handbook. Through an extensive user study, we\nshow that LLaDA's instructions are useful in disambiguating in-the-wild\nunexpected situations. We also demonstrate LLaDA's ability to adapt AV motion\nplanning policies in real-world datasets; LLaDA outperforms baseline planning\napproaches on all our metrics. Please check our website for more details:\nhttps://boyiliee.github.io/llada.",
        "publication_date": "2024-02-08T18:59:03Z",
        "upvotes": 3
    },
    "2402.06619": {
        "url": "https://arxiv.org/abs/2402.06619",
        "title": "Aya Dataset: An Open-Access Collection for Multilingual Instruction\n  Tuning",
        "authors": [
            "Shivalika Singh",
            "Freddie Vargus",
            "Daniel Dsouza",
            "B\u00f6rje F. Karlsson",
            "Abinaya Mahendiran",
            "Wei-Yin Ko",
            "Herumb Shandilya",
            "Jay Patel",
            "Deividas Mataciunas",
            "Laura OMahony",
            "Mike Zhang",
            "Ramith Hettiarachchi",
            "Joseph Wilson",
            "Marina Machado",
            "Luisa Souza Moura",
            "Dominik Krzemi\u0144ski",
            "Hakimeh Fadaei",
            "Irem Erg\u00fcn",
            "Ifeoma Okoh",
            "Aisha Alaagib",
            "Oshan Mudannayake",
            "Zaid Alyafeai",
            "Vu Minh Chien",
            "Sebastian Ruder",
            "Surya Guthikonda",
            "Emad A. Alghamdi",
            "Sebastian Gehrmann",
            "Niklas Muennighoff",
            "Max Bartolo",
            "Julia Kreutzer",
            "Ahmet \u00dcst\u00fcn",
            "Marzieh Fadaee",
            "Sara Hooker"
        ],
        "abstract": "Datasets are foundational to many breakthroughs in modern artificial\nintelligence. Many recent achievements in the space of natural language\nprocessing (NLP) can be attributed to the finetuning of pre-trained models on a\ndiverse set of tasks that enables a large language model (LLM) to respond to\ninstructions. Instruction fine-tuning (IFT) requires specifically constructed\nand annotated datasets. However, existing datasets are almost all in the\nEnglish language. In this work, our primary goal is to bridge the language gap\nby building a human-curated instruction-following dataset spanning 65\nlanguages. We worked with fluent speakers of languages from around the world to\ncollect natural instances of instructions and completions. Furthermore, we\ncreate the most extensive multilingual collection to date, comprising 513\nmillion instances through templating and translating existing datasets across\n114 languages. In total, we contribute four key resources: we develop and\nopen-source the Aya Annotation Platform, the Aya Dataset, the Aya Collection,\nand the Aya Evaluation Suite. The Aya initiative also serves as a valuable case\nstudy in participatory research, involving collaborators from 119 countries. We\nsee this as a valuable framework for future research collaborations that aim to\nbridge gaps in resources.",
        "publication_date": "2024-02-09T18:51:49Z",
        "upvotes": 47
    },
    "2402.06332": {
        "url": "https://arxiv.org/abs/2402.06332",
        "title": "InternLM-Math: Open Math Large Language Models Toward Verifiable\n  Reasoning",
        "authors": [
            "Huaiyuan Ying",
            "Shuo Zhang",
            "Linyang Li",
            "Zhejian Zhou",
            "Yunfan Shao",
            "Zhaoye Fei",
            "Yichuan Ma",
            "Jiawei Hong",
            "Kuikun Liu",
            "Ziyi Wang",
            "Yudong Wang",
            "Zijian Wu",
            "Shuaibin Li",
            "Fengzhe Zhou",
            "Hongwei Liu",
            "Songyang Zhang",
            "Wenwei Zhang",
            "Hang Yan",
            "Xipeng Qiu",
            "Jiayu Wang",
            "Kai Chen",
            "Dahua Lin"
        ],
        "abstract": "The math abilities of large language models can represent their abstract\nreasoning ability. In this paper, we introduce and open-source our math\nreasoning LLMs InternLM-Math which is continue pre-trained from InternLM2. We\nunify chain-of-thought reasoning, reward modeling, formal reasoning, data\naugmentation, and code interpreter in a unified seq2seq format and supervise\nour model to be a versatile math reasoner, verifier, prover, and augmenter.\nThese abilities can be used to develop the next math LLMs or self-iteration.\nInternLM-Math obtains open-sourced state-of-the-art performance under the\nsetting of in-context learning, supervised fine-tuning, and code-assisted\nreasoning in various informal and formal benchmarks including GSM8K, MATH,\nHungary math exam, MathBench-ZH, and MiniF2F. Our pre-trained model achieves\n30.3 on the MiniF2F test set without fine-tuning. We further explore how to use\nLEAN to solve math problems and study its performance under the setting of\nmulti-task learning which shows the possibility of using LEAN as a unified\nplatform for solving and proving in math. Our models, codes, and data are\nreleased at \\url{https://github.com/InternLM/InternLM-Math}.",
        "publication_date": "2024-02-09T11:22:08Z",
        "upvotes": 17
    },
    "2402.06149": {
        "url": "https://arxiv.org/abs/2402.06149",
        "title": "HeadStudio: Text to Animatable Head Avatars with 3D Gaussian Splatting",
        "authors": [
            "Zhenglin Zhou",
            "Fan Ma",
            "Hehe Fan",
            "Yi Yang"
        ],
        "abstract": "Creating digital avatars from textual prompts has long been a desirable yet\nchallenging task. Despite the promising outcomes obtained through 2D diffusion\npriors in recent works, current methods face challenges in achieving\nhigh-quality and animated avatars effectively. In this paper, we present\n$\\textbf{HeadStudio}$, a novel framework that utilizes 3D Gaussian splatting to\ngenerate realistic and animated avatars from text prompts. Our method drives 3D\nGaussians semantically to create a flexible and achievable appearance through\nthe intermediate FLAME representation. Specifically, we incorporate the FLAME\ninto both 3D representation and score distillation: 1) FLAME-based 3D Gaussian\nsplatting, driving 3D Gaussian points by rigging each point to a FLAME mesh. 2)\nFLAME-based score distillation sampling, utilizing FLAME-based fine-grained\ncontrol signal to guide score distillation from the text prompt. Extensive\nexperiments demonstrate the efficacy of HeadStudio in generating animatable\navatars from textual prompts, exhibiting visually appealing appearances. The\navatars are capable of rendering high-quality real-time ($\\geq 40$ fps) novel\nviews at a resolution of 1024. They can be smoothly controlled by real-world\nspeech and video. We hope that HeadStudio can advance digital avatar creation\nand that the present method can widely be applied across various domains.",
        "publication_date": "2024-02-09T02:58:37Z",
        "upvotes": 15
    },
    "2402.06071": {
        "url": "https://arxiv.org/abs/2402.06071",
        "title": "Keyframer: Empowering Animation Design using Large Language Models",
        "authors": [
            "Tiffany Tseng",
            "Ruijia Cheng",
            "Jeffrey Nichols"
        ],
        "abstract": "Large language models (LLMs) have the potential to impact a wide range of\ncreative domains, but the application of LLMs to animation is underexplored and\npresents novel challenges such as how users might effectively describe motion\nin natural language. In this paper, we present Keyframer, a design tool for\nanimating static images (SVGs) with natural language. Informed by interviews\nwith professional animation designers and engineers, Keyframer supports\nexploration and refinement of animations through the combination of prompting\nand direct editing of generated output. The system also enables users to\nrequest design variants, supporting comparison and ideation. Through a user\nstudy with 13 participants, we contribute a characterization of user prompting\nstrategies, including a taxonomy of semantic prompt types for describing motion\nand a 'decomposed' prompting style where users continually adapt their goals in\nresponse to generated output.We share how direct editing along with prompting\nenables iteration beyond one-shot prompting interfaces common in generative\ntools today. Through this work, we propose how LLMs might empower a range of\naudiences to engage with animation creation.",
        "publication_date": "2024-02-08T21:43:30Z",
        "upvotes": 13
    },
    "2402.06118": {
        "url": "https://arxiv.org/abs/2402.06118",
        "title": "ViGoR: Improving Visual Grounding of Large Vision Language Models with\n  Fine-Grained Reward Modeling",
        "authors": [
            "Siming Yan",
            "Min Bai",
            "Weifeng Chen",
            "Xiong Zhou",
            "Qixing Huang",
            "Li Erran Li"
        ],
        "abstract": "By combining natural language understanding and the generation capabilities\nand breadth of knowledge of large language models with image perception, recent\nlarge vision language models (LVLMs) have shown unprecedented reasoning\ncapabilities in the real world. However, the generated text often suffers from\ninaccurate grounding in the visual input, resulting in errors such as\nhallucinating nonexistent scene elements, missing significant parts of the\nscene, and inferring incorrect attributes and relationships between objects. To\naddress these issues, we introduce a novel framework, ViGoR (Visual Grounding\nThrough Fine-Grained Reward Modeling) that utilizes fine-grained reward\nmodeling to significantly enhance the visual grounding of LVLMs over\npre-trained baselines. This improvement is efficiently achieved using much\ncheaper human evaluations instead of full supervisions, as well as automated\nmethods. We show the effectiveness of our approach through numerous metrics on\nseveral benchmarks. Additionally, we construct a comprehensive and challenging\ndataset specifically designed to validate the visual grounding capabilities of\nLVLMs. Finally, we plan to release our human annotation comprising\napproximately 16,000 images and generated text pairs with fine-grained\nevaluations to contribute to related research in the community.",
        "publication_date": "2024-02-09T01:00:14Z",
        "upvotes": 12
    },
    "2402.06178": {
        "url": "https://arxiv.org/abs/2402.06178",
        "title": "MusicMagus: Zero-Shot Text-to-Music Editing via Diffusion Models",
        "authors": [
            "Yixiao Zhang",
            "Yukara Ikemiya",
            "Gus Xia",
            "Naoki Murata",
            "Marco Mart\u00ednez",
            "Wei-Hsiang Liao",
            "Yuki Mitsufuji",
            "Simon Dixon"
        ],
        "abstract": "Recent advances in text-to-music generation models have opened new avenues in\nmusical creativity. However, music generation usually involves iterative\nrefinements, and how to edit the generated music remains a significant\nchallenge. This paper introduces a novel approach to the editing of music\ngenerated by such models, enabling the modification of specific attributes,\nsuch as genre, mood and instrument, while maintaining other aspects unchanged.\nOur method transforms text editing to \\textit{latent space manipulation} while\nadding an extra constraint to enforce consistency. It seamlessly integrates\nwith existing pretrained text-to-music diffusion models without requiring\nadditional training. Experimental results demonstrate superior performance over\nboth zero-shot and certain supervised baselines in style and timbre transfer\nevaluations. Additionally, we showcase the practical applicability of our\napproach in real-world music editing scenarios.",
        "publication_date": "2024-02-09T04:34:08Z",
        "upvotes": 12
    },
    "2402.06082": {
        "url": "https://arxiv.org/abs/2402.06082",
        "title": "SubGen: Token Generation in Sublinear Time and Memory",
        "authors": [
            "Amir Zandieh",
            "Insu Han",
            "Vahab Mirrokni",
            "Amin Karbasi"
        ],
        "abstract": "Despite the significant success of large language models (LLMs), their\nextensive memory requirements pose challenges for deploying them in\nlong-context token generation. The substantial memory footprint of LLM decoders\narises from the necessity to store all previous tokens in the attention module,\na requirement imposed by key-value (KV) caching. In this work, our focus is on\ndeveloping an efficient compression technique for the KV cache. Empirical\nevidence indicates a significant clustering tendency within key embeddings in\nthe attention module. Building on this key insight, we have devised a novel\ncaching method with sublinear complexity, employing online clustering on key\ntokens and online $\\ell_2$ sampling on values. The result is a provably\naccurate and efficient attention decoding algorithm, termed SubGen. Not only\ndoes this algorithm ensure a sublinear memory footprint and sublinear time\ncomplexity, but we also establish a tight error bound for our approach.\nEmpirical evaluations on long-context question-answering tasks demonstrate that\nSubGen significantly outperforms existing and state-of-the-art KV cache\ncompression methods in terms of performance and efficiency.",
        "publication_date": "2024-02-08T22:17:40Z",
        "upvotes": 10
    },
    "2402.06155": {
        "url": "https://arxiv.org/abs/2402.06155",
        "title": "Model Editing with Canonical Examples",
        "authors": [
            "John Hewitt",
            "Sarah Chen",
            "Lanruo Lora Xie",
            "Edward Adams",
            "Percy Liang",
            "Christopher D. Manning"
        ],
        "abstract": "We introduce model editing with canonical examples, a setting in which (1) a\nsingle learning example is provided per desired behavior, (2) evaluation is\nperformed exclusively out-of-distribution, and (3) deviation from an initial\nmodel is strictly limited. A canonical example is a simple instance of good\nbehavior, e.g., The capital of Mauritius is Port Louis) or bad behavior, e.g.,\nAn aspect of researchers is coldhearted). The evaluation set contains more\ncomplex examples of each behavior (like a paragraph in which the capital of\nMauritius is called for.) We create three datasets and modify three more for\nmodel editing with canonical examples, covering knowledge-intensive\nimprovements, social bias mitigation, and syntactic edge cases. In our\nexperiments on Pythia language models, we find that LoRA outperforms full\nfinetuning and MEMIT. We then turn to the Backpack language model architecture\nbecause it is intended to enable targeted improvement. The Backpack defines a\nlarge bank of sense vectors--a decomposition of the different uses of each\nword--which are weighted and summed to form the output logits of the model. We\npropose sense finetuning, which selects and finetunes a few ($\\approx$ 10)\nsense vectors for each canonical example, and find that it outperforms other\nfinetuning methods, e.g., 4.8% improvement vs 0.3%. Finally, we improve\nGPT-J-6B by an inference-time ensemble with just the changes from sense\nfinetuning of a 35x smaller Backpack, in one setting outperforming editing\nGPT-J itself (4.1% vs 1.0%).",
        "publication_date": "2024-02-09T03:08:12Z",
        "upvotes": 9
    },
    "2402.06088": {
        "url": "https://arxiv.org/abs/2402.06088",
        "title": "Animated Stickers: Bringing Stickers to Life with Video Diffusion",
        "authors": [
            "David Yan",
            "Winnie Zhang",
            "Luxin Zhang",
            "Anmol Kalia",
            "Dingkang Wang",
            "Ankit Ramchandani",
            "Miao Liu",
            "Albert Pumarola",
            "Edgar Schoenfeld",
            "Elliot Blanchard",
            "Krishna Narni",
            "Yaqiao Luo",
            "Lawrence Chen",
            "Guan Pang",
            "Ali Thabet",
            "Peter Vajda",
            "Amy Bearman",
            "Licheng Yu"
        ],
        "abstract": "We introduce animated stickers, a video diffusion model which generates an\nanimation conditioned on a text prompt and static sticker image. Our model is\nbuilt on top of the state-of-the-art Emu text-to-image model, with the addition\nof temporal layers to model motion. Due to the domain gap, i.e. differences in\nvisual and motion style, a model which performed well on generating natural\nvideos can no longer generate vivid videos when applied to stickers. To bridge\nthis gap, we employ a two-stage finetuning pipeline: first with weakly\nin-domain data, followed by human-in-the-loop (HITL) strategy which we term\nensemble-of-teachers. It distills the best qualities of multiple teachers into\na smaller student model. We show that this strategy allows us to specifically\ntarget improvements to motion quality while maintaining the style from the\nstatic image. With inference optimizations, our model is able to generate an\neight-frame video with high-quality, interesting, and relevant motion in under\none second.",
        "publication_date": "2024-02-08T22:49:32Z",
        "upvotes": 9
    },
    "2402.06187": {
        "url": "https://arxiv.org/abs/2402.06187",
        "title": "Premier-TACO is a Few-Shot Policy Learner: Pretraining Multitask\n  Representation via Temporal Action-Driven Contrastive Loss",
        "authors": [
            "Ruijie Zheng",
            "Yongyuan Liang",
            "Xiyao Wang",
            "Shuang Ma",
            "Hal Daum\u00e9 III",
            "Huazhe Xu",
            "John Langford",
            "Praveen Palanisamy",
            "Kalyan Shankar Basu",
            "Furong Huang"
        ],
        "abstract": "We present Premier-TACO, a multitask feature representation learning approach\ndesigned to improve few-shot policy learning efficiency in sequential\ndecision-making tasks. Premier-TACO leverages a subset of multitask offline\ndatasets for pretraining a general feature representation, which captures\ncritical environmental dynamics and is fine-tuned using minimal expert\ndemonstrations. It advances the temporal action contrastive learning (TACO)\nobjective, known for state-of-the-art results in visual control tasks, by\nincorporating a novel negative example sampling strategy. This strategy is\ncrucial in significantly boosting TACO's computational efficiency, making\nlarge-scale multitask offline pretraining feasible. Our extensive empirical\nevaluation in a diverse set of continuous control benchmarks including Deepmind\nControl Suite, MetaWorld, and LIBERO demonstrate Premier-TACO's effectiveness\nin pretraining visual representations, significantly enhancing few-shot\nimitation learning of novel tasks. Our code, pretraining data, as well as\npretrained model checkpoints will be released at\nhttps://github.com/PremierTACO/premier-taco. Our project webpage is at\nhttps://premiertaco.github.io.",
        "publication_date": "2024-02-09T05:04:40Z",
        "upvotes": 8
    },
    "2402.06147": {
        "url": "https://arxiv.org/abs/2402.06147",
        "title": "DeAL: Decoding-time Alignment for Large Language Models",
        "authors": [
            "James Y. Huang",
            "Sailik Sengupta",
            "Daniele Bonadiman",
            "Yi-an Lai",
            "Arshit Gupta",
            "Nikolaos Pappas",
            "Saab Mansour",
            "Katrin Kirchhoff",
            "Dan Roth"
        ],
        "abstract": "Large Language Models (LLMs) are nowadays expected to generate content\naligned with human preferences. Current work focuses on alignment at model\ntraining time, through techniques such as Reinforcement Learning with Human\nFeedback (RLHF). However, it is unclear if such methods are an effective choice\nto teach alignment objectives to the model. First, the inability to incorporate\nmultiple, custom rewards and reliance on a model developer's view of universal\nand static principles are key limitations. Second, the residual gaps in model\ntraining and the reliability of such approaches are also questionable (e.g.\nsusceptibility to jail-breaking even after safety training). To address these,\nwe propose DeAL, a framework that allows the user to customize reward functions\nand enables Decoding-time Alignment of LLMs (DeAL). At its core, we view\ndecoding as a heuristic-guided search process and facilitate the use of a wide\nvariety of alignment objectives. Our experiments with programmatic constraints\nsuch as keyword and length constraints (studied widely in the pre-LLM era) and\nabstract objectives such as harmlessness and helpfulness (proposed in the\npost-LLM era) show that we can DeAL with fine-grained trade-offs, improve\nadherence to alignment objectives, and address residual gaps in LLMs. Lastly,\nwhile DeAL can be effectively paired with RLHF and prompting techniques, its\ngenerality makes decoding slower, an optimization we leave for future work.",
        "publication_date": "2024-02-05T06:12:29Z",
        "upvotes": 7
    },
    "2402.06102": {
        "url": "https://arxiv.org/abs/2402.06102",
        "title": "Real-World Fluid Directed Rigid Body Control via Deep Reinforcement\n  Learning",
        "authors": [
            "Mohak Bhardwaj",
            "Thomas Lampe",
            "Michael Neunert",
            "Francesco Romano",
            "Abbas Abdolmaleki",
            "Arunkumar Byravan",
            "Markus Wulfmeier",
            "Martin Riedmiller",
            "Jonas Buchli"
        ],
        "abstract": "Recent advances in real-world applications of reinforcement learning (RL)\nhave relied on the ability to accurately simulate systems at scale. However,\ndomains such as fluid dynamical systems exhibit complex dynamic phenomena that\nare hard to simulate at high integration rates, limiting the direct application\nof modern deep RL algorithms to often expensive or safety critical hardware. In\nthis work, we introduce \"Box o Flows\", a novel benchtop experimental control\nsystem for systematically evaluating RL algorithms in dynamic real-world\nscenarios. We describe the key components of the Box o Flows, and through a\nseries of experiments demonstrate how state-of-the-art model-free RL algorithms\ncan synthesize a variety of complex behaviors via simple reward specifications.\nFurthermore, we explore the role of offline RL in data-efficient hypothesis\ntesting by reusing past experiences. We believe that the insights gained from\nthis preliminary study and the availability of systems like the Box o Flows\nsupport the way forward for developing systematic RL algorithms that can be\ngenerally applied to complex, dynamical systems. Supplementary material and\nvideos of experiments are available at\nhttps://sites.google.com/view/box-o-flows/home.",
        "publication_date": "2024-02-08T23:35:03Z",
        "upvotes": 4
    },
    "2402.07827": {
        "url": "https://arxiv.org/abs/2402.07827",
        "title": "Aya Model: An Instruction Finetuned Open-Access Multilingual Language\n  Model",
        "authors": [
            "Ahmet \u00dcst\u00fcn",
            "Viraat Aryabumi",
            "Zheng-Xin Yong",
            "Wei-Yin Ko",
            "Daniel D'souza",
            "Gbemileke Onilude",
            "Neel Bhandari",
            "Shivalika Singh",
            "Hui-Lee Ooi",
            "Amr Kayid",
            "Freddie Vargus",
            "Phil Blunsom",
            "Shayne Longpre",
            "Niklas Muennighoff",
            "Marzieh Fadaee",
            "Julia Kreutzer",
            "Sara Hooker"
        ],
        "abstract": "Recent breakthroughs in large language models (LLMs) have centered around a\nhandful of data-rich languages. What does it take to broaden access to\nbreakthroughs beyond first-class citizen languages? Our work introduces Aya, a\nmassively multilingual generative language model that follows instructions in\n101 languages of which over 50% are considered as lower-resourced. Aya\noutperforms mT0 and BLOOMZ on the majority of tasks while covering double the\nnumber of languages. We introduce extensive new evaluation suites that broaden\nthe state-of-art for multilingual eval across 99 languages -- including\ndiscriminative and generative tasks, human evaluation, and simulated win rates\nthat cover both held-out tasks and in-distribution performance. Furthermore, we\nconduct detailed investigations on the optimal finetuning mixture composition,\ndata pruning, as well as the toxicity, bias, and safety of our models. We\nopen-source our instruction datasets and our model at\nhttps://hf.co/CohereForAI/aya-101",
        "publication_date": "2024-02-12T17:34:13Z",
        "upvotes": 43
    },
    "2402.07456": {
        "url": "https://arxiv.org/abs/2402.07456",
        "title": "OS-Copilot: Towards Generalist Computer Agents with Self-Improvement",
        "authors": [
            "Zhiyong Wu",
            "Chengcheng Han",
            "Zichen Ding",
            "Zhenmin Weng",
            "Zhoumianze Liu",
            "Shunyu Yao",
            "Tao Yu",
            "Lingpeng Kong"
        ],
        "abstract": "Autonomous interaction with the computer has been a longstanding challenge\nwith great potential, and the recent proliferation of large language models\n(LLMs) has markedly accelerated progress in building digital agents. However,\nmost of these agents are designed to interact with a narrow domain, such as a\nspecific software or website. This narrow focus constrains their applicability\nfor general computer tasks. To this end, we introduce OS-Copilot, a framework\nto build generalist agents capable of interfacing with comprehensive elements\nin an operating system (OS), including the web, code terminals, files,\nmultimedia, and various third-party applications. We use OS-Copilot to create\nFRIDAY, a self-improving embodied agent for automating general computer tasks.\nOn GAIA, a general AI assistants benchmark, FRIDAY outperforms previous methods\nby 35%, showcasing strong generalization to unseen applications via accumulated\nskills from previous tasks. We also present numerical and quantitative evidence\nthat FRIDAY learns to control and self-improve on Excel and Powerpoint with\nminimal supervision. Our OS-Copilot framework and empirical findings provide\ninfrastructure and insights for future research toward more capable and\ngeneral-purpose computer agents.",
        "publication_date": "2024-02-12T07:29:22Z",
        "upvotes": 37
    },
    "2402.07033": {
        "url": "https://arxiv.org/abs/2402.07033",
        "title": "Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts\n  Models",
        "authors": [
            "Keisuke Kamahori",
            "Yile Gu",
            "Kan Zhu",
            "Baris Kasikci"
        ],
        "abstract": "Large Language Models (LLMs) based on Mixture-of-Experts (MoE) architecture\nare showing promising performance on various tasks. However, running them on\nresource-constrained settings, where GPU memory resources are not abundant, is\nchallenging due to huge model sizes. Existing systems that offload model\nweights to CPU memory suffer from the significant overhead of frequently moving\ndata between CPU and GPU. In this paper, we propose Fiddler, a\nresource-efficient inference engine with CPU-GPU orchestration for MoE models.\nThe key idea of Fiddler is to use the computation ability of the CPU to\nminimize the data movement between the CPU and GPU. Our evaluation shows that\nFiddler can run the uncompressed Mixtral-8x7B model, which exceeds 90GB in\nparameters, to generate over $3$ tokens per second on a single GPU with 24GB\nmemory, showing an order of magnitude improvement over existing methods. The\ncode of Fiddler is publicly available at\n\\url{https://github.com/efeslab/fiddler}",
        "publication_date": "2024-02-10T19:54:08Z",
        "upvotes": 16
    },
    "2402.07872": {
        "url": "https://arxiv.org/abs/2402.07872",
        "title": "PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs",
        "authors": [
            "Soroush Nasiriany",
            "Fei Xia",
            "Wenhao Yu",
            "Ted Xiao",
            "Jacky Liang",
            "Ishita Dasgupta",
            "Annie Xie",
            "Danny Driess",
            "Ayzaan Wahid",
            "Zhuo Xu",
            "Quan Vuong",
            "Tingnan Zhang",
            "Tsang-Wei Edward Lee",
            "Kuang-Huei Lee",
            "Peng Xu",
            "Sean Kirmani",
            "Yuke Zhu",
            "Andy Zeng",
            "Karol Hausman",
            "Nicolas Heess",
            "Chelsea Finn",
            "Sergey Levine",
            "Brian Ichter"
        ],
        "abstract": "Vision language models (VLMs) have shown impressive capabilities across a\nvariety of tasks, from logical reasoning to visual understanding. This opens\nthe door to richer interaction with the world, for example robotic control.\nHowever, VLMs produce only textual outputs, while robotic control and other\nspatial tasks require outputting continuous coordinates, actions, or\ntrajectories. How can we enable VLMs to handle such settings without\nfine-tuning on task-specific data?\n  In this paper, we propose a novel visual prompting approach for VLMs that we\ncall Prompting with Iterative Visual Optimization (PIVOT), which casts tasks as\niterative visual question answering. In each iteration, the image is annotated\nwith a visual representation of proposals that the VLM can refer to (e.g.,\ncandidate robot actions, localizations, or trajectories). The VLM then selects\nthe best ones for the task. These proposals are iteratively refined, allowing\nthe VLM to eventually zero in on the best available answer. We investigate\nPIVOT on real-world robotic navigation, real-world manipulation from images,\ninstruction following in simulation, and additional spatial inference tasks\nsuch as localization. We find, perhaps surprisingly, that our approach enables\nzero-shot control of robotic systems without any robot training data,\nnavigation in a variety of environments, and other capabilities. Although\ncurrent performance is far from perfect, our work highlights potentials and\nlimitations of this new regime and shows a promising approach for\nInternet-Scale VLMs in robotic and spatial reasoning domains. Website:\npivot-prompt.github.io and HuggingFace:\nhttps://huggingface.co/spaces/pivot-prompt/pivot-prompt-demo.",
        "publication_date": "2024-02-12T18:33:47Z",
        "upvotes": 14
    },
    "2402.06852": {
        "url": "https://arxiv.org/abs/2402.06852",
        "title": "ChemLLM: A Chemical Large Language Model",
        "authors": [
            "Di Zhang",
            "Wei Liu",
            "Qian Tan",
            "Jingdan Chen",
            "Hang Yan",
            "Yuliang Yan",
            "Jiatong Li",
            "Weiran Huang",
            "Xiangyu Yue",
            "Dongzhan Zhou",
            "Shufei Zhang",
            "Mao Su",
            "Hansen Zhong",
            "Yuqiang Li",
            "Wanli Ouyang"
        ],
        "abstract": "Large language models (LLMs) have made impressive progress in chemistry\napplications, including molecular property prediction, molecular generation,\nexperimental protocol design, etc. However, the community lacks a\ndialogue-based model specifically designed for chemistry. The challenge arises\nfrom the fact that most chemical data and scientific knowledge are primarily\nstored in structured databases, and the direct use of these structured data\ncompromises the model's ability to maintain coherent dialogue. To tackle this\nissue, we develop a novel template-based instruction construction method that\ntransforms structured knowledge into plain dialogue, making it suitable for\nlanguage model training. By leveraging this approach, we develop ChemLLM, the\nfirst large language model dedicated to chemistry, capable of performing\nvarious tasks across chemical disciplines with smooth dialogue interaction.\nChemLLM beats GPT-3.5 on all three principal tasks in chemistry, i.e., name\nconversion, molecular caption, and reaction prediction, and surpasses GPT-4 on\ntwo of them. Remarkably, ChemLLM also shows exceptional adaptability to related\nmathematical and physical tasks despite being trained mainly on\nchemical-centric corpora. Furthermore, ChemLLM demonstrates proficiency in\nspecialized NLP tasks within chemistry, such as literature translation and\ncheminformatic programming. ChemLLM opens up a new avenue for exploration\nwithin chemical studies, while our method of integrating structured chemical\nknowledge into dialogue systems sets a new frontier for developing LLMs across\nvarious scientific fields. Codes, Datasets, and Model weights are publicly\naccessible at hf.co/AI4Chem/ChemLLM-7B-Chat.",
        "publication_date": "2024-02-10T01:11:59Z",
        "upvotes": 13
    },
    "2402.07319": {
        "url": "https://arxiv.org/abs/2402.07319",
        "title": "ODIN: Disentangled Reward Mitigates Hacking in RLHF",
        "authors": [
            "Lichang Chen",
            "Chen Zhu",
            "Davit Soselia",
            "Jiuhai Chen",
            "Tianyi Zhou",
            "Tom Goldstein",
            "Heng Huang",
            "Mohammad Shoeybi",
            "Bryan Catanzaro"
        ],
        "abstract": "In this work, we study the issue of reward hacking on the response length, a\nchallenge emerging in Reinforcement Learning from Human Feedback (RLHF) on\nLLMs. A well-formatted, verbose but less helpful response from the LLMs can\noften deceive LLMs or even human evaluators to achieve high scores. The same\nissue also holds for some reward models in RL. To address the challenges in\nboth training and evaluation, we establish a more reliable evaluation protocol\nfor comparing different training configurations, which inspects the trade-off\nbetween LLM evaluation score and response length obtained by varying training\nhyperparameters. Based on this evaluation, we conduct large-scale studies,\nwhere the results shed insights into the efficacy of hyperparameters and tricks\nused in RL on mitigating length bias. We further propose to improve the reward\nmodel by jointly training two linear heads on shared feature representations to\npredict the rewards, one trained to correlate with length, and the other\ntrained to decorrelate with length and therefore focus more on the actual\ncontent. We then discard the length head in RL to prevent reward hacking on\nlength. Experiments demonstrate that our approach almost eliminates the reward\ncorrelation with length, and improves the obtained policy by a significant\nmargin.",
        "publication_date": "2024-02-11T22:40:12Z",
        "upvotes": 12
    },
    "2402.07383": {
        "url": "https://arxiv.org/abs/2402.07383",
        "title": "Making Flow-Matching-Based Zero-Shot Text-to-Speech Laugh as You Like",
        "authors": [
            "Naoyuki Kanda",
            "Xiaofei Wang",
            "Sefik Emre Eskimez",
            "Manthan Thakker",
            "Hemin Yang",
            "Zirun Zhu",
            "Min Tang",
            "Canrun Li",
            "Chung-Hsien Tsai",
            "Zhen Xiao",
            "Yufei Xia",
            "Jinzhu Li",
            "Yanqing Liu",
            "Sheng Zhao",
            "Michael Zeng"
        ],
        "abstract": "Laughter is one of the most expressive and natural aspects of human speech,\nconveying emotions, social cues, and humor. However, most text-to-speech (TTS)\nsystems lack the ability to produce realistic and appropriate laughter sounds,\nlimiting their applications and user experience. While there have been prior\nworks to generate natural laughter, they fell short in terms of controlling the\ntiming and variety of the laughter to be generated. In this work, we propose\nELaTE, a zero-shot TTS that can generate natural laughing speech of any speaker\nbased on a short audio prompt with precise control of laughter timing and\nexpression. Specifically, ELaTE works on the audio prompt to mimic the voice\ncharacteristic, the text prompt to indicate the contents of the generated\nspeech, and the input to control the laughter expression, which can be either\nthe start and end times of laughter, or the additional audio prompt that\ncontains laughter to be mimicked. We develop our model based on the foundation\nof conditional flow-matching-based zero-shot TTS, and fine-tune it with\nframe-level representation from a laughter detector as additional conditioning.\nWith a simple scheme to mix small-scale laughter-conditioned data with\nlarge-scale pre-training data, we demonstrate that a pre-trained zero-shot TTS\nmodel can be readily fine-tuned to generate natural laughter with precise\ncontrollability, without losing any quality of the pre-trained zero-shot TTS\nmodel. Through objective and subjective evaluations, we show that ELaTE can\ngenerate laughing speech with significantly higher quality and controllability\ncompared to conventional models. See https://aka.ms/elate/ for demo samples.",
        "publication_date": "2024-02-12T02:58:10Z",
        "upvotes": 12
    },
    "2402.07865": {
        "url": "https://arxiv.org/abs/2402.07865",
        "title": "Prismatic VLMs: Investigating the Design Space of Visually-Conditioned\n  Language Models",
        "authors": [
            "Siddharth Karamcheti",
            "Suraj Nair",
            "Ashwin Balakrishna",
            "Percy Liang",
            "Thomas Kollar",
            "Dorsa Sadigh"
        ],
        "abstract": "Visually-conditioned language models (VLMs) have seen growing adoption in\napplications such as visual dialogue, scene understanding, and robotic task\nplanning; adoption that has fueled a wealth of new models such as LLaVa,\nInstructBLIP, and PaLI-3. Despite the volume of new releases, key design\ndecisions around image preprocessing, architecture, and optimization are\nunder-explored, making it challenging to understand what factors account for\nmodel performance $-$ a challenge further complicated by the lack of objective,\nconsistent evaluations. To address these gaps, we first compile a suite of\nstandardized evaluations spanning visual question answering, object\nlocalization from language, and targeted challenge sets that probe properties\nsuch as hallucination; evaluations that provide calibrated, fine-grained\ninsight into a VLM's capabilities. Second, we rigorously investigate VLMs along\nkey design axes, including pretrained visual representations and quantifying\nthe tradeoffs of using base vs. instruct-tuned language models, amongst others.\nWe couple our analysis with three resource contributions: (1) a unified\nframework for evaluating VLMs, (2) optimized, flexible code for VLM training,\nand (3) checkpoints for all models, including a family of VLMs at the 7-13B\nscale that strictly outperform InstructBLIP and LLaVa v1.5, the\nstate-of-the-art in open-source VLMs.",
        "publication_date": "2024-02-12T18:21:14Z",
        "upvotes": 11
    },
    "2402.07043": {
        "url": "https://arxiv.org/abs/2402.07043",
        "title": "A Tale of Tails: Model Collapse as a Change of Scaling Laws",
        "authors": [
            "Elvis Dohmatob",
            "Yunzhen Feng",
            "Pu Yang",
            "Francois Charton",
            "Julia Kempe"
        ],
        "abstract": "As AI model size grows, neural scaling laws have become a crucial tool to\npredict the improvements of large models when increasing capacity and the size\nof original (human or natural) training data. Yet, the widespread use of\npopular models means that the ecosystem of online data and text will co-evolve\nto progressively contain increased amounts of synthesized data. In this paper\nwe ask: How will the scaling laws change in the inevitable regime where\nsynthetic data makes its way into the training corpus? Will future models,\nstill improve, or be doomed to degenerate up to total (model) collapse? We\ndevelop a theoretical framework of model collapse through the lens of scaling\nlaws. We discover a wide range of decay phenomena, analyzing loss of scaling,\nshifted scaling with number of generations, the ''un-learning\" of skills, and\ngrokking when mixing human and synthesized data. Our theory is validated by\nlarge-scale experiments with a transformer on an arithmetic task and text\ngeneration using the large language model Llama2.",
        "publication_date": "2024-02-10T21:06:34Z",
        "upvotes": 11
    },
    "2402.07871": {
        "url": "https://arxiv.org/abs/2402.07871",
        "title": "Scaling Laws for Fine-Grained Mixture of Experts",
        "authors": [
            "Jakub Krajewski",
            "Jan Ludziejewski",
            "Kamil Adamczewski",
            "Maciej Pi\u00f3ro",
            "Micha\u0142 Krutul",
            "Szymon Antoniak",
            "Kamil Ciebiera",
            "Krystian Kr\u00f3l",
            "Tomasz Odrzyg\u00f3\u017ad\u017a",
            "Piotr Sankowski",
            "Marek Cygan",
            "Sebastian Jaszczur"
        ],
        "abstract": "Mixture of Experts (MoE) models have emerged as a primary solution for\nreducing the computational cost of Large Language Models. In this work, we\nanalyze their scaling properties, incorporating an expanded range of variables.\nSpecifically, we introduce a new hyperparameter, granularity, whose adjustment\nenables precise control over the size of the experts. Building on this, we\nestablish scaling laws for fine-grained MoE, taking into account the number of\ntraining tokens, model size, and granularity. Leveraging these laws, we derive\nthe optimal training configuration for a given computational budget. Our\nfindings not only show that MoE models consistently outperform dense\nTransformers but also highlight that the efficiency gap between dense and MoE\nmodels widens as we scale up the model size and training budget. Furthermore,\nwe demonstrate that the common practice of setting the size of experts in MoE\nto mirror the feed-forward layer is not optimal at almost any computational\nbudget.",
        "publication_date": "2024-02-12T18:33:47Z",
        "upvotes": 10
    },
    "2402.07625": {
        "url": "https://arxiv.org/abs/2402.07625",
        "title": "Autonomous Data Selection with Language Models for Mathematical Texts",
        "authors": [
            "Yifan Zhang",
            "Yifan Luo",
            "Yang Yuan",
            "Andrew Chi-Chih Yao"
        ],
        "abstract": "To improve language models' proficiency in mathematical reasoning via\ncontinual pretraining, we introduce a novel strategy that leverages base\nlanguage models for autonomous data selection. Departing from conventional\nsupervised fine-tuning or trained classifiers with human-annotated data, our\napproach Autonomous Data Selection (AutoDS) utilizes meta-prompted language\nmodels as zero-shot verifiers to evaluate and select high-quality mathematical\ncontent autonomously. To demonstrate the efficacy of our method, we\ncontinuously pretrained a 7B-parameter language model on our curated dataset,\nachieving substantial improvements in downstream performance on the MATH,\nGSM8K, and BIG-Bench Hard (BBH) tasks with a token amount reduced by orders of\nmagnitude compared to previous continual pretraining works. Our method\nshowcases a 2 times increase in pretraining token efficiency compared to\nstate-of-the-art baselines, underscoring the potential of our approach in\nenhancing models' mathematical reasoning capabilities. The AutoMathText dataset\nis available at https://huggingface.co/datasets/math-ai/AutoMathText. The code\nis available at https://github.com/yifanzhang-pro/AutoMathText.",
        "publication_date": "2024-02-12T13:09:21Z",
        "upvotes": 10
    },
    "2402.06859": {
        "url": "https://arxiv.org/abs/2402.06859",
        "title": "LiRank: Industrial Large Scale Ranking Models at LinkedIn",
        "authors": [
            "Fedor Borisyuk",
            "Mingzhou Zhou",
            "Qingquan Song",
            "Siyu Zhu",
            "Birjodh Tiwana",
            "Ganesh Parameswaran",
            "Siddharth Dangi",
            "Lars Hertel",
            "Qiang Xiao",
            "Xiaochen Hou",
            "Yunbo Ouyang",
            "Aman Gupta",
            "Sheallika Singh",
            "Dan Liu",
            "Hailing Cheng",
            "Lei Le",
            "Jonathan Hung",
            "Sathiya Keerthi",
            "Ruoyan Wang",
            "Fengyu Zhang",
            "Mohit Kothari",
            "Chen Zhu",
            "Daqi Sun",
            "Yun Dai",
            "Xun Luan",
            "Sirou Zhu",
            "Zhiwei Wang",
            "Neil Daftary",
            "Qianqi Shen",
            "Chengming Jiang",
            "Haichao Wei",
            "Maneesh Varshney",
            "Amol Ghoting",
            "Souvik Ghosh"
        ],
        "abstract": "We present LiRank, a large-scale ranking framework at LinkedIn that brings to\nproduction state-of-the-art modeling architectures and optimization methods. We\nunveil several modeling improvements, including Residual DCN, which adds\nattention and residual connections to the famous DCNv2 architecture. We share\ninsights into combining and tuning SOTA architectures to create a unified\nmodel, including Dense Gating, Transformers and Residual DCN. We also propose\nnovel techniques for calibration and describe how we productionalized deep\nlearning based explore/exploit methods. To enable effective, production-grade\nserving of large ranking models, we detail how to train and compress models\nusing quantization and vocabulary compression. We provide details about the\ndeployment setup for large-scale use cases of Feed ranking, Jobs\nRecommendations, and Ads click-through rate (CTR) prediction. We summarize our\nlearnings from various A/B tests by elucidating the most effective technical\napproaches. These ideas have contributed to relative metrics improvements\nacross the board at LinkedIn: +0.5% member sessions in the Feed, +1.76%\nqualified job applications for Jobs search and recommendations, and +4.3% for\nAds CTR. We hope this work can provide practical insights and solutions for\npractitioners interested in leveraging large-scale deep ranking systems.",
        "publication_date": "2024-02-10T01:47:10Z",
        "upvotes": 8
    },
    "2402.07610": {
        "url": "https://arxiv.org/abs/2402.07610",
        "title": "Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping",
        "authors": [
            "Haoyu Wang",
            "Guozheng Ma",
            "Ziqiao Meng",
            "Zeyu Qin",
            "Li Shen",
            "Zhong Zhang",
            "Bingzhe Wu",
            "Liu Liu",
            "Yatao Bian",
            "Tingyang Xu",
            "Xueqian Wang",
            "Peilin Zhao"
        ],
        "abstract": "Self-alignment is an effective way to reduce the cost of human annotation\nwhile ensuring promising model capability. However, most current methods\ncomplete the data collection and training steps in a single round, which may\noverlook the continuously improving ability of self-aligned models. This gives\nrise to a key query: What if we do multi-time bootstrapping self-alignment?\nDoes this strategy enhance model performance or lead to rapid degradation? In\nthis paper, our pioneering exploration delves into the impact of bootstrapping\nself-alignment on large language models. Our findings reveal that bootstrapping\nself-alignment markedly surpasses the single-round approach, by guaranteeing\ndata diversity from in-context learning. To further exploit the capabilities of\nbootstrapping, we investigate and adjust the training order of data, which\nyields improved performance of the model. Drawing on these findings, we propose\nStep-On-Feet Tuning (SOFT) which leverages model's continuously enhanced\nfew-shot ability to boost zero or one-shot performance. Based on easy-to-hard\ntraining recipe, we propose SOFT+ which further boost self-alignment's\nperformance. Our experiments demonstrate the efficiency of SOFT (SOFT+) across\nvarious classification and generation tasks, highlighting the potential of\nbootstrapping self-alignment on continually enhancing model alignment\nperformance.",
        "publication_date": "2024-02-12T12:30:42Z",
        "upvotes": 7
    },
    "2402.07207": {
        "url": "https://arxiv.org/abs/2402.07207",
        "title": "GALA3D: Towards Text-to-3D Complex Scene Generation via Layout-guided\n  Generative Gaussian Splatting",
        "authors": [
            "Xiaoyu Zhou",
            "Xingjian Ran",
            "Yajiao Xiong",
            "Jinlin He",
            "Zhiwei Lin",
            "Yongtao Wang",
            "Deqing Sun",
            "Ming-Hsuan Yang"
        ],
        "abstract": "We present GALA3D, generative 3D GAussians with LAyout-guided control, for\neffective compositional text-to-3D generation. We first utilize large language\nmodels (LLMs) to generate the initial layout and introduce a layout-guided 3D\nGaussian representation for 3D content generation with adaptive geometric\nconstraints. We then propose an object-scene compositional optimization\nmechanism with conditioned diffusion to collaboratively generate realistic 3D\nscenes with consistent geometry, texture, scale, and accurate interactions\namong multiple objects while simultaneously adjusting the coarse layout priors\nextracted from the LLMs to align with the generated scene. Experiments show\nthat GALA3D is a user-friendly, end-to-end framework for state-of-the-art\nscene-level 3D content generation and controllable editing while ensuring the\nhigh fidelity of object-level entities within the scene. Source codes and\nmodels will be available at https://gala3d.github.io/.",
        "publication_date": "2024-02-11T13:40:08Z",
        "upvotes": 7
    },
    "2402.07896": {
        "url": "https://arxiv.org/abs/2402.07896",
        "title": "Suppressing Pink Elephants with Direct Principle Feedback",
        "authors": [
            "Louis Castricato",
            "Nathan Lile",
            "Suraj Anand",
            "Hailey Schoelkopf",
            "Siddharth Verma",
            "Stella Biderman"
        ],
        "abstract": "Existing methods for controlling language models, such as RLHF and\nConstitutional AI, involve determining which LLM behaviors are desirable and\ntraining them into a language model. However, in many cases, it is desirable\nfor LLMs to be controllable at inference time, so that they can be used in\nmultiple contexts with diverse needs. We illustrate this with the Pink Elephant\nProblem: instructing an LLM to avoid discussing a certain entity (a ``Pink\nElephant''), and instead discuss a preferred entity (``Grey Elephant''). We\napply a novel simplification of Constitutional AI, Direct Principle Feedback,\nwhich skips the ranking of responses and uses DPO directly on critiques and\nrevisions. Our results show that after DPF fine-tuning on our synthetic Pink\nElephants dataset, our 13B fine-tuned LLaMA 2 model significantly outperforms\nLlama-2-13B-Chat and a prompted baseline, and performs as well as GPT-4 in on\nour curated test set assessing the Pink Elephant Problem.",
        "publication_date": "2024-02-12T18:57:46Z",
        "upvotes": 6
    },
    "2402.07876": {
        "url": "https://arxiv.org/abs/2402.07876",
        "title": "Policy Improvement using Language Feedback Models",
        "authors": [
            "Victor Zhong",
            "Dipendra Misra",
            "Xingdi Yuan",
            "Marc-Alexandre C\u00f4t\u00e9"
        ],
        "abstract": "We introduce Language Feedback Models (LFMs) that identify desirable\nbehaviour - actions that help achieve tasks specified in the instruction - for\nimitation learning in instruction following. To train LFMs, we obtain feedback\nfrom Large Language Models (LLMs) on visual trajectories verbalized to language\ndescriptions. First, by using LFMs to identify desirable behaviour to imitate,\nwe improve in task-completion rate over strong behavioural cloning baselines on\nthree distinct language grounding environments (Touchdown, ScienceWorld, and\nALFWorld). Second, LFMs outperform using LLMs as experts to directly predict\nactions, when controlling for the number of LLM output tokens. Third, LFMs\ngeneralize to unseen environments, improving task-completion rate by 3.5-12.0%\nthrough one round of adaptation. Finally, LFM can be modified to provide\nhuman-interpretable feedback without performance loss, allowing human\nverification of desirable behaviour for imitation learning.",
        "publication_date": "2024-02-12T18:41:34Z",
        "upvotes": 5
    },
    "2402.08093": {
        "url": "https://arxiv.org/abs/2402.08093",
        "title": "BASE TTS: Lessons from building a billion-parameter Text-to-Speech model\n  on 100K hours of data",
        "authors": [
            "Mateusz \u0141ajszczak",
            "Guillermo C\u00e1mbara",
            "Yang Li",
            "Fatih Beyhan",
            "Arent van Korlaar",
            "Fan Yang",
            "Arnaud Joly",
            "\u00c1lvaro Mart\u00edn-Cortinas",
            "Ammar Abbas",
            "Adam Michalski",
            "Alexis Moinet",
            "Sri Karlapati",
            "Ewa Muszy\u0144ska",
            "Haohan Guo",
            "Bartosz Putrycz",
            "Soledad L\u00f3pez Gambino",
            "Kayeon Yoo",
            "Elena Sokolova",
            "Thomas Drugman"
        ],
        "abstract": "We introduce a text-to-speech (TTS) model called BASE TTS, which stands for\n$\\textbf{B}$ig $\\textbf{A}$daptive $\\textbf{S}$treamable TTS with\n$\\textbf{E}$mergent abilities. BASE TTS is the largest TTS model to-date,\ntrained on 100K hours of public domain speech data, achieving a new\nstate-of-the-art in speech naturalness. It deploys a 1-billion-parameter\nautoregressive Transformer that converts raw texts into discrete codes\n(\"speechcodes\") followed by a convolution-based decoder which converts these\nspeechcodes into waveforms in an incremental, streamable manner. Further, our\nspeechcodes are built using a novel speech tokenization technique that features\nspeaker ID disentanglement and compression with byte-pair encoding. Echoing the\nwidely-reported \"emergent abilities\" of large language models when trained on\nincreasing volume of data, we show that BASE TTS variants built with 10K+ hours\nand 500M+ parameters begin to demonstrate natural prosody on textually complex\nsentences. We design and share a specialized dataset to measure these emergent\nabilities for text-to-speech. We showcase state-of-the-art naturalness of BASE\nTTS by evaluating against baselines that include publicly available large-scale\ntext-to-speech systems: YourTTS, Bark and TortoiseTTS. Audio samples generated\nby the model can be heard at https://amazon-ltts-paper.com/.",
        "publication_date": "2024-02-12T22:21:30Z",
        "upvotes": 52
    },
    "2402.08609": {
        "url": "https://arxiv.org/abs/2402.08609",
        "title": "Mixtures of Experts Unlock Parameter Scaling for Deep RL",
        "authors": [
            "Johan Obando-Ceron",
            "Ghada Sokar",
            "Timon Willi",
            "Clare Lyle",
            "Jesse Farebrother",
            "Jakob Foerster",
            "Gintare Karolina Dziugaite",
            "Doina Precup",
            "Pablo Samuel Castro"
        ],
        "abstract": "The recent rapid progress in (self) supervised learning models is in large\npart predicted by empirical scaling laws: a model's performance scales\nproportionally to its size. Analogous scaling laws remain elusive for\nreinforcement learning domains, however, where increasing the parameter count\nof a model often hurts its final performance. In this paper, we demonstrate\nthat incorporating Mixture-of-Expert (MoE) modules, and in particular Soft MoEs\n(Puigcerver et al., 2023), into value-based networks results in more\nparameter-scalable models, evidenced by substantial performance increases\nacross a variety of training regimes and model sizes. This work thus provides\nstrong empirical evidence towards developing scaling laws for reinforcement\nlearning.",
        "publication_date": "2024-02-13T17:18:56Z",
        "upvotes": 34
    },
    "2402.08268": {
        "url": "https://arxiv.org/abs/2402.08268",
        "title": "World Model on Million-Length Video And Language With Blockwise\n  RingAttention",
        "authors": [
            "Hao Liu",
            "Wilson Yan",
            "Matei Zaharia",
            "Pieter Abbeel"
        ],
        "abstract": "Current language models fall short in understanding aspects of the world not\neasily described in words, and struggle with complex, long-form tasks. Video\nsequences offer valuable temporal information absent in language and static\nimages, making them attractive for joint modeling with language. Such models\ncould develop a understanding of both human textual knowledge and the physical\nworld, enabling broader AI capabilities for assisting humans. However, learning\nfrom millions of tokens of video and language sequences poses challenges due to\nmemory constraints, computational complexity, and limited datasets. To address\nthese challenges, we curate a large dataset of diverse videos and books,\nutilize the Blockwise RingAttention technique to scalably train on long\nsequences, and gradually increase context size from 4K to 1M tokens. This paper\nmakes the following contributions: (a) Largest context size neural network: We\ntrain one of the largest context size transformers on long video and language\nsequences, setting new benchmarks in difficult retrieval tasks and long video\nunderstanding. (b) Solutions for overcoming vision-language training\nchallenges, including using masked sequence packing for mixing different\nsequence lengths, loss weighting to balance language and vision, and\nmodel-generated QA dataset for long sequence chat. (c) A highly-optimized\nimplementation with RingAttention, Blockwise Transformers, masked sequence\npacking, and other key features for training on millions-length multimodal\nsequences. (d) Fully open-sourced a family of 7B parameter models capable of\nprocessing long text documents (LWM-Text, LWM-Text-Chat) and videos (LWM,\nLWM-Chat) of over 1M tokens. This work paves the way for training on massive\ndatasets of long video and language to develop understanding of both human\nknowledge and the multimodal world, and broader capabilities.",
        "publication_date": "2024-02-13T07:47:36Z",
        "upvotes": 33
    },
    "2402.08017": {
        "url": "https://arxiv.org/abs/2402.08017",
        "title": "Lumos : Empowering Multimodal LLMs with Scene Text Recognition",
        "authors": [
            "Ashish Shenoy",
            "Yichao Lu",
            "Srihari Jayakumar",
            "Debojeet Chatterjee",
            "Mohsen Moslehpour",
            "Pierce Chuang",
            "Abhay Harpale",
            "Vikas Bhardwaj",
            "Di Xu",
            "Shicong Zhao",
            "Longfang Zhao",
            "Ankit Ramchandani",
            "Xin Luna Dong",
            "Anuj Kumar"
        ],
        "abstract": "We introduce Lumos, the first end-to-end multimodal question-answering system\nwith text understanding capabilities. At the core of Lumos is a Scene Text\nRecognition (STR) component that extracts text from first person point-of-view\nimages, the output of which is used to augment input to a Multimodal Large\nLanguage Model (MM-LLM). While building Lumos, we encountered numerous\nchallenges related to STR quality, overall latency, and model inference. In\nthis paper, we delve into those challenges, and discuss the system\narchitecture, design choices, and modeling techniques employed to overcome\nthese obstacles. We also provide a comprehensive evaluation for each component,\nshowcasing high quality and efficiency.",
        "publication_date": "2024-02-12T19:27:26Z",
        "upvotes": 22
    },
    "2402.07939": {
        "url": "https://arxiv.org/abs/2402.07939",
        "title": "UFO: A UI-Focused Agent for Windows OS Interaction",
        "authors": [
            "Chaoyun Zhang",
            "Liqun Li",
            "Shilin He",
            "Xu Zhang",
            "Bo Qiao",
            "Si Qin",
            "Minghua Ma",
            "Yu Kang",
            "Qingwei Lin",
            "Saravan Rajmohan",
            "Dongmei Zhang",
            "Qi Zhang"
        ],
        "abstract": "We introduce UFO, an innovative UI-Focused agent to fulfill user requests\ntailored to applications on Windows OS, harnessing the capabilities of\nGPT-Vision. UFO employs a dual-agent framework to meticulously observe and\nanalyze the graphical user interface (GUI) and control information of Windows\napplications. This enables the agent to seamlessly navigate and operate within\nindividual applications and across them to fulfill user requests, even when\nspanning multiple applications. The framework incorporates a control\ninteraction module, facilitating action grounding without human intervention\nand enabling fully automated execution. Consequently, UFO transforms arduous\nand time-consuming processes into simple tasks achievable solely through\nnatural language commands. We conducted testing of UFO across 9 popular Windows\napplications, encompassing a variety of scenarios reflective of users' daily\nusage. The results, derived from both quantitative metrics and real-case\nstudies, underscore the superior effectiveness of UFO in fulfilling user\nrequests. To the best of our knowledge, UFO stands as the first UI agent\nspecifically tailored for task completion within the Windows OS environment.\nThe open-source code for UFO is available on https://github.com/microsoft/UFO.",
        "publication_date": "2024-02-08T15:40:35Z",
        "upvotes": 13
    },
    "2402.08678": {
        "url": "https://arxiv.org/abs/2402.08678",
        "title": "Graph Mamba: Towards Learning on Graphs with State Space Models",
        "authors": [
            "Ali Behrouz",
            "Farnoosh Hashemi"
        ],
        "abstract": "Graph Neural Networks (GNNs) have shown promising potential in graph\nrepresentation learning. The majority of GNNs define a local message-passing\nmechanism, propagating information over the graph by stacking multiple layers.\nThese methods, however, are known to suffer from two major limitations:\nover-squashing and poor capturing of long-range dependencies. Recently, Graph\nTransformers (GTs) emerged as a powerful alternative to Message-Passing Neural\nNetworks (MPNNs). GTs, however, have quadratic computational cost, lack\ninductive biases on graph structures, and rely on complex Positional/Structural\nEncodings (SE/PE). In this paper, we show that while Transformers, complex\nmessage-passing, and SE/PE are sufficient for good performance in practice,\nneither is necessary. Motivated by the recent success of State Space Models\n(SSMs), such as Mamba, we present Graph Mamba Networks (GMNs), a general\nframework for a new class of GNNs based on selective SSMs. We discuss and\ncategorize the new challenges when adapting SSMs to graph-structured data, and\npresent four required and one optional steps to design GMNs, where we choose\n(1) Neighborhood Tokenization, (2) Token Ordering, (3) Architecture of\nBidirectional Selective SSM Encoder, (4) Local Encoding, and dispensable (5) PE\nand SE. We further provide theoretical justification for the power of GMNs.\nExperiments demonstrate that despite much less computational cost, GMNs attain\nan outstanding performance in long-range, small-scale, large-scale, and\nheterophilic benchmark datasets.",
        "publication_date": "2024-02-13T18:58:17Z",
        "upvotes": 11
    },
    "2402.08682": {
        "url": "https://arxiv.org/abs/2402.08682",
        "title": "IM-3D: Iterative Multiview Diffusion and Reconstruction for High-Quality\n  3D Generation",
        "authors": [
            "Luke Melas-Kyriazi",
            "Iro Laina",
            "Christian Rupprecht",
            "Natalia Neverova",
            "Andrea Vedaldi",
            "Oran Gafni",
            "Filippos Kokkinos"
        ],
        "abstract": "Most text-to-3D generators build upon off-the-shelf text-to-image models\ntrained on billions of images. They use variants of Score Distillation Sampling\n(SDS), which is slow, somewhat unstable, and prone to artifacts. A mitigation\nis to fine-tune the 2D generator to be multi-view aware, which can help\ndistillation or can be combined with reconstruction networks to output 3D\nobjects directly. In this paper, we further explore the design space of\ntext-to-3D models. We significantly improve multi-view generation by\nconsidering video instead of image generators. Combined with a 3D\nreconstruction algorithm which, by using Gaussian splatting, can optimize a\nrobust image-based loss, we directly produce high-quality 3D outputs from the\ngenerated views. Our new method, IM-3D, reduces the number of evaluations of\nthe 2D generator network 10-100x, resulting in a much more efficient pipeline,\nbetter quality, fewer geometric inconsistencies, and higher yield of usable 3D\nassets.",
        "publication_date": "2024-02-13T18:59:51Z",
        "upvotes": 11
    },
    "2402.08303": {
        "url": "https://arxiv.org/abs/2402.08303",
        "title": "ChatCell: Facilitating Single-Cell Analysis with Natural Language",
        "authors": [
            "Yin Fang",
            "Kangwei Liu",
            "Ningyu Zhang",
            "Xinle Deng",
            "Penghui Yang",
            "Zhuo Chen",
            "Xiangru Tang",
            "Mark Gerstein",
            "Xiaohui Fan",
            "Huajun Chen"
        ],
        "abstract": "As Large Language Models (LLMs) rapidly evolve, their influence in science is\nbecoming increasingly prominent. The emerging capabilities of LLMs in task\ngeneralization and free-form dialogue can significantly advance fields like\nchemistry and biology. However, the field of single-cell biology, which forms\nthe foundational building blocks of living organisms, still faces several\nchallenges. High knowledge barriers and limited scalability in current methods\nrestrict the full exploitation of LLMs in mastering single-cell data, impeding\ndirect accessibility and rapid iteration. To this end, we introduce ChatCell,\nwhich signifies a paradigm shift by facilitating single-cell analysis with\nnatural language. Leveraging vocabulary adaptation and unified sequence\ngeneration, ChatCell has acquired profound expertise in single-cell biology and\nthe capability to accommodate a diverse range of analysis tasks. Extensive\nexperiments further demonstrate ChatCell's robust performance and potential to\ndeepen single-cell insights, paving the way for more accessible and intuitive\nexploration in this pivotal field. Our project homepage is available at\nhttps://zjunlp.github.io/project/ChatCell.",
        "publication_date": "2024-02-13T09:06:14Z",
        "upvotes": 9
    },
    "2402.08654": {
        "url": "https://arxiv.org/abs/2402.08654",
        "title": "Learning Continuous 3D Words for Text-to-Image Generation",
        "authors": [
            "Ta-Ying Cheng",
            "Matheus Gadelha",
            "Thibault Groueix",
            "Matthew Fisher",
            "Radomir Mech",
            "Andrew Markham",
            "Niki Trigoni"
        ],
        "abstract": "Current controls over diffusion models (e.g., through text or ControlNet) for\nimage generation fall short in recognizing abstract, continuous attributes like\nillumination direction or non-rigid shape change. In this paper, we present an\napproach for allowing users of text-to-image models to have fine-grained\ncontrol of several attributes in an image. We do this by engineering special\nsets of input tokens that can be transformed in a continuous manner -- we call\nthem Continuous 3D Words. These attributes can, for example, be represented as\nsliders and applied jointly with text prompts for fine-grained control over\nimage generation. Given only a single mesh and a rendering engine, we show that\nour approach can be adopted to provide continuous user control over several\n3D-aware attributes, including time-of-day illumination, bird wing orientation,\ndollyzoom effect, and object poses. Our method is capable of conditioning image\ncreation with multiple Continuous 3D Words and text descriptions simultaneously\nwhile adding no overhead to the generative process. Project Page:\nhttps://ttchengab.github.io/continuous_3d_words",
        "publication_date": "2024-02-13T18:34:10Z",
        "upvotes": 9
    },
    "2402.08420": {
        "url": "https://arxiv.org/abs/2402.08420",
        "title": "Vision-Based Hand Gesture Customization from a Single Demonstration",
        "authors": [
            "Soroush Shahi",
            "Cori Tymoszek Park",
            "Richard Kang",
            "Asaf Liberman",
            "Oron Levy",
            "Jun Gong",
            "Abdelkareem Bedri",
            "Gierad Laput"
        ],
        "abstract": "Hand gesture recognition is becoming a more prevalent mode of human-computer\ninteraction, especially as cameras proliferate across everyday devices. Despite\ncontinued progress in this field, gesture customization is often underexplored.\nCustomization is crucial since it enables users to define and demonstrate\ngestures that are more natural, memorable, and accessible. However,\ncustomization requires efficient usage of user-provided data. We introduce a\nmethod that enables users to easily design bespoke gestures with a monocular\ncamera from one demonstration. We employ transformers and meta-learning\ntechniques to address few-shot learning challenges. Unlike prior work, our\nmethod supports any combination of one-handed, two-handed, static, and dynamic\ngestures, including different viewpoints. We evaluated our customization method\nthrough a user study with 20 gestures collected from 21 participants, achieving\nup to 97% average recognition accuracy from one demonstration. Our work\nprovides a viable path for vision-based gesture customization, laying the\nfoundation for future advancements in this domain.",
        "publication_date": "2024-02-13T12:49:13Z",
        "upvotes": 7
    },
    "2402.08644": {
        "url": "https://arxiv.org/abs/2402.08644",
        "title": "Tandem Transformers for Inference Efficient LLMs",
        "authors": [
            "Aishwarya P S",
            "Pranav Ajit Nair",
            "Yashas Samaga",
            "Toby Boyd",
            "Sanjiv Kumar",
            "Prateek Jain",
            "Praneeth Netrapalli"
        ],
        "abstract": "The autoregressive nature of conventional large language models (LLMs)\ninherently limits inference speed, as tokens are generated sequentially. While\nspeculative and parallel decoding techniques attempt to mitigate this, they\nface limitations: either relying on less accurate smaller models for generation\nor failing to fully leverage the base LLM's representations.\n  We introduce a novel architecture, Tandem transformers, to address these\nissues. This architecture uniquely combines (1) a small autoregressive model\nand (2) a large model operating in block mode (processing multiple tokens\nsimultaneously). The small model's predictive accuracy is substantially\nenhanced by granting it attention to the large model's richer representations.\nOn the PaLM2 pretraining dataset, a tandem of PaLM2-Bison and PaLM2-Gecko\ndemonstrates a 3.3% improvement in next-token prediction accuracy over a\nstandalone PaLM2-Gecko, offering a 1.16x speedup compared to a PaLM2-Otter\nmodel with comparable downstream performance. We further incorporate the tandem\nmodel within the speculative decoding (SPEED) framework where the large model\nvalidates tokens from the small model. This ensures that the Tandem of\nPaLM2-Bison and PaLM2-Gecko achieves substantial speedup (around 1.14x faster\nthan using vanilla PaLM2-Gecko in SPEED) while maintaining identical downstream\ntask accuracy.",
        "publication_date": "2024-02-13T18:24:08Z",
        "upvotes": 7
    },
    "2402.08622": {
        "url": "https://arxiv.org/abs/2402.08622",
        "title": "NeRF Analogies: Example-Based Visual Attribute Transfer for NeRFs",
        "authors": [
            "Michael Fischer",
            "Zhengqin Li",
            "Thu Nguyen-Phuoc",
            "Aljaz Bozic",
            "Zhao Dong",
            "Carl Marshall",
            "Tobias Ritschel"
        ],
        "abstract": "A Neural Radiance Field (NeRF) encodes the specific relation of 3D geometry\nand appearance of a scene. We here ask the question whether we can transfer the\nappearance from a source NeRF onto a target 3D geometry in a semantically\nmeaningful way, such that the resulting new NeRF retains the target geometry\nbut has an appearance that is an analogy to the source NeRF. To this end, we\ngeneralize classic image analogies from 2D images to NeRFs. We leverage\ncorrespondence transfer along semantic affinity that is driven by semantic\nfeatures from large, pre-trained 2D image models to achieve multi-view\nconsistent appearance transfer. Our method allows exploring the mix-and-match\nproduct space of 3D geometry and appearance. We show that our method\noutperforms traditional stylization-based methods and that a large majority of\nusers prefer our method over several typical baselines.",
        "publication_date": "2024-02-13T17:47:42Z",
        "upvotes": 3
    },
    "2402.09368": {
        "url": "https://arxiv.org/abs/2402.09368",
        "title": "Magic-Me: Identity-Specific Video Customized Diffusion",
        "authors": [
            "Ze Ma",
            "Daquan Zhou",
            "Chun-Hsiao Yeh",
            "Xue-She Wang",
            "Xiuyu Li",
            "Huanrui Yang",
            "Zhen Dong",
            "Kurt Keutzer",
            "Jiashi Feng"
        ],
        "abstract": "Creating content with specified identities (ID) has attracted significant\ninterest in the field of generative models. In the field of text-to-image\ngeneration (T2I), subject-driven creation has achieved great progress with the\nidentity controlled via reference images. However, its extension to video\ngeneration is not well explored. In this work, we propose a simple yet\neffective subject identity controllable video generation framework, termed\nVideo Custom Diffusion (VCD). With a specified identity defined by a few\nimages, VCD reinforces the identity characteristics and injects frame-wise\ncorrelation at the initialization stage for stable video outputs. To achieve\nthis, we propose three novel components that are essential for high-quality\nidentity preservation and stable video generation: 1) a noise initialization\nmethod with 3D Gaussian Noise Prior for better inter-frame stability; 2) an ID\nmodule based on extended Textual Inversion trained with the cropped identity to\ndisentangle the ID information from the background 3) Face VCD and Tiled VCD\nmodules to reinforce faces and upscale the video to higher resolution while\npreserving the identity's features. We conducted extensive experiments to\nverify that VCD is able to generate stable videos with better ID over the\nbaselines. Besides, with the transferability of the encoded identity in the ID\nmodule, VCD is also working well with personalized text-to-image models\navailable publicly. The codes are available at\nhttps://github.com/Zhen-Dong/Magic-Me.",
        "publication_date": "2024-02-14T18:13:51Z",
        "upvotes": 24
    },
    "2402.08939": {
        "url": "https://arxiv.org/abs/2402.08939",
        "title": "Premise Order Matters in Reasoning with Large Language Models",
        "authors": [
            "Xinyun Chen",
            "Ryan A. Chi",
            "Xuezhi Wang",
            "Denny Zhou"
        ],
        "abstract": "Large language models (LLMs) have accomplished remarkable reasoning\nperformance in various domains. However, in the domain of reasoning tasks, we\ndiscover a frailty: LLMs are surprisingly brittle to the ordering of the\npremises, despite the fact that such ordering does not alter the underlying\ntask. In particular, we observe that LLMs achieve the best performance when the\npremise order aligns with the context required in intermediate reasoning steps.\nFor example, in deductive reasoning tasks, presenting the premises in the same\norder as the ground truth proof in the prompt (as opposed to random ordering)\ndrastically increases the model's accuracy. We first examine the effect of\npremise ordering on deductive reasoning on a variety of LLMs, and our\nevaluation shows that permuting the premise order can cause a performance drop\nof over 30%. In addition, we release the benchmark R-GSM, based on GSM8K, to\nexamine the ordering effect for mathematical problem-solving, and we again\nobserve a significant drop in accuracy, relative to the original GSM8K\nbenchmark.",
        "publication_date": "2024-02-14T04:50:18Z",
        "upvotes": 23
    },
    "2402.09052": {
        "url": "https://arxiv.org/abs/2402.09052",
        "title": "L3GO: Language Agents with Chain-of-3D-Thoughts for Generating\n  Unconventional Objects",
        "authors": [
            "Yutaro Yamada",
            "Khyathi Chandu",
            "Yuchen Lin",
            "Jack Hessel",
            "Ilker Yildirim",
            "Yejin Choi"
        ],
        "abstract": "Diffusion-based image generation models such as DALL-E 3 and Stable\nDiffusion-XL demonstrate remarkable capabilities in generating images with\nrealistic and unique compositions. Yet, these models are not robust in\nprecisely reasoning about physical and spatial configurations of objects,\nespecially when instructed with unconventional, thereby out-of-distribution\ndescriptions, such as \"a chair with five legs\". In this paper, we propose a\nlanguage agent with chain-of-3D-thoughts (L3GO), an inference-time approach\nthat can reason about part-based 3D mesh generation of unconventional objects\nthat current data-driven diffusion models struggle with. More concretely, we\nuse large language models as agents to compose a desired object via\ntrial-and-error within the 3D simulation environment. To facilitate our\ninvestigation, we develop a new benchmark, Unconventionally Feasible Objects\n(UFO), as well as SimpleBlenv, a wrapper environment built on top of Blender\nwhere language agents can build and compose atomic building blocks via API\ncalls. Human and automatic GPT-4V evaluations show that our approach surpasses\nthe standard GPT-4 and other language agents (e.g., ReAct and Reflexion) for 3D\nmesh generation on ShapeNet. Moreover, when tested on our UFO benchmark, our\napproach outperforms other state-of-the-art text-to-2D image and text-to-3D\nmodels based on human evaluation.",
        "publication_date": "2024-02-14T09:51:05Z",
        "upvotes": 15
    },
    "2402.09371": {
        "url": "https://arxiv.org/abs/2402.09371",
        "title": "Transformers Can Achieve Length Generalization But Not Robustly",
        "authors": [
            "Yongchao Zhou",
            "Uri Alon",
            "Xinyun Chen",
            "Xuezhi Wang",
            "Rishabh Agarwal",
            "Denny Zhou"
        ],
        "abstract": "Length generalization, defined as the ability to extrapolate from shorter\ntraining sequences to longer test ones, is a significant challenge for language\nmodels. This issue persists even with large-scale Transformers handling\nrelatively straightforward tasks. In this paper, we test the Transformer's\nability of length generalization using the task of addition of two integers. We\nshow that the success of length generalization is intricately linked to the\ndata format and the type of position encoding. Using the right combination of\ndata format and position encodings, we show for the first time that standard\nTransformers can extrapolate to a sequence length that is 2.5x the input\nlength. Nevertheless, unlike in-distribution generalization, length\ngeneralization remains fragile, significantly influenced by factors like random\nweight initialization and training data order, leading to large variances\nacross different random seeds.",
        "publication_date": "2024-02-14T18:18:29Z",
        "upvotes": 12
    },
    "2402.08797": {
        "url": "https://arxiv.org/abs/2402.08797",
        "title": "Computing Power and the Governance of Artificial Intelligence",
        "authors": [
            "Girish Sastry",
            "Lennart Heim",
            "Haydn Belfield",
            "Markus Anderljung",
            "Miles Brundage",
            "Julian Hazell",
            "Cullen O'Keefe",
            "Gillian K. Hadfield",
            "Richard Ngo",
            "Konstantin Pilz",
            "George Gor",
            "Emma Bluemke",
            "Sarah Shoker",
            "Janet Egan",
            "Robert F. Trager",
            "Shahar Avin",
            "Adrian Weller",
            "Yoshua Bengio",
            "Diane Coyle"
        ],
        "abstract": "Computing power, or \"compute,\" is crucial for the development and deployment\nof artificial intelligence (AI) capabilities. As a result, governments and\ncompanies have started to leverage compute as a means to govern AI. For\nexample, governments are investing in domestic compute capacity, controlling\nthe flow of compute to competing countries, and subsidizing compute access to\ncertain sectors. However, these efforts only scratch the surface of how compute\ncan be used to govern AI development and deployment. Relative to other key\ninputs to AI (data and algorithms), AI-relevant compute is a particularly\neffective point of intervention: it is detectable, excludable, and\nquantifiable, and is produced via an extremely concentrated supply chain. These\ncharacteristics, alongside the singular importance of compute for cutting-edge\nAI models, suggest that governing compute can contribute to achieving common\npolicy objectives, such as ensuring the safety and beneficial use of AI. More\nprecisely, policymakers could use compute to facilitate regulatory visibility\nof AI, allocate resources to promote beneficial outcomes, and enforce\nrestrictions against irresponsible or malicious AI development and usage.\nHowever, while compute-based policies and technologies have the potential to\nassist in these areas, there is significant variation in their readiness for\nimplementation. Some ideas are currently being piloted, while others are\nhindered by the need for fundamental research. Furthermore, naive or poorly\nscoped approaches to compute governance carry significant risks in areas like\nprivacy, economic impacts, and centralization of power. We end by suggesting\nguardrails to minimize these risks from compute governance.",
        "publication_date": "2024-02-13T21:10:21Z",
        "upvotes": 11
    },
    "2402.08714": {
        "url": "https://arxiv.org/abs/2402.08714",
        "title": "PRDP: Proximal Reward Difference Prediction for Large-Scale Reward\n  Finetuning of Diffusion Models",
        "authors": [
            "Fei Deng",
            "Qifei Wang",
            "Wei Wei",
            "Matthias Grundmann",
            "Tingbo Hou"
        ],
        "abstract": "Reward finetuning has emerged as a promising approach to aligning foundation\nmodels with downstream objectives. Remarkable success has been achieved in the\nlanguage domain by using reinforcement learning (RL) to maximize rewards that\nreflect human preference. However, in the vision domain, existing RL-based\nreward finetuning methods are limited by their instability in large-scale\ntraining, rendering them incapable of generalizing to complex, unseen prompts.\nIn this paper, we propose Proximal Reward Difference Prediction (PRDP),\nenabling stable black-box reward finetuning for diffusion models for the first\ntime on large-scale prompt datasets with over 100K prompts. Our key innovation\nis the Reward Difference Prediction (RDP) objective that has the same optimal\nsolution as the RL objective while enjoying better training stability.\nSpecifically, the RDP objective is a supervised regression objective that tasks\nthe diffusion model with predicting the reward difference of generated image\npairs from their denoising trajectories. We theoretically prove that the\ndiffusion model that obtains perfect reward difference prediction is exactly\nthe maximizer of the RL objective. We further develop an online algorithm with\nproximal updates to stably optimize the RDP objective. In experiments, we\ndemonstrate that PRDP can match the reward maximization ability of\nwell-established RL-based methods in small-scale training. Furthermore, through\nlarge-scale training on text prompts from the Human Preference Dataset v2 and\nthe Pick-a-Pic v1 dataset, PRDP achieves superior generation quality on a\ndiverse set of complex, unseen prompts whereas RL-based methods completely\nfail.",
        "publication_date": "2024-02-13T18:58:16Z",
        "upvotes": 10
    },
    "2402.09126": {
        "url": "https://arxiv.org/abs/2402.09126",
        "title": "MPIrigen: MPI Code Generation through Domain-Specific Language Models",
        "authors": [
            "Nadav Schneider",
            "Niranjan Hasabnis",
            "Vy A. Vo",
            "Tal Kadosh",
            "Neva Krien",
            "Mihai Capot\u0103",
            "Abdul Wasay",
            "Guy Tamir",
            "Ted Willke",
            "Nesreen Ahmed",
            "Yuval Pinter",
            "Timothy Mattson",
            "Gal Oren"
        ],
        "abstract": "The imperative need to scale computation across numerous nodes highlights the\nsignificance of efficient parallel computing, particularly in the realm of\nMessage Passing Interface (MPI) integration. The challenging parallel\nprogramming task of generating MPI-based parallel programs has remained\nunexplored. This study first investigates the performance of state-of-the-art\nlanguage models in generating MPI-based parallel programs. Findings reveal that\nwidely used models such as GPT-3.5 and PolyCoder (specialized multi-lingual\ncode models) exhibit notable performance degradation, when generating MPI-based\nprograms compared to general-purpose programs. In contrast, domain-specific\nmodels such as MonoCoder, which are pretrained on MPI-related programming\nlanguages of C and C++, outperform larger models. Subsequently, we introduce a\ndedicated downstream task of MPI-based program generation by fine-tuning\nMonoCoder on HPCorpusMPI. We call the resulting model as MPIrigen. We propose\nan innovative preprocessing for completion only after observing the whole code,\nthus enabling better completion with a wider context. Comparative analysis\nagainst GPT-3.5 zero-shot performance, using a novel HPC-oriented evaluation\nmethod, demonstrates that MPIrigen excels in generating accurate MPI functions\nup to 0.8 accuracy in location and function predictions, and with more than 0.9\naccuracy for argument predictions. The success of this tailored solution\nunderscores the importance of domain-specific fine-tuning in optimizing\nlanguage models for parallel computing code generation, paving the way for a\nnew generation of automatic parallelization tools. The sources of this work are\navailable at our GitHub MPIrigen repository:\nhttps://github.com/Scientific-Computing-Lab-NRCN/MPI-rigen",
        "publication_date": "2024-02-14T12:24:21Z",
        "upvotes": 10
    },
    "2402.08855": {
        "url": "https://arxiv.org/abs/2402.08855",
        "title": "GhostWriter: Augmenting Collaborative Human-AI Writing Experiences\n  Through Personalization and Agency",
        "authors": [
            "Catherine Yeh",
            "Gonzalo Ramos",
            "Rachel Ng",
            "Andy Huntington",
            "Richard Banks"
        ],
        "abstract": "Large language models (LLMs) are becoming more prevalent and have found a\nubiquitous use in providing different forms of writing assistance. However,\nLLM-powered writing systems can frustrate users due to their limited\npersonalization and control, which can be exacerbated when users lack\nexperience with prompt engineering. We see design as one way to address these\nchallenges and introduce GhostWriter, an AI-enhanced writing design probe where\nusers can exercise enhanced agency and personalization. GhostWriter leverages\nLLMs to learn the user's intended writing style implicitly as they write, while\nallowing explicit teaching moments through manual style edits and annotations.\nWe study 18 participants who use GhostWriter on two different writing tasks,\nobserving that it helps users craft personalized text generations and empowers\nthem by providing multiple ways to control the system's writing style. From\nthis study, we present insights regarding people's relationship with\nAI-assisted writing and offer design recommendations for future work.",
        "publication_date": "2024-02-13T23:48:59Z",
        "upvotes": 9
    },
    "2402.08958": {
        "url": "https://arxiv.org/abs/2402.08958",
        "title": "Towards Next-Level Post-Training Quantization of Hyper-Scale\n  Transformers",
        "authors": [
            "Junhan Kim",
            "Kyungphil Park",
            "Chungman Lee",
            "Ho-young Kim",
            "Joonyoung Kim",
            "Yongkweon Jeon"
        ],
        "abstract": "With the increasing complexity of generative AI models, post-training\nquantization (PTQ) has emerged as a promising solution for deploying\nhyper-scale models on edge devices such as mobile devices and TVs. Existing PTQ\nschemes, however, consume considerable time and resources, which could be a\nbottleneck in real situations where frequent model updates and multiple\nhyper-parameter tunings are required. As a cost-effective alternative, one-shot\nPTQ schemes have been proposed. Still, the performance is somewhat limited\nbecause they cannot consider the inter-layer dependency within the attention\nmodule, which is a very important feature of Transformers. In this paper, we\nthus propose a novel PTQ algorithm that balances accuracy and efficiency. The\nkey idea of the proposed algorithm called aespa is to perform quantization\nlayer-wise for efficiency while considering cross-layer dependency to preserve\nthe attention score. Through extensive experiments on various language models\nand complexity analysis, we demonstrate that aespa is accurate and efficient in\nquantizing Transformer models.",
        "publication_date": "2024-02-14T05:58:43Z",
        "upvotes": 3
    },
    "2402.10200": {
        "url": "https://arxiv.org/abs/2402.10200",
        "title": "Chain-of-Thought Reasoning Without Prompting",
        "authors": [
            "Xuezhi Wang",
            "Denny Zhou"
        ],
        "abstract": "In enhancing the reasoning capabilities of large language models (LLMs),\nprior research primarily focuses on specific prompting techniques such as\nfew-shot or zero-shot chain-of-thought (CoT) prompting. These methods, while\neffective, often involve manually intensive prompt engineering. Our study takes\na novel approach by asking: Can LLMs reason effectively without prompting? Our\nfindings reveal that, intriguingly, CoT reasoning paths can be elicited from\npre-trained LLMs by simply altering the \\textit{decoding} process. Rather than\nconventional greedy decoding, we investigate the top-$k$ alternative tokens,\nuncovering that CoT paths are frequently inherent in these sequences. This\napproach not only bypasses the confounders of prompting but also allows us to\nassess the LLMs' \\textit{intrinsic} reasoning abilities. Moreover, we observe\nthat the presence of a CoT in the decoding path correlates with a higher\nconfidence in the model's decoded answer. This confidence metric effectively\ndifferentiates between CoT and non-CoT paths. Extensive empirical studies on\nvarious reasoning benchmarks show that the proposed CoT-decoding substantially\noutperforms the standard greedy decoding.",
        "publication_date": "2024-02-15T18:55:41Z",
        "upvotes": 88
    },
    "2402.09906": {
        "url": "https://arxiv.org/abs/2402.09906",
        "title": "Generative Representational Instruction Tuning",
        "authors": [
            "Niklas Muennighoff",
            "Hongjin Su",
            "Liang Wang",
            "Nan Yang",
            "Furu Wei",
            "Tao Yu",
            "Amanpreet Singh",
            "Douwe Kiela"
        ],
        "abstract": "All text-based language problems can be reduced to either generation or\nembedding. Current models only perform well at one or the other. We introduce\ngenerative representational instruction tuning (GRIT) whereby a large language\nmodel is trained to handle both generative and embedding tasks by\ndistinguishing between them through instructions. Compared to other open\nmodels, our resulting GritLM 7B sets a new state of the art on the Massive Text\nEmbedding Benchmark (MTEB) and outperforms all models up to its size on a range\nof generative tasks. By scaling up further, GritLM 8x7B outperforms all open\ngenerative language models that we tried while still being among the best\nembedding models. Notably, we find that GRIT matches training on only\ngenerative or embedding data, thus we can unify both at no performance loss.\nAmong other benefits, the unification via GRIT speeds up Retrieval-Augmented\nGeneration (RAG) by > 60% for long documents, by no longer requiring separate\nretrieval and generation models. Models, code, etc. are freely available at\nhttps://github.com/ContextualAI/gritlm.",
        "publication_date": "2024-02-15T12:12:19Z",
        "upvotes": 49
    },
    "2402.09727": {
        "url": "https://arxiv.org/abs/2402.09727",
        "title": "A Human-Inspired Reading Agent with Gist Memory of Very Long Contexts",
        "authors": [
            "Kuang-Huei Lee",
            "Xinyun Chen",
            "Hiroki Furuta",
            "John Canny",
            "Ian Fischer"
        ],
        "abstract": "Current Large Language Models (LLMs) are not only limited to some maximum\ncontext length, but also are not able to robustly consume long inputs. To\naddress these limitations, we propose ReadAgent, an LLM agent system that\nincreases effective context length up to 20x in our experiments. Inspired by\nhow humans interactively read long documents, we implement ReadAgent as a\nsimple prompting system that uses the advanced language capabilities of LLMs to\n(1) decide what content to store together in a memory episode, (2) compress\nthose memory episodes into short episodic memories called gist memories, and\n(3) take actions to look up passages in the original text if ReadAgent needs to\nremind itself of relevant details to complete a task. We evaluate ReadAgent\nagainst baselines using retrieval methods, using the original long contexts,\nand using the gist memories. These evaluations are performed on three\nlong-document reading comprehension tasks: QuALITY, NarrativeQA, and QMSum.\nReadAgent outperforms the baselines on all three tasks while extending the\neffective context window by 3-20x.",
        "publication_date": "2024-02-15T05:40:21Z",
        "upvotes": 35
    },
    "2402.09668": {
        "url": "https://arxiv.org/abs/2402.09668",
        "title": "How to Train Data-Efficient LLMs",
        "authors": [
            "Noveen Sachdeva",
            "Benjamin Coleman",
            "Wang-Cheng Kang",
            "Jianmo Ni",
            "Lichan Hong",
            "Ed H. Chi",
            "James Caverlee",
            "Julian McAuley",
            "Derek Zhiyuan Cheng"
        ],
        "abstract": "The training of large language models (LLMs) is expensive. In this paper, we\nstudy data-efficient approaches for pre-training LLMs, i.e., techniques that\naim to optimize the Pareto frontier of model quality and training resource/data\nconsumption. We seek to understand the tradeoffs associated with data selection\nroutines based on (i) expensive-to-compute data-quality estimates, and (ii)\nmaximization of coverage and diversity-based measures in the feature space. Our\nfirst technique, Ask-LLM, leverages the zero-shot reasoning capabilities of\ninstruction-tuned LLMs to directly assess the quality of a training example. To\ntarget coverage, we propose Density sampling, which models the data\ndistribution to select a diverse sample. In our comparison of 19 samplers,\ninvolving hundreds of evaluation tasks and pre-training runs, we find that\nAsk-LLM and Density are the best methods in their respective categories.\nCoverage sampling can recover the performance of the full data, while models\ntrained on Ask-LLM data consistently outperform full-data training -- even when\nwe reject 90% of the original dataset, while converging up to 70% faster.",
        "publication_date": "2024-02-15T02:27:57Z",
        "upvotes": 33
    },
    "2402.10176": {
        "url": "https://arxiv.org/abs/2402.10176",
        "title": "OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset",
        "authors": [
            "Shubham Toshniwal",
            "Ivan Moshkov",
            "Sean Narenthiran",
            "Daria Gitman",
            "Fei Jia",
            "Igor Gitman"
        ],
        "abstract": "Recent work has shown the immense potential of synthetically generated\ndatasets for training large language models (LLMs), especially for acquiring\ntargeted skills. Current large-scale math instruction tuning datasets such as\nMetaMathQA (Yu et al., 2024) and MAmmoTH (Yue et al., 2024) are constructed\nusing outputs from closed-source LLMs with commercially restrictive licenses. A\nkey reason limiting the use of open-source LLMs in these data generation\npipelines has been the wide gap between the mathematical skills of the best\nclosed-source LLMs, such as GPT-4, and the best open-source LLMs. Building on\nthe recent progress in open-source LLMs, our proposed prompting novelty, and\nsome brute-force scaling, we construct OpenMathInstruct-1, a math instruction\ntuning dataset with 1.8M problem-solution pairs. The dataset is constructed by\nsynthesizing code-interpreter solutions for GSM8K and MATH, two popular math\nreasoning benchmarks, using the recently released and permissively licensed\nMixtral model. Our best model, OpenMath-CodeLlama-70B, trained on a subset of\nOpenMathInstruct-1, achieves a score of 84.6% on GSM8K and 50.7% on MATH, which\nis competitive with the best gpt-distilled models. We release our code, models,\nand the OpenMathInstruct-1 dataset under a commercially permissive license.",
        "publication_date": "2024-02-15T18:26:11Z",
        "upvotes": 32
    },
    "2402.10210": {
        "url": "https://arxiv.org/abs/2402.10210",
        "title": "Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation",
        "authors": [
            "Huizhuo Yuan",
            "Zixiang Chen",
            "Kaixuan Ji",
            "Quanquan Gu"
        ],
        "abstract": "Fine-tuning Diffusion Models remains an underexplored frontier in generative\nartificial intelligence (GenAI), especially when compared with the remarkable\nprogress made in fine-tuning Large Language Models (LLMs). While cutting-edge\ndiffusion models such as Stable Diffusion (SD) and SDXL rely on supervised\nfine-tuning, their performance inevitably plateaus after seeing a certain\nvolume of data. Recently, reinforcement learning (RL) has been employed to\nfine-tune diffusion models with human preference data, but it requires at least\ntwo images (\"winner\" and \"loser\" images) for each text prompt. In this paper,\nwe introduce an innovative technique called self-play fine-tuning for diffusion\nmodels (SPIN-Diffusion), where the diffusion model engages in competition with\nits earlier versions, facilitating an iterative self-improvement process. Our\napproach offers an alternative to conventional supervised fine-tuning and RL\nstrategies, significantly improving both model performance and alignment. Our\nexperiments on the Pick-a-Pic dataset reveal that SPIN-Diffusion outperforms\nthe existing supervised fine-tuning method in aspects of human preference\nalignment and visual appeal right from its first iteration. By the second\niteration, it exceeds the performance of RLHF-based methods across all metrics,\nachieving these results with less data.",
        "publication_date": "2024-02-15T18:59:18Z",
        "upvotes": 26
    },
    "2402.10009": {
        "url": "https://arxiv.org/abs/2402.10009",
        "title": "Zero-Shot Unsupervised and Text-Based Audio Editing Using DDPM Inversion",
        "authors": [
            "Hila Manor",
            "Tomer Michaeli"
        ],
        "abstract": "Editing signals using large pre-trained models, in a zero-shot manner, has\nrecently seen rapid advancements in the image domain. However, this wave has\nyet to reach the audio domain. In this paper, we explore two zero-shot editing\ntechniques for audio signals, which use DDPM inversion on pre-trained diffusion\nmodels. The first, adopted from the image domain, allows text-based editing.\nThe second, is a novel approach for discovering semantically meaningful editing\ndirections without supervision. When applied to music signals, this method\nexposes a range of musically interesting modifications, from controlling the\nparticipation of specific instruments to improvisations on the melody. Samples\nand code can be found on our examples page in\nhttps://hilamanor.github.io/AudioEditing/ .",
        "publication_date": "2024-02-15T15:17:26Z",
        "upvotes": 18
    },
    "2402.10171": {
        "url": "https://arxiv.org/abs/2402.10171",
        "title": "Data Engineering for Scaling Language Models to 128K Context",
        "authors": [
            "Yao Fu",
            "Rameswar Panda",
            "Xinyao Niu",
            "Xiang Yue",
            "Hannaneh Hajishirzi",
            "Yoon Kim",
            "Hao Peng"
        ],
        "abstract": "We study the continual pretraining recipe for scaling language models'\ncontext lengths to 128K, with a focus on data engineering. We hypothesize that\nlong context modeling, in particular \\textit{the ability to utilize information\nat arbitrary input locations}, is a capability that is mostly already acquired\nthrough large-scale pretraining, and that this capability can be readily\nextended to contexts substantially longer than seen during training~(e.g., 4K\nto 128K) through lightweight continual pretraining on appropriate data mixture.\nWe investigate the \\textit{quantity} and \\textit{quality} of the data for\ncontinual pretraining: (1) for quantity, we show that 500 million to 5 billion\ntokens are enough to enable the model to retrieve information anywhere within\nthe 128K context; (2) for quality, our results equally emphasize \\textit{domain\nbalance} and \\textit{length upsampling}. Concretely, we find that naively\nupsampling longer data on certain domains like books, a common practice of\nexisting work, gives suboptimal performance, and that a balanced domain mixture\nis important. We demonstrate that continual pretraining of the full model on\n1B-5B tokens of such data is an effective and affordable strategy for scaling\nthe context length of language models to 128K. Our recipe outperforms strong\nopen-source long-context models and closes the gap to frontier models like\nGPT-4 128K.",
        "publication_date": "2024-02-15T18:19:16Z",
        "upvotes": 18
    },
    "2402.10193": {
        "url": "https://arxiv.org/abs/2402.10193",
        "title": "BitDelta: Your Fine-Tune May Only Be Worth One Bit",
        "authors": [
            "James Liu",
            "Guangxuan Xiao",
            "Kai Li",
            "Jason D. Lee",
            "Song Han",
            "Tri Dao",
            "Tianle Cai"
        ],
        "abstract": "Large Language Models (LLMs) are typically trained in two phases:\npre-training on large internet-scale datasets, and fine-tuning for downstream\ntasks. Given the higher computational demand of pre-training, it's intuitive to\nassume that fine-tuning adds less new information to the model, and is thus\nmore compressible. We explore this assumption by decomposing the weights of\nfine-tuned models into their pre-trained components and an additional delta. We\nintroduce a simple method, BitDelta, which successfully quantizes this delta\ndown to 1 bit without compromising performance. This interesting finding not\nonly highlights the potential redundancy of information added during\nfine-tuning, but also has significant implications for the multi-tenant serving\nand multi-tenant storage of fine-tuned models. By enabling the use of a single\nhigh-precision base model accompanied by multiple 1-bit deltas, BitDelta\ndramatically reduces GPU memory requirements by more than 10x, which can also\nbe translated to enhanced generation latency in multi-tenant settings. We\nvalidate BitDelta through experiments across Llama-2 and Mistral model\nfamilies, and on models up to 70B parameters, showcasing minimal performance\ndegradation over all tested settings.",
        "publication_date": "2024-02-15T18:50:06Z",
        "upvotes": 17
    },
    "2402.10128": {
        "url": "https://arxiv.org/abs/2402.10128",
        "title": "GES: Generalized Exponential Splatting for Efficient Radiance Field\n  Rendering",
        "authors": [
            "Abdullah Hamdi",
            "Luke Melas-Kyriazi",
            "Guocheng Qian",
            "Jinjie Mai",
            "Ruoshi Liu",
            "Carl Vondrick",
            "Bernard Ghanem",
            "Andrea Vedaldi"
        ],
        "abstract": "Advancements in 3D Gaussian Splatting have significantly accelerated 3D\nreconstruction and generation. However, it may require a large number of\nGaussians, which creates a substantial memory footprint. This paper introduces\nGES (Generalized Exponential Splatting), a novel representation that employs\nGeneralized Exponential Function (GEF) to model 3D scenes, requiring far fewer\nparticles to represent a scene and thus significantly outperforming Gaussian\nSplatting methods in efficiency with a plug-and-play replacement ability for\nGaussian-based utilities. GES is validated theoretically and empirically in\nboth principled 1D setup and realistic 3D scenes.\n  It is shown to represent signals with sharp edges more accurately, which are\ntypically challenging for Gaussians due to their inherent low-pass\ncharacteristics. Our empirical analysis demonstrates that GEF outperforms\nGaussians in fitting natural-occurring signals (e.g. squares, triangles, and\nparabolic signals), thereby reducing the need for extensive splitting\noperations that increase the memory footprint of Gaussian Splatting. With the\naid of a frequency-modulated loss, GES achieves competitive performance in\nnovel-view synthesis benchmarks while requiring less than half the memory\nstorage of Gaussian Splatting and increasing the rendering speed by up to 39%.\nThe code is available on the project website https://abdullahamdi.com/ges .",
        "publication_date": "2024-02-15T17:32:50Z",
        "upvotes": 14
    },
    "2402.09812": {
        "url": "https://arxiv.org/abs/2402.09812",
        "title": "DreamMatcher: Appearance Matching Self-Attention for\n  Semantically-Consistent Text-to-Image Personalization",
        "authors": [
            "Jisu Nam",
            "Heesu Kim",
            "DongJae Lee",
            "Siyoon Jin",
            "Seungryong Kim",
            "Seunggyu Chang"
        ],
        "abstract": "The objective of text-to-image (T2I) personalization is to customize a\ndiffusion model to a user-provided reference concept, generating diverse images\nof the concept aligned with the target prompts. Conventional methods\nrepresenting the reference concepts using unique text embeddings often fail to\naccurately mimic the appearance of the reference. To address this, one solution\nmay be explicitly conditioning the reference images into the target denoising\nprocess, known as key-value replacement. However, prior works are constrained\nto local editing since they disrupt the structure path of the pre-trained T2I\nmodel. To overcome this, we propose a novel plug-in method, called\nDreamMatcher, which reformulates T2I personalization as semantic matching.\nSpecifically, DreamMatcher replaces the target values with reference values\naligned by semantic matching, while leaving the structure path unchanged to\npreserve the versatile capability of pre-trained T2I models for generating\ndiverse structures. We also introduce a semantic-consistent masking strategy to\nisolate the personalized concept from irrelevant regions introduced by the\ntarget prompts. Compatible with existing T2I models, DreamMatcher shows\nsignificant improvements in complex scenarios. Intensive analyses demonstrate\nthe effectiveness of our approach.",
        "publication_date": "2024-02-15T09:21:16Z",
        "upvotes": 11
    },
    "2402.10211": {
        "url": "https://arxiv.org/abs/2402.10211",
        "title": "Hierarchical State Space Models for Continuous Sequence-to-Sequence\n  Modeling",
        "authors": [
            "Raunaq Bhirangi",
            "Chenyu Wang",
            "Venkatesh Pattabiraman",
            "Carmel Majidi",
            "Abhinav Gupta",
            "Tess Hellebrekers",
            "Lerrel Pinto"
        ],
        "abstract": "Reasoning from sequences of raw sensory data is a ubiquitous problem across\nfields ranging from medical devices to robotics. These problems often involve\nusing long sequences of raw sensor data (e.g. magnetometers, piezoresistors) to\npredict sequences of desirable physical quantities (e.g. force, inertial\nmeasurements). While classical approaches are powerful for locally-linear\nprediction problems, they often fall short when using real-world sensors. These\nsensors are typically non-linear, are affected by extraneous variables (e.g.\nvibration), and exhibit data-dependent drift. For many problems, the prediction\ntask is exacerbated by small labeled datasets since obtaining ground-truth\nlabels requires expensive equipment. In this work, we present Hierarchical\nState-Space Models (HiSS), a conceptually simple, new technique for continuous\nsequential prediction. HiSS stacks structured state-space models on top of each\nother to create a temporal hierarchy. Across six real-world sensor datasets,\nfrom tactile-based state prediction to accelerometer-based inertial\nmeasurement, HiSS outperforms state-of-the-art sequence models such as causal\nTransformers, LSTMs, S4, and Mamba by at least 23% on MSE. Our experiments\nfurther indicate that HiSS demonstrates efficient scaling to smaller datasets\nand is compatible with existing data-filtering techniques. Code, datasets and\nvideos can be found on https://hiss-csp.github.io.",
        "publication_date": "2024-02-15T18:59:43Z",
        "upvotes": 8
    },
    "2402.09470": {
        "url": "https://arxiv.org/abs/2402.09470",
        "title": "Rolling Diffusion Models",
        "authors": [
            "David Ruhe",
            "Jonathan Heek",
            "Tim Salimans",
            "Emiel Hoogeboom"
        ],
        "abstract": "Diffusion models have recently been increasingly applied to temporal data\nsuch as video, fluid mechanics simulations, or climate data. These methods\ngenerally treat subsequent frames equally regarding the amount of noise in the\ndiffusion process. This paper explores Rolling Diffusion: a new approach that\nuses a sliding window denoising process. It ensures that the diffusion process\nprogressively corrupts through time by assigning more noise to frames that\nappear later in a sequence, reflecting greater uncertainty about the future as\nthe generation process unfolds. Empirically, we show that when the temporal\ndynamics are complex, Rolling Diffusion is superior to standard diffusion. In\nparticular, this result is demonstrated in a video prediction task using the\nKinetics-600 video dataset and in a chaotic fluid dynamics forecasting\nexperiment.",
        "publication_date": "2024-02-12T08:16:10Z",
        "upvotes": 8
    },
    "2402.10644": {
        "url": "https://arxiv.org/abs/2402.10644",
        "title": "Linear Transformers with Learnable Kernel Functions are Better\n  In-Context Models",
        "authors": [
            "Yaroslav Aksenov",
            "Nikita Balagansky",
            "Sofia Maria Lo Cicero Vaina",
            "Boris Shaposhnikov",
            "Alexey Gorbatovski",
            "Daniil Gavrilov"
        ],
        "abstract": "Advancing the frontier of subquadratic architectures for Language Models\n(LMs) is crucial in the rapidly evolving field of natural language processing.\nCurrent innovations, including State Space Models, were initially celebrated\nfor surpassing Transformer performance on language modeling tasks. However,\nthese models have revealed deficiencies in essential In-Context Learning\ncapabilities - a domain where the Transformer traditionally shines. The Based\nmodel emerged as a hybrid solution, blending a Linear Transformer with a kernel\ninspired by the Taylor expansion of exponential functions, augmented by\nconvolutional networks. Mirroring the Transformer's in-context adeptness, it\nbecame a strong contender in the field. In our work, we present a singular,\nelegant alteration to the Based kernel that amplifies its In-Context Learning\nabilities evaluated with the Multi-Query Associative Recall task and overall\nlanguage modeling process, as demonstrated on the Pile dataset.",
        "publication_date": "2024-02-16T12:44:15Z",
        "upvotes": 71
    },
    "2402.10790": {
        "url": "https://arxiv.org/abs/2402.10790",
        "title": "In Search of Needles in a 11M Haystack: Recurrent Memory Finds What LLMs\n  Miss",
        "authors": [
            "Yuri Kuratov",
            "Aydar Bulatov",
            "Petr Anokhin",
            "Dmitry Sorokin",
            "Artyom Sorokin",
            "Mikhail Burtsev"
        ],
        "abstract": "This paper addresses the challenge of processing long documents using\ngenerative transformer models. To evaluate different approaches, we introduce\nBABILong, a new benchmark designed to assess model capabilities in extracting\nand processing distributed facts within extensive texts. Our evaluation, which\nincludes benchmarks for GPT-4 and RAG, reveals that common methods are\neffective only for sequences up to $10^4$ elements. In contrast, fine-tuning\nGPT-2 with recurrent memory augmentations enables it to handle tasks involving\nup to $11\\times 10^6$ elements. This achievement marks a substantial leap, as\nit is by far the longest input processed by any neural network model to date,\ndemonstrating a significant improvement in the processing capabilities for long\nsequences.",
        "publication_date": "2024-02-16T16:15:01Z",
        "upvotes": 39
    },
    "2402.10555": {
        "url": "https://arxiv.org/abs/2402.10555",
        "title": "SPAR: Personalized Content-Based Recommendation via Long Engagement\n  Attention",
        "authors": [
            "Chiyu Zhang",
            "Yifei Sun",
            "Jun Chen",
            "Jie Lei",
            "Muhammad Abdul-Mageed",
            "Sinong Wang",
            "Rong Jin",
            "Sem Park",
            "Ning Yao",
            "Bo Long"
        ],
        "abstract": "Leveraging users' long engagement histories is essential for personalized\ncontent recommendations. The success of pretrained language models (PLMs) in\nNLP has led to their use in encoding user histories and candidate items,\nframing content recommendations as textual semantic matching tasks. However,\nexisting works still struggle with processing very long user historical text\nand insufficient user-item interaction. In this paper, we introduce a\ncontent-based recommendation framework, SPAR, which effectively tackles the\nchallenges of holistic user interest extraction from the long user engagement\nhistory. It achieves so by leveraging PLM, poly-attention layers and attention\nsparsity mechanisms to encode user's history in a session-based manner. The\nuser and item side features are sufficiently fused for engagement prediction\nwhile maintaining standalone representations for both sides, which is efficient\nfor practical model deployment. Moreover, we enhance user profiling by\nexploiting large language model (LLM) to extract global interests from user\nengagement history. Extensive experiments on two benchmark datasets demonstrate\nthat our framework outperforms existing state-of-the-art (SoTA) methods.",
        "publication_date": "2024-02-16T10:36:38Z",
        "upvotes": 31
    },
    "2402.10379": {
        "url": "https://arxiv.org/abs/2402.10379",
        "title": "DataDreamer: A Tool for Synthetic Data Generation and Reproducible LLM\n  Workflows",
        "authors": [
            "Ajay Patel",
            "Colin Raffel",
            "Chris Callison-Burch"
        ],
        "abstract": "Large language models (LLMs) have become a dominant and important tool for\nNLP researchers in a wide range of tasks. Today, many researchers use LLMs in\nsynthetic data generation, task evaluation, fine-tuning, distillation, and\nother model-in-the-loop research workflows. However, challenges arise when\nusing these models that stem from their scale, their closed source nature, and\nthe lack of standardized tooling for these new and emerging workflows. The\nrapid rise to prominence of these models and these unique challenges has had\nimmediate adverse impacts on open science and on the reproducibility of work\nthat uses them. In this paper, we introduce DataDreamer, an open source Python\nlibrary that allows researchers to write simple code to implement powerful LLM\nworkflows. DataDreamer also helps researchers adhere to best practices that we\npropose to encourage open science and reproducibility. The library and\ndocumentation are available at https://github.com/datadreamer-dev/DataDreamer .",
        "publication_date": "2024-02-16T00:10:26Z",
        "upvotes": 27
    },
    "2402.10294": {
        "url": "https://arxiv.org/abs/2402.10294",
        "title": "LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video\n  Editing",
        "authors": [
            "Bryan Wang",
            "Yuliang Li",
            "Zhaoyang Lv",
            "Haijun Xia",
            "Yan Xu",
            "Raj Sodhi"
        ],
        "abstract": "Video creation has become increasingly popular, yet the expertise and effort\nrequired for editing often pose barriers to beginners. In this paper, we\nexplore the integration of large language models (LLMs) into the video editing\nworkflow to reduce these barriers. Our design vision is embodied in LAVE, a\nnovel system that provides LLM-powered agent assistance and language-augmented\nediting features. LAVE automatically generates language descriptions for the\nuser's footage, serving as the foundation for enabling the LLM to process\nvideos and assist in editing tasks. When the user provides editing objectives,\nthe agent plans and executes relevant actions to fulfill them. Moreover, LAVE\nallows users to edit videos through either the agent or direct UI manipulation,\nproviding flexibility and enabling manual refinement of agent actions. Our user\nstudy, which included eight participants ranging from novices to proficient\neditors, demonstrated LAVE's effectiveness. The results also shed light on user\nperceptions of the proposed LLM-assisted editing paradigm and its impact on\nusers' creativity and sense of co-creation. Based on these findings, we propose\ndesign implications to inform the future development of agent-assisted content\nediting.",
        "publication_date": "2024-02-15T19:53:11Z",
        "upvotes": 19
    },
    "2402.10524": {
        "url": "https://arxiv.org/abs/2402.10524",
        "title": "LLM Comparator: Visual Analytics for Side-by-Side Evaluation of Large\n  Language Models",
        "authors": [
            "Minsuk Kahng",
            "Ian Tenney",
            "Mahima Pushkarna",
            "Michael Xieyang Liu",
            "James Wexler",
            "Emily Reif",
            "Krystal Kallarackal",
            "Minsuk Chang",
            "Michael Terry",
            "Lucas Dixon"
        ],
        "abstract": "Automatic side-by-side evaluation has emerged as a promising approach to\nevaluating the quality of responses from large language models (LLMs). However,\nanalyzing the results from this evaluation approach raises scalability and\ninterpretability challenges. In this paper, we present LLM Comparator, a novel\nvisual analytics tool for interactively analyzing results from automatic\nside-by-side evaluation. The tool supports interactive workflows for users to\nunderstand when and why a model performs better or worse than a baseline model,\nand how the responses from two models are qualitatively different. We\niteratively designed and developed the tool by closely working with researchers\nand engineers at a large technology company. This paper details the user\nchallenges we identified, the design and development of the tool, and an\nobservational study with participants who regularly evaluate their models.",
        "publication_date": "2024-02-16T09:14:49Z",
        "upvotes": 18
    },
    "2402.10466": {
        "url": "https://arxiv.org/abs/2402.10466",
        "title": "Large Language Models as Zero-shot Dialogue State Tracker through\n  Function Calling",
        "authors": [
            "Zekun Li",
            "Zhiyu Zoey Chen",
            "Mike Ross",
            "Patrick Huber",
            "Seungwhan Moon",
            "Zhaojiang Lin",
            "Xin Luna Dong",
            "Adithya Sagar",
            "Xifeng Yan",
            "Paul A. Crook"
        ],
        "abstract": "Large language models (LLMs) are increasingly prevalent in conversational\nsystems due to their advanced understanding and generative capabilities in\ngeneral contexts. However, their effectiveness in task-oriented dialogues\n(TOD), which requires not only response generation but also effective dialogue\nstate tracking (DST) within specific tasks and domains, remains less\nsatisfying. In this work, we propose a novel approach FnCTOD for solving DST\nwith LLMs through function calling. This method improves zero-shot DST,\nallowing adaptation to diverse domains without extensive data collection or\nmodel tuning. Our experimental results demonstrate that our approach achieves\nexceptional performance with both modestly sized open-source and also\nproprietary LLMs: with in-context prompting it enables various 7B or 13B\nparameter models to surpass the previous state-of-the-art (SOTA) achieved by\nChatGPT, and improves ChatGPT's performance beating the SOTA by 5.6% Avg. JGA.\nIndividual model results for GPT-3.5 and GPT-4 are boosted by 4.8% and 14%,\nrespectively. We also show that by fine-tuning on a small collection of diverse\ntask-oriented dialogues, we can equip modestly sized models, specifically a 13B\nparameter LLaMA2-Chat model, with function-calling capabilities and DST\nperformance comparable to ChatGPT while maintaining their chat capabilities. We\nplan to open-source experimental code and model.",
        "publication_date": "2024-02-16T06:13:18Z",
        "upvotes": 16
    },
    "2402.10491": {
        "url": "https://arxiv.org/abs/2402.10491",
        "title": "Make a Cheap Scaling: A Self-Cascade Diffusion Model for\n  Higher-Resolution Adaptation",
        "authors": [
            "Lanqing Guo",
            "Yingqing He",
            "Haoxin Chen",
            "Menghan Xia",
            "Xiaodong Cun",
            "Yufei Wang",
            "Siyu Huang",
            "Yong Zhang",
            "Xintao Wang",
            "Qifeng Chen",
            "Ying Shan",
            "Bihan Wen"
        ],
        "abstract": "Diffusion models have proven to be highly effective in image and video\ngeneration; however, they still face composition challenges when generating\nimages of varying sizes due to single-scale training data. Adapting large\npre-trained diffusion models for higher resolution demands substantial\ncomputational and optimization resources, yet achieving a generation capability\ncomparable to low-resolution models remains elusive. This paper proposes a\nnovel self-cascade diffusion model that leverages the rich knowledge gained\nfrom a well-trained low-resolution model for rapid adaptation to\nhigher-resolution image and video generation, employing either tuning-free or\ncheap upsampler tuning paradigms. Integrating a sequence of multi-scale\nupsampler modules, the self-cascade diffusion model can efficiently adapt to a\nhigher resolution, preserving the original composition and generation\ncapabilities. We further propose a pivot-guided noise re-schedule strategy to\nspeed up the inference process and improve local structural details. Compared\nto full fine-tuning, our approach achieves a 5X training speed-up and requires\nonly an additional 0.002M tuning parameters. Extensive experiments demonstrate\nthat our approach can quickly adapt to higher resolution image and video\nsynthesis by fine-tuning for just 10k steps, with virtually no additional\ninference time.",
        "publication_date": "2024-02-16T07:48:35Z",
        "upvotes": 15
    },
    "2402.10329": {
        "url": "https://arxiv.org/abs/2402.10329",
        "title": "Universal Manipulation Interface: In-The-Wild Robot Teaching Without\n  In-The-Wild Robots",
        "authors": [
            "Cheng Chi",
            "Zhenjia Xu",
            "Chuer Pan",
            "Eric Cousineau",
            "Benjamin Burchfiel",
            "Siyuan Feng",
            "Russ Tedrake",
            "Shuran Song"
        ],
        "abstract": "We present Universal Manipulation Interface (UMI) -- a data collection and\npolicy learning framework that allows direct skill transfer from in-the-wild\nhuman demonstrations to deployable robot policies. UMI employs hand-held\ngrippers coupled with careful interface design to enable portable, low-cost,\nand information-rich data collection for challenging bimanual and dynamic\nmanipulation demonstrations. To facilitate deployable policy learning, UMI\nincorporates a carefully designed policy interface with inference-time latency\nmatching and a relative-trajectory action representation. The resulting learned\npolicies are hardware-agnostic and deployable across multiple robot platforms.\nEquipped with these features, UMI framework unlocks new robot manipulation\ncapabilities, allowing zero-shot generalizable dynamic, bimanual, precise, and\nlong-horizon behaviors, by only changing the training data for each task. We\ndemonstrate UMI's versatility and efficacy with comprehensive real-world\nexperiments, where policies learned via UMI zero-shot generalize to novel\nenvironments and objects when trained on diverse human demonstrations. UMI's\nhardware and software system is open-sourced at https://umi-gripper.github.io.",
        "publication_date": "2024-02-15T21:11:50Z",
        "upvotes": 13
    },
    "2402.10259": {
        "url": "https://arxiv.org/abs/2402.10259",
        "title": "GaussianObject: Just Taking Four Images to Get A High-Quality 3D Object\n  with Gaussian Splatting",
        "authors": [
            "Chen Yang",
            "Sikuang Li",
            "Jiemin Fang",
            "Ruofan Liang",
            "Lingxi Xie",
            "Xiaopeng Zhang",
            "Wei Shen",
            "Qi Tian"
        ],
        "abstract": "Reconstructing and rendering 3D objects from highly sparse views is of\ncritical importance for promoting applications of 3D vision techniques and\nimproving user experience. However, images from sparse views only contain very\nlimited 3D information, leading to two significant challenges: 1) Difficulty in\nbuilding multi-view consistency as images for matching are too few; 2)\nPartially omitted or highly compressed object information as view coverage is\ninsufficient. To tackle these challenges, we propose GaussianObject, a\nframework to represent and render the 3D object with Gaussian splatting, that\nachieves high rendering quality with only 4 input images. We first introduce\ntechniques of visual hull and floater elimination which explicitly inject\nstructure priors into the initial optimization process for helping build\nmulti-view consistency, yielding a coarse 3D Gaussian representation. Then we\nconstruct a Gaussian repair model based on diffusion models to supplement the\nomitted object information, where Gaussians are further refined. We design a\nself-generating strategy to obtain image pairs for training the repair model.\nOur GaussianObject is evaluated on several challenging datasets, including\nMipNeRF360, OmniObject3D, and OpenIllumination, achieving strong reconstruction\nresults from only 4 views and significantly outperforming previous\nstate-of-the-art methods.",
        "publication_date": "2024-02-15T18:42:33Z",
        "upvotes": 13
    },
    "2402.10896": {
        "url": "https://arxiv.org/abs/2402.10896",
        "title": "PaLM2-VAdapter: Progressively Aligned Language Model Makes a Strong\n  Vision-language Adapter",
        "authors": [
            "Junfei Xiao",
            "Zheng Xu",
            "Alan Yuille",
            "Shen Yan",
            "Boyu Wang"
        ],
        "abstract": "This paper demonstrates that a progressively aligned language model can\neffectively bridge frozen vision encoders and large language models (LLMs).\nWhile the fundamental architecture and pre-training methods of vision encoders\nand LLMs have been extensively studied, the architecture and training strategy\nof vision-language adapters vary significantly across recent works. Our\nresearch undertakes a thorough exploration of the state-of-the-art perceiver\nresampler architecture and builds a strong baseline. However, we observe that\nthe vision-language alignment with perceiver resampler exhibits slow\nconvergence and limited scalability with a lack of direct supervision. To\naddress this issue, we propose PaLM2-VAdapter, employing a progressively\naligned language model as the vision-language adapter. Compared to the strong\nbaseline with perceiver resampler, our method empirically shows faster\nconvergence, higher performance, and stronger scalability. Extensive\nexperiments across various Visual Question Answering (VQA) and captioning tasks\non both images and videos demonstrate that our model exhibits state-of-the-art\nvisual understanding and multi-modal reasoning capabilities. Notably, our\nmethod achieves these advancements with 30~70% fewer parameters than the\nstate-of-the-art large vision-language models, marking a significant efficiency\nimprovement.",
        "publication_date": "2024-02-16T18:54:47Z",
        "upvotes": 13
    },
    "2402.10893": {
        "url": "https://arxiv.org/abs/2402.10893",
        "title": "RLVF: Learning from Verbal Feedback without Overgeneralization",
        "authors": [
            "Moritz Stephan",
            "Alexander Khazatsky",
            "Eric Mitchell",
            "Annie S Chen",
            "Sheryl Hsu",
            "Archit Sharma",
            "Chelsea Finn"
        ],
        "abstract": "The diversity of contexts in which large language models (LLMs) are deployed\nrequires the ability to modify or customize default model behaviors to\nincorporate nuanced requirements and preferences. A convenient interface to\nspecify such model adjustments is high-level verbal feedback, such as \"Don't\nuse emojis when drafting emails to my boss.\" However, while writing high-level\nfeedback is far simpler than collecting annotations for reinforcement learning\nfrom human feedback (RLHF), we find that simply prompting a model with such\nfeedback leads to overgeneralization of the feedback to contexts where it is\nnot relevant. We study the problem of incorporating verbal feedback without\nsuch overgeneralization, inspiring a new method Contextualized Critiques with\nConstrained Preference Optimization (C3PO). C3PO uses a piece of high-level\nfeedback to generate a small synthetic preference dataset specifying how the\nfeedback should (and should not) be applied. It then fine-tunes the model in\naccordance with the synthetic preference data while minimizing the divergence\nfrom the original model for prompts where the feedback does not apply. Our\nexperimental results indicate that our approach effectively applies verbal\nfeedback to relevant scenarios while preserving existing behaviors for other\ncontexts. For both human- and GPT-4-generated high-level feedback, C3PO\neffectively adheres to the given feedback comparably to in-context baselines\nwhile reducing overgeneralization by 30%.",
        "publication_date": "2024-02-16T18:50:24Z",
        "upvotes": 10
    },
    "2402.10986": {
        "url": "https://arxiv.org/abs/2402.10986",
        "title": "FinTral: A Family of GPT-4 Level Multimodal Financial Large Language\n  Models",
        "authors": [
            "Gagan Bhatia",
            "El Moatez Billah Nagoudi",
            "Hasan Cavusoglu",
            "Muhammad Abdul-Mageed"
        ],
        "abstract": "We introduce FinTral, a suite of state-of-the-art multimodal large language\nmodels (LLMs) built upon the Mistral-7b model and tailored for financial\nanalysis. FinTral integrates textual, numerical, tabular, and image data. We\nenhance FinTral with domain-specific pretraining, instruction fine-tuning, and\nRLAIF training by exploiting a large collection of textual and visual datasets\nwe curate for this work. We also introduce an extensive benchmark featuring\nnine tasks and 25 datasets for evaluation, including hallucinations in the\nfinancial domain. Our FinTral model trained with direct preference optimization\nemploying advanced Tools and Retrieval methods, dubbed FinTral-DPO-T&R,\ndemonstrates an exceptional zero-shot performance. It outperforms ChatGPT-3.5\nin all tasks and surpasses GPT-4 in five out of nine tasks, marking a\nsignificant advancement in AI-driven financial technology. We also demonstrate\nthat FinTral has the potential to excel in real-time analysis and\ndecision-making in diverse financial contexts.",
        "publication_date": "2024-02-16T05:05:12Z",
        "upvotes": 72
    },
    "2402.12376": {
        "url": "https://arxiv.org/abs/2402.12376",
        "title": "FiT: Flexible Vision Transformer for Diffusion Model",
        "authors": [
            "Zeyu Lu",
            "Zidong Wang",
            "Di Huang",
            "Chengyue Wu",
            "Xihui Liu",
            "Wanli Ouyang",
            "Lei Bai"
        ],
        "abstract": "Nature is infinitely resolution-free. In the context of this reality,\nexisting diffusion models, such as Diffusion Transformers, often face\nchallenges when processing image resolutions outside of their trained domain.\nTo overcome this limitation, we present the Flexible Vision Transformer (FiT),\na transformer architecture specifically designed for generating images with\nunrestricted resolutions and aspect ratios. Unlike traditional methods that\nperceive images as static-resolution grids, FiT conceptualizes images as\nsequences of dynamically-sized tokens. This perspective enables a flexible\ntraining strategy that effortlessly adapts to diverse aspect ratios during both\ntraining and inference phases, thus promoting resolution generalization and\neliminating biases induced by image cropping. Enhanced by a meticulously\nadjusted network structure and the integration of training-free extrapolation\ntechniques, FiT exhibits remarkable flexibility in resolution extrapolation\ngeneration. Comprehensive experiments demonstrate the exceptional performance\nof FiT across a broad range of resolutions, showcasing its effectiveness both\nwithin and beyond its training resolution distribution. Repository available at\nhttps://github.com/whlzy/FiT.",
        "publication_date": "2024-02-19T18:59:07Z",
        "upvotes": 45
    },
    "2402.11131": {
        "url": "https://arxiv.org/abs/2402.11131",
        "title": "Speculative Streaming: Fast LLM Inference without Auxiliary Models",
        "authors": [
            "Nikhil Bhendawade",
            "Irina Belousova",
            "Qichen Fu",
            "Henry Mason",
            "Mohammad Rastegari",
            "Mahyar Najibi"
        ],
        "abstract": "Speculative decoding is a prominent technique to speed up the inference of a\nlarge target language model based on predictions of an auxiliary draft model.\nWhile effective, in application-specific settings, it often involves\nfine-tuning both draft and target models to achieve high acceptance rates. As\nthe number of downstream tasks grows, these draft models add significant\ncomplexity to inference systems. We propose Speculative Streaming, a\nsingle-model speculative decoding method that fuses drafting into the target\nmodel by changing the fine-tuning objective from next token prediction to\nfuture n-gram prediction. Speculative Streaming speeds up decoding by 1.8 -\n3.1X in a diverse set of tasks, such as Summarization, Structured Queries, and\nMeaning Representation, without sacrificing generation quality. Additionally,\nSpeculative Streaming is parameter-efficient. It achieves on-par/higher\nspeed-ups than Medusa-style architectures while using ~10000X fewer extra\nparameters, making it well-suited for resource-constrained devices.",
        "publication_date": "2024-02-16T23:36:43Z",
        "upvotes": 41
    },
    "2402.12226": {
        "url": "https://arxiv.org/abs/2402.12226",
        "title": "AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling",
        "authors": [
            "Jun Zhan",
            "Junqi Dai",
            "Jiasheng Ye",
            "Yunhua Zhou",
            "Dong Zhang",
            "Zhigeng Liu",
            "Xin Zhang",
            "Ruibin Yuan",
            "Ge Zhang",
            "Linyang Li",
            "Hang Yan",
            "Jie Fu",
            "Tao Gui",
            "Tianxiang Sun",
            "Yugang Jiang",
            "Xipeng Qiu"
        ],
        "abstract": "We introduce AnyGPT, an any-to-any multimodal language model that utilizes\ndiscrete representations for the unified processing of various modalities,\nincluding speech, text, images, and music. AnyGPT can be trained stably without\nany alterations to the current large language model (LLM) architecture or\ntraining paradigms. Instead, it relies exclusively on data-level preprocessing,\nfacilitating the seamless integration of new modalities into LLMs, akin to the\nincorporation of new languages. We build a multimodal text-centric dataset for\nmultimodal alignment pre-training. Utilizing generative models, we synthesize\nthe first large-scale any-to-any multimodal instruction dataset. It consists of\n108k samples of multi-turn conversations that intricately interweave various\nmodalities, thus equipping the model to handle arbitrary combinations of\nmultimodal inputs and outputs. Experimental results demonstrate that AnyGPT is\ncapable of facilitating any-to-any multimodal conversation while achieving\nperformance comparable to specialized models across all modalities, proving\nthat discrete representations can effectively and conveniently unify multiple\nmodalities within a language model. Demos are shown in\nhttps://junzhan2000.github.io/AnyGPT.github.io/",
        "publication_date": "2024-02-19T15:33:10Z",
        "upvotes": 37
    },
    "2402.11450": {
        "url": "https://arxiv.org/abs/2402.11450",
        "title": "Learning to Learn Faster from Human Feedback with Language Model\n  Predictive Control",
        "authors": [
            "Jacky Liang",
            "Fei Xia",
            "Wenhao Yu",
            "Andy Zeng",
            "Montserrat Gonzalez Arenas",
            "Maria Attarian",
            "Maria Bauza",
            "Matthew Bennice",
            "Alex Bewley",
            "Adil Dostmohamed",
            "Chuyuan Kelly Fu",
            "Nimrod Gileadi",
            "Marissa Giustina",
            "Keerthana Gopalakrishnan",
            "Leonard Hasenclever",
            "Jan Humplik",
            "Jasmine Hsu",
            "Nikhil Joshi",
            "Ben Jyenis",
            "Chase Kew",
            "Sean Kirmani",
            "Tsang-Wei Edward Lee",
            "Kuang-Huei Lee",
            "Assaf Hurwitz Michaely",
            "Joss Moore",
            "Ken Oslund",
            "Dushyant Rao",
            "Allen Ren",
            "Baruch Tabanpour",
            "Quan Vuong",
            "Ayzaan Wahid",
            "Ted Xiao",
            "Ying Xu",
            "Vincent Zhuang",
            "Peng Xu",
            "Erik Frey",
            "Ken Caluwaerts",
            "Tingnan Zhang",
            "Brian Ichter",
            "Jonathan Tompson",
            "Leila Takayama",
            "Vincent Vanhoucke",
            "Izhak Shafran",
            "Maja Mataric",
            "Dorsa Sadigh",
            "Nicolas Heess",
            "Kanishka Rao",
            "Nik Stewart",
            "Jie Tan",
            "Carolina Parada"
        ],
        "abstract": "Large language models (LLMs) have been shown to exhibit a wide range of\ncapabilities, such as writing robot code from language commands -- enabling\nnon-experts to direct robot behaviors, modify them based on feedback, or\ncompose them to perform new tasks. However, these capabilities (driven by\nin-context learning) are limited to short-term interactions, where users'\nfeedback remains relevant for only as long as it fits within the context size\nof the LLM, and can be forgotten over longer interactions. In this work, we\ninvestigate fine-tuning the robot code-writing LLMs, to remember their\nin-context interactions and improve their teachability i.e., how efficiently\nthey adapt to human inputs (measured by average number of corrections before\nthe user considers the task successful). Our key observation is that when\nhuman-robot interactions are formulated as a partially observable Markov\ndecision process (in which human language inputs are observations, and robot\ncode outputs are actions), then training an LLM to complete previous\ninteractions can be viewed as training a transition dynamics model -- that can\nbe combined with classic robotics techniques such as model predictive control\n(MPC) to discover shorter paths to success. This gives rise to Language Model\nPredictive Control (LMPC), a framework that fine-tunes PaLM 2 to improve its\nteachability on 78 tasks across 5 robot embodiments -- improving non-expert\nteaching success rates of unseen tasks by 26.9% while reducing the average\nnumber of human corrections from 2.4 to 1.9. Experiments show that LMPC also\nproduces strong meta-learners, improving the success rate of in-context\nlearning new tasks on unseen robot embodiments and APIs by 31.5%. See videos,\ncode, and demos at: https://robot-teaching.github.io/.",
        "publication_date": "2024-02-18T04:16:24Z",
        "upvotes": 20
    },
    "2402.11295": {
        "url": "https://arxiv.org/abs/2402.11295",
        "title": "OneBit: Towards Extremely Low-bit Large Language Models",
        "authors": [
            "Yuzhuang Xu",
            "Xu Han",
            "Zonghan Yang",
            "Shuo Wang",
            "Qingfu Zhu",
            "Zhiyuan Liu",
            "Weidong Liu",
            "Wanxiang Che"
        ],
        "abstract": "Model quantification uses low bit-width values to represent the weight\nmatrices of models, which is a promising approach to reduce both storage and\ncomputational overheads of deploying highly anticipated LLMs. However, existing\nquantization methods suffer severe performance degradation when the bit-width\nis extremely reduced, and thus focus on utilizing 4-bit or 8-bit values to\nquantize models. This paper boldly quantizes the weight matrices of LLMs to\n1-bit, paving the way for the extremely low bit-width deployment of LLMs. For\nthis target, we introduce a 1-bit quantization-aware training (QAT) framework\nnamed OneBit, including a novel 1-bit parameter representation method to better\nquantize LLMs as well as an effective parameter initialization method based on\nmatrix decomposition to improve the convergence speed of the QAT framework.\nSufficient experimental results indicate that OneBit achieves good performance\n(at least 83% of the non-quantized performance) with robust training processes\nwhen only using 1-bit weight matrices.",
        "publication_date": "2024-02-17T14:26:57Z",
        "upvotes": 19
    },
    "2402.12219": {
        "url": "https://arxiv.org/abs/2402.12219",
        "title": "Reformatted Alignment",
        "authors": [
            "Run-Ze Fan",
            "Xuefeng Li",
            "Haoyang Zou",
            "Junlong Li",
            "Shwai He",
            "Ethan Chern",
            "Jiewen Hu",
            "Pengfei Liu"
        ],
        "abstract": "The quality of finetuning data is crucial for aligning large language models\n(LLMs) with human values. Current methods to improve data quality are either\nlabor-intensive or prone to factual errors caused by LLM hallucinations. This\npaper explores elevating the quality of existing instruction data to better\nalign with human values, introducing a simple and effective approach named\nReAlign, which reformats the responses of instruction data into a format that\nbetter aligns with pre-established criteria and the collated evidence. This\napproach minimizes human annotation, hallucination, and the difficulty in\nscaling, remaining orthogonal to existing alignment techniques. Experimentally,\nReAlign significantly boosts the general alignment ability, math reasoning,\nfactuality, and readability of the LLMs.\n  Encouragingly, without introducing any additional data or advanced training\ntechniques, and merely by reformatting the response, LLaMA-2-13B's mathematical\nreasoning ability on GSM8K can be improved from 46.77% to 56.63% in accuracy.\nAdditionally, a mere 5% of ReAlign data yields a 67% boost in general alignment\nability measured by the Alpaca dataset. This work highlights the need for\nfurther research into the science and mechanistic interpretability of LLMs. We\nhave made the associated code and data publicly accessible to support future\nstudies at https://github.com/GAIR-NLP/ReAlign.",
        "publication_date": "2024-02-19T15:21:58Z",
        "upvotes": 15
    },
    "2402.11248": {
        "url": "https://arxiv.org/abs/2402.11248",
        "title": "CoLLaVO: Crayon Large Language and Vision mOdel",
        "authors": [
            "Byung-Kwan Lee",
            "Beomchan Park",
            "Chae Won Kim",
            "Yong Man Ro"
        ],
        "abstract": "The remarkable success of Large Language Models (LLMs) and instruction tuning\ndrives the evolution of Vision Language Models (VLMs) towards a versatile\ngeneral-purpose model. Yet, it remains unexplored whether current VLMs\ngenuinely possess quality object-level image understanding capabilities\ndetermined from `what objects are in the image?' or `which object corresponds\nto a specified bounding box?'. Our findings reveal that the image understanding\ncapabilities of current VLMs are strongly correlated with their zero-shot\nperformance on vision language (VL) tasks. This suggests that prioritizing\nbasic image understanding is crucial for VLMs to excel at VL tasks. To enhance\nobject-level image understanding, we propose Crayon Large Language and Vision\nmOdel(CoLLaVO), which incorporates instruction tuning with Crayon Prompt as a\nnew visual prompt tuning scheme based on panoptic color maps. Furthermore, we\npresent a learning strategy of Dual QLoRA to preserve object-level image\nunderstanding without forgetting it during visual instruction tuning, thereby\nachieving a significant leap in numerous VL benchmarks in a zero-shot setting.",
        "publication_date": "2024-02-17T11:03:02Z",
        "upvotes": 15
    },
    "2402.11550": {
        "url": "https://arxiv.org/abs/2402.11550",
        "title": "LongAgent: Scaling Language Models to 128k Context through Multi-Agent\n  Collaboration",
        "authors": [
            "Jun Zhao",
            "Can Zu",
            "Hao Xu",
            "Yi Lu",
            "Wei He",
            "Yiwen Ding",
            "Tao Gui",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "abstract": "Large language models (LLMs) have demonstrated impressive performance in\nunderstanding language and executing complex reasoning tasks. However, LLMs\nwith long context windows have been notorious for their expensive training\ncosts and high inference latency. Even the most advanced models such as GPT-4\nand Claude2 often make mistakes when processing inputs of over $100k$ tokens, a\nphenomenon also known as \\textit{lost in the middle}. In this paper, we propose\n\\textsc{LongAgent}, a method based on multi-agent collaboration, which scales\nLLMs (e.g., LLaMA) to a context of 128K and demonstrates potential superiority\nin long-text processing compared to GPT-4. In \\textsc{LongAgent}, a leader is\nresponsible for understanding user intent and directing team members to acquire\ninformation from documents. Due to members' hallucinations, it is non-trivial\nfor a leader to obtain accurate information from the responses of dozens to\nhundreds of members. To address this, we develop an \\textit{inter-member\ncommunication} mechanism to resolve response conflicts caused by hallucinations\nthrough information sharing. Our experimental results indicate that\n\\textsc{LongAgent} offers a promising alternative for long-text processing. The\nagent team instantiated with LLaMA-7B achieves significant improvements in\ntasks such as 128k-long text retrieval, multi-hop question answering, compared\nto GPT-4.",
        "publication_date": "2024-02-18T11:46:52Z",
        "upvotes": 12
    },
    "2402.10963": {
        "url": "https://arxiv.org/abs/2402.10963",
        "title": "GLoRe: When, Where, and How to Improve LLM Reasoning via Global and\n  Local Refinements",
        "authors": [
            "Alex Havrilla",
            "Sharath Raparthy",
            "Christoforus Nalmpantis",
            "Jane Dwivedi-Yu",
            "Maksym Zhuravinskyi",
            "Eric Hambro",
            "Roberta Railneau"
        ],
        "abstract": "State-of-the-art language models can exhibit impressive reasoning refinement\ncapabilities on math, science or coding tasks. However, recent work\ndemonstrates that even the best models struggle to identify \\textit{when and\nwhere to refine} without access to external feedback. Outcome-based Reward\nModels (\\textbf{ORMs}), trained to predict correctness of the final answer\nindicating when to refine, offer one convenient solution for deciding when to\nrefine. Process Based Reward Models (\\textbf{PRMs}), trained to predict\ncorrectness of intermediate steps, can then be used to indicate where to\nrefine. But they are expensive to train, requiring extensive human annotations.\nIn this paper, we propose Stepwise ORMs (\\textbf{SORMs}) which are trained,\nonly on synthetic data, to approximate the expected future reward of the\noptimal policy or $V^{\\star}$. More specifically, SORMs are trained to predict\nthe correctness of the final answer when sampling the current policy many times\n(rather than only once as in the case of ORMs). Our experiments show that SORMs\ncan more accurately detect incorrect reasoning steps compared to ORMs, thus\nimproving downstream accuracy when doing refinements. We then train\n\\textit{global} refinement models, which take only the question and a draft\nsolution as input and predict a corrected solution, and \\textit{local}\nrefinement models which also take as input a critique indicating the location\nof the first reasoning error. We generate training data for both models\nsynthetically by reusing data used to train the SORM. We find combining global\nand local refinements, using the ORM as a reranker, significantly outperforms\neither one individually, as well as a best of three sample baseline. With this\nstrategy we can improve the accuracy of a LLaMA-2 13B model (already fine-tuned\nwith RL) on GSM8K from 53\\% to 65\\% when greedily sampled.",
        "publication_date": "2024-02-13T20:16:29Z",
        "upvotes": 9
    },
    "2402.11929": {
        "url": "https://arxiv.org/abs/2402.11929",
        "title": "DiLightNet: Fine-grained Lighting Control for Diffusion-based Image\n  Generation",
        "authors": [
            "Chong Zeng",
            "Yue Dong",
            "Pieter Peers",
            "Youkang Kong",
            "Hongzhi Wu",
            "Xin Tong"
        ],
        "abstract": "This paper presents a novel method for exerting fine-grained lighting control\nduring text-driven diffusion-based image generation. While existing diffusion\nmodels already have the ability to generate images under any lighting\ncondition, without additional guidance these models tend to correlate image\ncontent and lighting. Moreover, text prompts lack the necessary expressional\npower to describe detailed lighting setups. To provide the content creator with\nfine-grained control over the lighting during image generation, we augment the\ntext-prompt with detailed lighting information in the form of radiance hints,\ni.e., visualizations of the scene geometry with a homogeneous canonical\nmaterial under the target lighting. However, the scene geometry needed to\nproduce the radiance hints is unknown. Our key observation is that we only need\nto guide the diffusion process, hence exact radiance hints are not necessary;\nwe only need to point the diffusion model in the right direction. Based on this\nobservation, we introduce a three stage method for controlling the lighting\nduring image generation. In the first stage, we leverage a standard pretrained\ndiffusion model to generate a provisional image under uncontrolled lighting.\nNext, in the second stage, we resynthesize and refine the foreground object in\nthe generated image by passing the target lighting to a refined diffusion\nmodel, named DiLightNet, using radiance hints computed on a coarse shape of the\nforeground object inferred from the provisional image. To retain the texture\ndetails, we multiply the radiance hints with a neural encoding of the\nprovisional synthesized image before passing it to DiLightNet. Finally, in the\nthird stage, we resynthesize the background to be consistent with the lighting\non the foreground object. We demonstrate and validate our lighting controlled\ndiffusion model on a variety of text prompts and lighting conditions.",
        "publication_date": "2024-02-19T08:17:21Z",
        "upvotes": 9
    },
    "2402.12377": {
        "url": "https://arxiv.org/abs/2402.12377",
        "title": "Binary Opacity Grids: Capturing Fine Geometric Detail for Mesh-Based\n  View Synthesis",
        "authors": [
            "Christian Reiser",
            "Stephan Garbin",
            "Pratul P. Srinivasan",
            "Dor Verbin",
            "Richard Szeliski",
            "Ben Mildenhall",
            "Jonathan T. Barron",
            "Peter Hedman",
            "Andreas Geiger"
        ],
        "abstract": "While surface-based view synthesis algorithms are appealing due to their low\ncomputational requirements, they often struggle to reproduce thin structures.\nIn contrast, more expensive methods that model the scene's geometry as a\nvolumetric density field (e.g. NeRF) excel at reconstructing fine geometric\ndetail. However, density fields often represent geometry in a \"fuzzy\" manner,\nwhich hinders exact localization of the surface. In this work, we modify\ndensity fields to encourage them to converge towards surfaces, without\ncompromising their ability to reconstruct thin structures. First, we employ a\ndiscrete opacity grid representation instead of a continuous density field,\nwhich allows opacity values to discontinuously transition from zero to one at\nthe surface. Second, we anti-alias by casting multiple rays per pixel, which\nallows occlusion boundaries and subpixel structures to be modelled without\nusing semi-transparent voxels. Third, we minimize the binary entropy of the\nopacity values, which facilitates the extraction of surface geometry by\nencouraging opacity values to binarize towards the end of training. Lastly, we\ndevelop a fusion-based meshing strategy followed by mesh simplification and\nappearance model fitting. The compact meshes produced by our model can be\nrendered in real-time on mobile devices and achieve significantly higher view\nsynthesis quality compared to existing mesh-based approaches.",
        "publication_date": "2024-02-19T18:59:41Z",
        "upvotes": 8
    },
    "2402.11690": {
        "url": "https://arxiv.org/abs/2402.11690",
        "title": "Vision-Flan: Scaling Human-Labeled Tasks in Visual Instruction Tuning",
        "authors": [
            "Zhiyang Xu",
            "Chao Feng",
            "Rulin Shao",
            "Trevor Ashby",
            "Ying Shen",
            "Di Jin",
            "Yu Cheng",
            "Qifan Wang",
            "Lifu Huang"
        ],
        "abstract": "Despite vision-language models' (VLMs) remarkable capabilities as versatile\nvisual assistants, two substantial challenges persist within the existing VLM\nframeworks: (1) lacking task diversity in pretraining and visual instruction\ntuning, and (2) annotation error and bias in GPT-4 synthesized instruction\ntuning data. Both challenges lead to issues such as poor generalizability,\nhallucination, and catastrophic forgetting. To address these challenges, we\nconstruct Vision-Flan, the most diverse publicly available visual instruction\ntuning dataset to date, comprising 187 diverse tasks and 1,664,261 instances\nsourced from academic datasets, and each task is accompanied by an\nexpert-written instruction. In addition, we propose a two-stage instruction\ntuning framework, in which VLMs are firstly finetuned on Vision-Flan and\nfurther tuned on GPT-4 synthesized data. We find this two-stage tuning\nframework significantly outperforms the traditional single-stage visual\ninstruction tuning framework and achieves the state-of-the-art performance\nacross a wide range of multi-modal evaluation benchmarks. Finally, we conduct\nin-depth analyses to understand visual instruction tuning and our findings\nreveal that: (1) GPT-4 synthesized data does not substantially enhance VLMs'\ncapabilities but rather modulates the model's responses to human-preferred\nformats; (2) A minimal quantity (e.g., 1,000) of GPT-4 synthesized data can\neffectively align VLM responses with human-preference; (3) Visual instruction\ntuning mainly helps large-language models (LLMs) to understand visual features.",
        "publication_date": "2024-02-18T19:38:44Z",
        "upvotes": 6
    },
    "2402.12225": {
        "url": "https://arxiv.org/abs/2402.12225",
        "title": "Pushing Auto-regressive Models for 3D Shape Generation at Capacity and\n  Scalability",
        "authors": [
            "Xuelin Qian",
            "Yu Wang",
            "Simian Luo",
            "Yinda Zhang",
            "Ying Tai",
            "Zhenyu Zhang",
            "Chengjie Wang",
            "Xiangyang Xue",
            "Bo Zhao",
            "Tiejun Huang",
            "Yunsheng Wu",
            "Yanwei Fu"
        ],
        "abstract": "Auto-regressive models have achieved impressive results in 2D image\ngeneration by modeling joint distributions in grid space. In this paper, we\nextend auto-regressive models to 3D domains, and seek a stronger ability of 3D\nshape generation by improving auto-regressive models at capacity and\nscalability simultaneously. Firstly, we leverage an ensemble of publicly\navailable 3D datasets to facilitate the training of large-scale models. It\nconsists of a comprehensive collection of approximately 900,000 objects, with\nmultiple properties of meshes, points, voxels, rendered images, and text\ncaptions. This diverse labeled dataset, termed Objaverse-Mix, empowers our\nmodel to learn from a wide range of object variations. However, directly\napplying 3D auto-regression encounters critical challenges of high\ncomputational demands on volumetric grids and ambiguous auto-regressive order\nalong grid dimensions, resulting in inferior quality of 3D shapes. To this end,\nwe then present a novel framework Argus3D in terms of capacity. Concretely, our\napproach introduces discrete representation learning based on a latent vector\ninstead of volumetric grids, which not only reduces computational costs but\nalso preserves essential geometric details by learning the joint distributions\nin a more tractable order. The capacity of conditional generation can thus be\nrealized by simply concatenating various conditioning inputs to the latent\nvector, such as point clouds, categories, images, and texts. In addition,\nthanks to the simplicity of our model architecture, we naturally scale up our\napproach to a larger model with an impressive 3.6 billion parameters, further\nenhancing the quality of versatile 3D generation. Extensive experiments on four\ngeneration tasks demonstrate that Argus3D can synthesize diverse and faithful\nshapes across multiple categories, achieving remarkable performance.",
        "publication_date": "2024-02-19T15:33:09Z",
        "upvotes": 5
    },
    "2402.13144": {
        "url": "https://arxiv.org/abs/2402.13144",
        "title": "Neural Network Diffusion",
        "authors": [
            "Kai Wang",
            "Zhaopan Xu",
            "Yukun Zhou",
            "Zelin Zang",
            "Trevor Darrell",
            "Zhuang Liu",
            "Yang You"
        ],
        "abstract": "Diffusion models have achieved remarkable success in image and video\ngeneration. In this work, we demonstrate that diffusion models can also\n\\textit{generate high-performing neural network parameters}. Our approach is\nsimple, utilizing an autoencoder and a standard latent diffusion model. The\nautoencoder extracts latent representations of a subset of the trained network\nparameters. A diffusion model is then trained to synthesize these latent\nparameter representations from random noise. It then generates new\nrepresentations that are passed through the autoencoder's decoder, whose\noutputs are ready to use as new subsets of network parameters. Across various\narchitectures and datasets, our diffusion process consistently generates models\nof comparable or improved performance over trained networks, with minimal\nadditional cost. Notably, we empirically find that the generated models perform\ndifferently with the trained networks. Our results encourage more exploration\non the versatile use of diffusion models.",
        "publication_date": "2024-02-20T16:59:03Z",
        "upvotes": 91
    },
    "2402.13064": {
        "url": "https://arxiv.org/abs/2402.13064",
        "title": "Synthetic Data (Almost) from Scratch: Generalized Instruction Tuning for\n  Language Models",
        "authors": [
            "Haoran Li",
            "Qingxiu Dong",
            "Zhengyang Tang",
            "Chaojun Wang",
            "Xingxing Zhang",
            "Haoyang Huang",
            "Shaohan Huang",
            "Xiaolong Huang",
            "Zeqiang Huang",
            "Dongdong Zhang",
            "Yuxian Gu",
            "Xin Cheng",
            "Xun Wang",
            "Si-Qing Chen",
            "Li Dong",
            "Wei Lu",
            "Zhifang Sui",
            "Benyou Wang",
            "Wai Lam",
            "Furu Wei"
        ],
        "abstract": "We introduce Generalized Instruction Tuning (called GLAN), a general and\nscalable method for instruction tuning of Large Language Models (LLMs). Unlike\nprior work that relies on seed examples or existing datasets to construct\ninstruction tuning data, GLAN exclusively utilizes a pre-curated taxonomy of\nhuman knowledge and capabilities as input and generates large-scale synthetic\ninstruction data across all disciplines. Specifically, inspired by the\nsystematic structure in human education system, we build the taxonomy by\ndecomposing human knowledge and capabilities to various fields, sub-fields and\nultimately, distinct disciplines semi-automatically, facilitated by LLMs.\nSubsequently, we generate a comprehensive list of subjects for every discipline\nand proceed to design a syllabus tailored to each subject, again utilizing\nLLMs. With the fine-grained key concepts detailed in every class session of the\nsyllabus, we are able to generate diverse instructions with a broad coverage\nacross the entire spectrum of human knowledge and skills. Extensive experiments\non large language models (e.g., Mistral) demonstrate that GLAN excels in\nmultiple dimensions from mathematical reasoning, coding, academic exams,\nlogical reasoning to general instruction following without using task-specific\ntraining data of these tasks. In addition, GLAN allows for easy customization\nand new fields or skills can be added by simply incorporating a new node into\nour taxonomy.",
        "publication_date": "2024-02-20T15:00:35Z",
        "upvotes": 45
    },
    "2402.12847": {
        "url": "https://arxiv.org/abs/2402.12847",
        "title": "Instruction-tuned Language Models are Better Knowledge Learners",
        "authors": [
            "Zhengbao Jiang",
            "Zhiqing Sun",
            "Weijia Shi",
            "Pedro Rodriguez",
            "Chunting Zhou",
            "Graham Neubig",
            "Xi Victoria Lin",
            "Wen-tau Yih",
            "Srinivasan Iyer"
        ],
        "abstract": "In order for large language model (LLM)-based assistants to effectively adapt\nto evolving information needs, it must be possible to update their factual\nknowledge through continued training on new data. The standard recipe for doing\nso involves continued pre-training on new documents followed by\ninstruction-tuning on question-answer (QA) pairs. However, we find that LLMs\ntrained with this recipe struggle to answer questions, even though the\nperplexity of documents is minimized. We found that QA pairs are generally\nstraightforward, while documents are more complex, weaving many factual\nstatements together in an intricate manner. Therefore, we hypothesize that it\nis beneficial to expose LLMs to QA pairs before continued pre-training on\ndocuments so that the process of encoding knowledge from complex documents\ntakes into account how this knowledge is accessed through questions. Based on\nthis, we propose pre-instruction-tuning (PIT), a method that instruction-tunes\non questions prior to training on documents. This contrasts with standard\ninstruction-tuning, which learns how to extract knowledge after training on\ndocuments. Extensive experiments and ablation studies demonstrate that PIT\nsignificantly enhances the ability of LLMs to absorb knowledge from new\ndocuments, outperforming standard instruction-tuning by 17.8%.",
        "publication_date": "2024-02-20T09:20:32Z",
        "upvotes": 23
    },
    "2402.13217": {
        "url": "https://arxiv.org/abs/2402.13217",
        "title": "VideoPrism: A Foundational Visual Encoder for Video Understanding",
        "authors": [
            "Long Zhao",
            "Nitesh B. Gundavarapu",
            "Liangzhe Yuan",
            "Hao Zhou",
            "Shen Yan",
            "Jennifer J. Sun",
            "Luke Friedman",
            "Rui Qian",
            "Tobias Weyand",
            "Yue Zhao",
            "Rachel Hornung",
            "Florian Schroff",
            "Ming-Hsuan Yang",
            "David A. Ross",
            "Huisheng Wang",
            "Hartwig Adam",
            "Mikhail Sirotenko",
            "Ting Liu",
            "Boqing Gong"
        ],
        "abstract": "We introduce VideoPrism, a general-purpose video encoder that tackles diverse\nvideo understanding tasks with a single frozen model. We pretrain VideoPrism on\na heterogeneous corpus containing 36M high-quality video-caption pairs and 582M\nvideo clips with noisy parallel text (e.g., ASR transcripts). The pretraining\napproach improves upon masked autoencoding by global-local distillation of\nsemantic video embeddings and a token shuffling scheme, enabling VideoPrism to\nfocus primarily on the video modality while leveraging the invaluable text\nassociated with videos. We extensively test VideoPrism on four broad groups of\nvideo understanding tasks, from web video question answering to CV for science,\nachieving state-of-the-art performance on 30 out of 33 video understanding\nbenchmarks.",
        "publication_date": "2024-02-20T18:29:49Z",
        "upvotes": 18
    },
    "2402.13250": {
        "url": "https://arxiv.org/abs/2402.13250",
        "title": "Video ReCap: Recursive Captioning of Hour-Long Videos",
        "authors": [
            "Md Mohaiminul Islam",
            "Ngan Ho",
            "Xitong Yang",
            "Tushar Nagarajan",
            "Lorenzo Torresani",
            "Gedas Bertasius"
        ],
        "abstract": "Most video captioning models are designed to process short video clips of few\nseconds and output text describing low-level visual concepts (e.g., objects,\nscenes, atomic actions). However, most real-world videos last for minutes or\nhours and have a complex hierarchical structure spanning different temporal\ngranularities. We propose Video ReCap, a recursive video captioning model that\ncan process video inputs of dramatically different lengths (from 1 second to 2\nhours) and output video captions at multiple hierarchy levels. The recursive\nvideo-language architecture exploits the synergy between different video\nhierarchies and can process hour-long videos efficiently. We utilize a\ncurriculum learning training scheme to learn the hierarchical structure of\nvideos, starting from clip-level captions describing atomic actions, then\nfocusing on segment-level descriptions, and concluding with generating\nsummaries for hour-long videos. Furthermore, we introduce Ego4D-HCap dataset by\naugmenting Ego4D with 8,267 manually collected long-range video summaries. Our\nrecursive model can flexibly generate captions at different hierarchy levels\nwhile also being useful for other complex video understanding tasks, such as\nVideoQA on EgoSchema. Data, code, and models are available at:\nhttps://sites.google.com/view/vidrecap",
        "publication_date": "2024-02-20T18:58:54Z",
        "upvotes": 18
    },
    "2402.13252": {
        "url": "https://arxiv.org/abs/2402.13252",
        "title": "Improving Robustness for Joint Optimization of Camera Poses and\n  Decomposed Low-Rank Tensorial Radiance Fields",
        "authors": [
            "Bo-Yu Cheng",
            "Wei-Chen Chiu",
            "Yu-Lun Liu"
        ],
        "abstract": "In this paper, we propose an algorithm that allows joint refinement of camera\npose and scene geometry represented by decomposed low-rank tensor, using only\n2D images as supervision. First, we conduct a pilot study based on a 1D signal\nand relate our findings to 3D scenarios, where the naive joint pose\noptimization on voxel-based NeRFs can easily lead to sub-optimal solutions.\nMoreover, based on the analysis of the frequency spectrum, we propose to apply\nconvolutional Gaussian filters on 2D and 3D radiance fields for a\ncoarse-to-fine training schedule that enables joint camera pose optimization.\nLeveraging the decomposition property in decomposed low-rank tensor, our method\nachieves an equivalent effect to brute-force 3D convolution with only incurring\nlittle computational overhead. To further improve the robustness and stability\nof joint optimization, we also propose techniques of smoothed 2D supervision,\nrandomly scaled kernel parameters, and edge-guided loss mask. Extensive\nquantitative and qualitative evaluations demonstrate that our proposed\nframework achieves superior performance in novel view synthesis as well as\nrapid convergence for optimization.",
        "publication_date": "2024-02-20T18:59:02Z",
        "upvotes": 17
    },
    "2402.13251": {
        "url": "https://arxiv.org/abs/2402.13251",
        "title": "FlashTex: Fast Relightable Mesh Texturing with LightControlNet",
        "authors": [
            "Kangle Deng",
            "Timothy Omernick",
            "Alexander Weiss",
            "Deva Ramanan",
            "Jun-Yan Zhu",
            "Tinghui Zhou",
            "Maneesh Agrawala"
        ],
        "abstract": "Manually creating textures for 3D meshes is time-consuming, even for expert\nvisual content creators. We propose a fast approach for automatically texturing\nan input 3D mesh based on a user-provided text prompt. Importantly, our\napproach disentangles lighting from surface material/reflectance in the\nresulting texture so that the mesh can be properly relit and rendered in any\nlighting environment. We introduce LightControlNet, a new text-to-image model\nbased on the ControlNet architecture, which allows the specification of the\ndesired lighting as a conditioning image to the model. Our text-to-texture\npipeline then constructs the texture in two stages. The first stage produces a\nsparse set of visually consistent reference views of the mesh using\nLightControlNet. The second stage applies a texture optimization based on Score\nDistillation Sampling (SDS) that works with LightControlNet to increase the\ntexture quality while disentangling surface material from lighting. Our\npipeline is significantly faster than previous text-to-texture methods, while\nproducing high-quality and relightable textures.",
        "publication_date": "2024-02-20T18:59:00Z",
        "upvotes": 13
    },
    "2402.12659": {
        "url": "https://arxiv.org/abs/2402.12659",
        "title": "The FinBen: An Holistic Financial Benchmark for Large Language Models",
        "authors": [
            "Qianqian Xie",
            "Weiguang Han",
            "Zhengyu Chen",
            "Ruoyu Xiang",
            "Xiao Zhang",
            "Yueru He",
            "Mengxi Xiao",
            "Dong Li",
            "Yongfu Dai",
            "Duanyu Feng",
            "Yijing Xu",
            "Haoqiang Kang",
            "Ziyan Kuang",
            "Chenhan Yuan",
            "Kailai Yang",
            "Zheheng Luo",
            "Tianlin Zhang",
            "Zhiwei Liu",
            "Guojun Xiong",
            "Zhiyang Deng",
            "Yuechen Jiang",
            "Zhiyuan Yao",
            "Haohang Li",
            "Yangyang Yu",
            "Gang Hu",
            "Jiajia Huang",
            "Xiao-Yang Liu",
            "Alejandro Lopez-Lira",
            "Benyou Wang",
            "Yanzhao Lai",
            "Hao Wang",
            "Min Peng",
            "Sophia Ananiadou",
            "Jimin Huang"
        ],
        "abstract": "LLMs have transformed NLP and shown promise in various fields, yet their\npotential in finance is underexplored due to a lack of thorough evaluations and\nthe complexity of financial tasks. This along with the rapid development of\nLLMs, highlights the urgent need for a systematic financial evaluation\nbenchmark for LLMs. In this paper, we introduce FinBen, the first comprehensive\nopen-sourced evaluation benchmark, specifically designed to thoroughly assess\nthe capabilities of LLMs in the financial domain. FinBen encompasses 35\ndatasets across 23 financial tasks, organized into three spectrums of\ndifficulty inspired by the Cattell-Horn-Carroll theory, to evaluate LLMs'\ncognitive abilities in inductive reasoning, associative memory, quantitative\nreasoning, crystallized intelligence, and more. Our evaluation of 15\nrepresentative LLMs, including GPT-4, ChatGPT, and the latest Gemini, reveals\ninsights into their strengths and limitations within the financial domain. The\nfindings indicate that GPT-4 leads in quantification, extraction, numerical\nreasoning, and stock trading, while Gemini shines in generation and\nforecasting; however, both struggle with complex extraction and forecasting,\nshowing a clear need for targeted enhancements. Instruction tuning boosts\nsimple task performance but falls short in improving complex reasoning and\nforecasting abilities. FinBen seeks to continuously evaluate LLMs in finance,\nfostering AI development with regular updates of tasks and models.",
        "publication_date": "2024-02-20T02:16:16Z",
        "upvotes": 13
    },
    "2402.12712": {
        "url": "https://arxiv.org/abs/2402.12712",
        "title": "MVDiffusion++: A Dense High-resolution Multi-view Diffusion Model for\n  Single or Sparse-view 3D Object Reconstruction",
        "authors": [
            "Shitao Tang",
            "Jiacheng Chen",
            "Dilin Wang",
            "Chengzhou Tang",
            "Fuyang Zhang",
            "Yuchen Fan",
            "Vikas Chandra",
            "Yasutaka Furukawa",
            "Rakesh Ranjan"
        ],
        "abstract": "This paper presents a neural architecture MVDiffusion++ for 3D object\nreconstruction that synthesizes dense and high-resolution views of an object\ngiven one or a few images without camera poses. MVDiffusion++ achieves superior\nflexibility and scalability with two surprisingly simple ideas: 1) A\n``pose-free architecture'' where standard self-attention among 2D latent\nfeatures learns 3D consistency across an arbitrary number of conditional and\ngeneration views without explicitly using camera pose information; and 2) A\n``view dropout strategy'' that discards a substantial number of output views\nduring training, which reduces the training-time memory footprint and enables\ndense and high-resolution view synthesis at test time. We use the Objaverse for\ntraining and the Google Scanned Objects for evaluation with standard novel view\nsynthesis and 3D reconstruction metrics, where MVDiffusion++ significantly\noutperforms the current state of the arts. We also demonstrate a text-to-3D\napplication example by combining MVDiffusion++ with a text-to-image generative\nmodel. The project page is at https://mvdiffusion-plusplus.github.io.",
        "publication_date": "2024-02-20T04:25:57Z",
        "upvotes": 13
    },
    "2402.13220": {
        "url": "https://arxiv.org/abs/2402.13220",
        "title": "How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis on\n  Deceptive Prompts",
        "authors": [
            "Yusu Qian",
            "Haotian Zhang",
            "Yinfei Yang",
            "Zhe Gan"
        ],
        "abstract": "The remarkable advancements in Multimodal Large Language Models (MLLMs) have\nnot rendered them immune to challenges, particularly in the context of handling\ndeceptive information in prompts, thus producing hallucinated responses under\nsuch conditions. To quantitatively assess this vulnerability, we present\nMAD-Bench, a carefully curated benchmark that contains 850 test samples divided\ninto 6 categories, such as non-existent objects, count of objects, spatial\nrelationship, and visual confusion. We provide a comprehensive analysis of\npopular MLLMs, ranging from GPT-4V, Gemini-Pro, to open-sourced models, such as\nLLaVA-1.5 and CogVLM. Empirically, we observe significant performance gaps\nbetween GPT-4V and other models; and previous robust instruction-tuned models,\nsuch as LRV-Instruction and LLaVA-RLHF, are not effective on this new\nbenchmark. While GPT-4V achieves 75.02% accuracy on MAD-Bench, the accuracy of\nany other model in our experiments ranges from 5% to 35%. We further propose a\nremedy that adds an additional paragraph to the deceptive prompts to encourage\nmodels to think twice before answering the question. Surprisingly, this simple\nmethod can even double the accuracy; however, the absolute numbers are still\ntoo low to be satisfactory. We hope MAD-Bench can serve as a valuable benchmark\nto stimulate further research to enhance models' resilience against deceptive\nprompts.",
        "publication_date": "2024-02-20T18:31:27Z",
        "upvotes": 12
    },
    "2402.13232": {
        "url": "https://arxiv.org/abs/2402.13232",
        "title": "A Touch, Vision, and Language Dataset for Multimodal Alignment",
        "authors": [
            "Letian Fu",
            "Gaurav Datta",
            "Huang Huang",
            "William Chung-Ho Panitch",
            "Jaimyn Drake",
            "Joseph Ortiz",
            "Mustafa Mukadam",
            "Mike Lambeta",
            "Roberto Calandra",
            "Ken Goldberg"
        ],
        "abstract": "Touch is an important sensing modality for humans, but it has not yet been\nincorporated into a multimodal generative language model. This is partially due\nto the difficulty of obtaining natural language labels for tactile data and the\ncomplexity of aligning tactile readings with both visual observations and\nlanguage descriptions. As a step towards bridging that gap, this work\nintroduces a new dataset of 44K in-the-wild vision-touch pairs, with English\nlanguage labels annotated by humans (10%) and textual pseudo-labels from GPT-4V\n(90%). We use this dataset to train a vision-language-aligned tactile encoder\nfor open-vocabulary classification and a touch-vision-language (TVL) model for\ntext generation using the trained encoder. Results suggest that by\nincorporating touch, the TVL model improves (+29% classification accuracy)\ntouch-vision-language alignment over existing models trained on any pair of\nthose modalities. Although only a small fraction of the dataset is\nhuman-labeled, the TVL model demonstrates improved visual-tactile understanding\nover GPT-4V (+12%) and open-source vision-language models (+32%) on a new\ntouch-vision understanding benchmark. Code and data:\nhttps://tactile-vlm.github.io.",
        "publication_date": "2024-02-20T18:47:56Z",
        "upvotes": 11
    },
    "2402.13249": {
        "url": "https://arxiv.org/abs/2402.13249",
        "title": "TofuEval: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue\n  Summarization",
        "authors": [
            "Liyan Tang",
            "Igor Shalyminov",
            "Amy Wing-mei Wong",
            "Jon Burnsky",
            "Jake W. Vincent",
            "Yu'an Yang",
            "Siffi Singh",
            "Song Feng",
            "Hwanjun Song",
            "Hang Su",
            "Lijia Sun",
            "Yi Zhang",
            "Saab Mansour",
            "Kathleen McKeown"
        ],
        "abstract": "Single document news summarization has seen substantial progress on\nfaithfulness in recent years, driven by research on the evaluation of factual\nconsistency, or hallucinations. We ask whether these advances carry over to\nother text summarization domains. We propose a new evaluation benchmark on\ntopic-focused dialogue summarization, generated by LLMs of varying sizes. We\nprovide binary sentence-level human annotations of the factual consistency of\nthese summaries along with detailed explanations of factually inconsistent\nsentences. Our analysis shows that existing LLMs hallucinate significant\namounts of factual errors in the dialogue domain, regardless of the model's\nsize. On the other hand, when LLMs, including GPT-4, serve as binary factual\nevaluators, they perform poorly and can be outperformed by prevailing\nstate-of-the-art specialized factuality evaluation metrics. Finally, we\nconducted an analysis of hallucination types with a curated error taxonomy. We\nfind that there are diverse errors and error distributions in model-generated\nsummaries and that non-LLM based metrics can capture all error types better\nthan LLM-based evaluators.",
        "publication_date": "2024-02-20T18:58:49Z",
        "upvotes": 10
    },
    "2402.12908": {
        "url": "https://arxiv.org/abs/2402.12908",
        "title": "RealCompo: Dynamic Equilibrium between Realism and Compositionality\n  Improves Text-to-Image Diffusion Models",
        "authors": [
            "Xinchen Zhang",
            "Ling Yang",
            "Yaqi Cai",
            "Zhaochen Yu",
            "Jiake Xie",
            "Ye Tian",
            "Minkai Xu",
            "Yong Tang",
            "Yujiu Yang",
            "Bin Cui"
        ],
        "abstract": "Diffusion models have achieved remarkable advancements in text-to-image\ngeneration. However, existing models still have many difficulties when faced\nwith multiple-object compositional generation. In this paper, we propose a new\ntraining-free and transferred-friendly text-to-image generation framework,\nnamely RealCompo, which aims to leverage the advantages of text-to-image and\nlayout-to-image models to enhance both realism and compositionality of the\ngenerated images. An intuitive and novel balancer is proposed to dynamically\nbalance the strengths of the two models in denoising process, allowing\nplug-and-play use of any model without extra training. Extensive experiments\nshow that our RealCompo consistently outperforms state-of-the-art text-to-image\nmodels and layout-to-image models in multiple-object compositional generation\nwhile keeping satisfactory realism and compositionality of the generated\nimages. Code is available at https://github.com/YangLing0818/RealCompo",
        "publication_date": "2024-02-20T10:56:52Z",
        "upvotes": 5
    },
    "2402.13753": {
        "url": "https://arxiv.org/abs/2402.13753",
        "title": "LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens",
        "authors": [
            "Yiran Ding",
            "Li Lyna Zhang",
            "Chengruidong Zhang",
            "Yuanyuan Xu",
            "Ning Shang",
            "Jiahang Xu",
            "Fan Yang",
            "Mao Yang"
        ],
        "abstract": "Large context window is a desirable feature in large language models (LLMs).\nHowever, due to high fine-tuning costs, scarcity of long texts, and\ncatastrophic values introduced by new token positions, current extended context\nwindows are limited to around 128k tokens. This paper introduces LongRoPE that,\nfor the first time, extends the context window of pre-trained LLMs to an\nimpressive 2048k tokens, with up to only 1k fine-tuning steps at within 256k\ntraining lengths, while maintaining performance at the original short context\nwindow. This is achieved by three key innovations: (i) we identify and exploit\ntwo forms of non-uniformities in positional interpolation through an efficient\nsearch, providing a better initialization for fine-tuning and enabling an 8x\nextension in non-fine-tuning scenarios; (ii) we introduce a progressive\nextension strategy that first fine-tunes a 256k length LLM and then conducts a\nsecond positional interpolation on the fine-tuned extended LLM to achieve a\n2048k context window; (iii) we readjust LongRoPE on 8k length to recover the\nshort context window performance. Extensive experiments on LLaMA2 and Mistral\nacross various tasks demonstrate the effectiveness of our method. Models\nextended via LongRoPE retain the original architecture with minor modifications\nto the positional embedding, and can reuse most pre-existing optimizations.",
        "publication_date": "2024-02-21T12:30:33Z",
        "upvotes": 99
    },
    "2402.13616": {
        "url": "https://arxiv.org/abs/2402.13616",
        "title": "YOLOv9: Learning What You Want to Learn Using Programmable Gradient\n  Information",
        "authors": [
            "Chien-Yao Wang",
            "I-Hau Yeh",
            "Hong-Yuan Mark Liao"
        ],
        "abstract": "Today's deep learning methods focus on how to design the most appropriate\nobjective functions so that the prediction results of the model can be closest\nto the ground truth. Meanwhile, an appropriate architecture that can facilitate\nacquisition of enough information for prediction has to be designed. Existing\nmethods ignore a fact that when input data undergoes layer-by-layer feature\nextraction and spatial transformation, large amount of information will be\nlost. This paper will delve into the important issues of data loss when data is\ntransmitted through deep networks, namely information bottleneck and reversible\nfunctions. We proposed the concept of programmable gradient information (PGI)\nto cope with the various changes required by deep networks to achieve multiple\nobjectives. PGI can provide complete input information for the target task to\ncalculate objective function, so that reliable gradient information can be\nobtained to update network weights. In addition, a new lightweight network\narchitecture -- Generalized Efficient Layer Aggregation Network (GELAN), based\non gradient path planning is designed. GELAN's architecture confirms that PGI\nhas gained superior results on lightweight models. We verified the proposed\nGELAN and PGI on MS COCO dataset based object detection. The results show that\nGELAN only uses conventional convolution operators to achieve better parameter\nutilization than the state-of-the-art methods developed based on depth-wise\nconvolution. PGI can be used for variety of models from lightweight to large.\nIt can be used to obtain complete information, so that train-from-scratch\nmodels can achieve better results than state-of-the-art models pre-trained\nusing large datasets, the comparison results are shown in Figure 1. The source\ncodes are at: https://github.com/WongKinYiu/yolov9.",
        "publication_date": "2024-02-21T08:42:53Z",
        "upvotes": 43
    },
    "2402.13349": {
        "url": "https://arxiv.org/abs/2402.13349",
        "title": "Aria Everyday Activities Dataset",
        "authors": [
            "Zhaoyang Lv",
            "Nicholas Charron",
            "Pierre Moulon",
            "Alexander Gamino",
            "Cheng Peng",
            "Chris Sweeney",
            "Edward Miller",
            "Huixuan Tang",
            "Jeff Meissner",
            "Jing Dong",
            "Kiran Somasundaram",
            "Luis Pesqueira",
            "Mark Schwesinger",
            "Omkar Parkhi",
            "Qiao Gu",
            "Renzo De Nardi",
            "Shangyi Cheng",
            "Steve Saarinen",
            "Vijay Baiyya",
            "Yuyang Zou",
            "Richard Newcombe",
            "Jakob Julian Engel",
            "Xiaqing Pan",
            "Carl Ren"
        ],
        "abstract": "We present Aria Everyday Activities (AEA) Dataset, an egocentric multimodal\nopen dataset recorded using Project Aria glasses. AEA contains 143 daily\nactivity sequences recorded by multiple wearers in five geographically diverse\nindoor locations. Each of the recording contains multimodal sensor data\nrecorded through the Project Aria glasses. In addition, AEA provides machine\nperception data including high frequency globally aligned 3D trajectories,\nscene point cloud, per-frame 3D eye gaze vector and time aligned speech\ntranscription. In this paper, we demonstrate a few exemplar research\napplications enabled by this dataset, including neural scene reconstruction and\nprompted segmentation. AEA is an open source dataset that can be downloaded\nfrom https://www.projectaria.com/datasets/aea/. We are also providing\nopen-source implementations and examples of how to use the dataset in Project\nAria Tools https://github.com/facebookresearch/projectaria_tools.",
        "publication_date": "2024-02-20T19:53:15Z",
        "upvotes": 28
    },
    "2402.13929": {
        "url": "https://arxiv.org/abs/2402.13929",
        "title": "SDXL-Lightning: Progressive Adversarial Diffusion Distillation",
        "authors": [
            "Shanchuan Lin",
            "Anran Wang",
            "Xiao Yang"
        ],
        "abstract": "We propose a diffusion distillation method that achieves new state-of-the-art\nin one-step/few-step 1024px text-to-image generation based on SDXL. Our method\ncombines progressive and adversarial distillation to achieve a balance between\nquality and mode coverage. In this paper, we discuss the theoretical analysis,\ndiscriminator design, model formulation, and training techniques. We\nopen-source our distilled SDXL-Lightning models both as LoRA and full UNet\nweights.",
        "publication_date": "2024-02-21T16:51:05Z",
        "upvotes": 24
    },
    "2402.13598": {
        "url": "https://arxiv.org/abs/2402.13598",
        "title": "User-LLM: Efficient LLM Contextualization with User Embeddings",
        "authors": [
            "Lin Ning",
            "Luyang Liu",
            "Jiaxing Wu",
            "Neo Wu",
            "Devora Berlowitz",
            "Sushant Prakash",
            "Bradley Green",
            "Shawn O'Banion",
            "Jun Xie"
        ],
        "abstract": "Large language models (LLMs) have revolutionized natural language processing.\nHowever, effectively incorporating complex and potentially noisy user\ninteraction data remains a challenge. To address this, we propose User-LLM, a\nnovel framework that leverages user embeddings to contextualize LLMs. These\nembeddings, distilled from diverse user interactions using self-supervised\npretraining, capture latent user preferences and their evolution over time. We\nintegrate these user embeddings with LLMs through cross-attention and\nsoft-prompting, enabling LLMs to dynamically adapt to user context. Our\ncomprehensive experiments on MovieLens, Amazon Review, and Google Local Review\ndatasets demonstrate significant performance gains across various tasks.\nNotably, our approach outperforms text-prompt-based contextualization on long\nsequence tasks and tasks that require deep user understanding while being\ncomputationally efficient. We further incorporate Perceiver layers to\nstreamline the integration between user encoders and LLMs, reducing\ncomputational demands.",
        "publication_date": "2024-02-21T08:03:27Z",
        "upvotes": 17
    },
    "2402.12479": {
        "url": "https://arxiv.org/abs/2402.12479",
        "title": "In deep reinforcement learning, a pruned network is a good network",
        "authors": [
            "Johan Obando-Ceron",
            "Aaron Courville",
            "Pablo Samuel Castro"
        ],
        "abstract": "Recent work has shown that deep reinforcement learning agents have difficulty\nin effectively using their network parameters. We leverage prior insights into\nthe advantages of sparse training techniques and demonstrate that gradual\nmagnitude pruning enables agents to maximize parameter effectiveness. This\nresults in networks that yield dramatic performance improvements over\ntraditional networks and exhibit a type of \"scaling law\", using only a small\nfraction of the full network parameters.",
        "publication_date": "2024-02-19T19:34:07Z",
        "upvotes": 16
    },
    "2402.14020": {
        "url": "https://arxiv.org/abs/2402.14020",
        "title": "Coercing LLMs to do and reveal (almost) anything",
        "authors": [
            "Jonas Geiping",
            "Alex Stein",
            "Manli Shu",
            "Khalid Saifullah",
            "Yuxin Wen",
            "Tom Goldstein"
        ],
        "abstract": "It has recently been shown that adversarial attacks on large language models\n(LLMs) can \"jailbreak\" the model into making harmful statements. In this work,\nwe argue that the spectrum of adversarial attacks on LLMs is much larger than\nmerely jailbreaking. We provide a broad overview of possible attack surfaces\nand attack goals. Based on a series of concrete examples, we discuss,\ncategorize and systematize attacks that coerce varied unintended behaviors,\nsuch as misdirection, model control, denial-of-service, or data extraction.\n  We analyze these attacks in controlled experiments, and find that many of\nthem stem from the practice of pre-training LLMs with coding capabilities, as\nwell as the continued existence of strange \"glitch\" tokens in common LLM\nvocabularies that should be removed for security reasons.",
        "publication_date": "2024-02-21T18:59:13Z",
        "upvotes": 11
    },
    "2402.13763": {
        "url": "https://arxiv.org/abs/2402.13763",
        "title": "Music Style Transfer with Time-Varying Inversion of Diffusion Models",
        "authors": [
            "Sifei Li",
            "Yuxin Zhang",
            "Fan Tang",
            "Chongyang Ma",
            "Weiming dong",
            "Changsheng Xu"
        ],
        "abstract": "With the development of diffusion models, text-guided image style transfer\nhas demonstrated high-quality controllable synthesis results. However, the\nutilization of text for diverse music style transfer poses significant\nchallenges, primarily due to the limited availability of matched audio-text\ndatasets. Music, being an abstract and complex art form, exhibits variations\nand intricacies even within the same genre, thereby making accurate textual\ndescriptions challenging. This paper presents a music style transfer approach\nthat effectively captures musical attributes using minimal data. We introduce a\nnovel time-varying textual inversion module to precisely capture\nmel-spectrogram features at different levels. During inference, we propose a\nbias-reduced stylization technique to obtain stable results. Experimental\nresults demonstrate that our method can transfer the style of specific\ninstruments, as well as incorporate natural sounds to compose melodies. Samples\nand source code are available at https://lsfhuihuiff.github.io/MusicTI/.",
        "publication_date": "2024-02-21T12:38:48Z",
        "upvotes": 9
    },
    "2402.13573": {
        "url": "https://arxiv.org/abs/2402.13573",
        "title": "ToDo: Token Downsampling for Efficient Generation of High-Resolution\n  Images",
        "authors": [
            "Ethan Smith",
            "Nayan Saxena",
            "Aninda Saha"
        ],
        "abstract": "Attention mechanism has been crucial for image diffusion models, however,\ntheir quadratic computational complexity limits the sizes of images we can\nprocess within reasonable time and memory constraints. This paper investigates\nthe importance of dense attention in generative image models, which often\ncontain redundant features, making them suitable for sparser attention\nmechanisms. We propose a novel training-free method ToDo that relies on token\ndownsampling of key and value tokens to accelerate Stable Diffusion inference\nby up to 2x for common sizes and up to 4.5x or more for high resolutions like\n2048x2048. We demonstrate that our approach outperforms previous methods in\nbalancing efficient throughput and fidelity.",
        "publication_date": "2024-02-21T07:10:28Z",
        "upvotes": 7
    },
    "2402.14017": {
        "url": "https://arxiv.org/abs/2402.14017",
        "title": "D-Flow: Differentiating through Flows for Controlled Generation",
        "authors": [
            "Heli Ben-Hamu",
            "Omri Puny",
            "Itai Gat",
            "Brian Karrer",
            "Uriel Singer",
            "Yaron Lipman"
        ],
        "abstract": "Taming the generation outcome of state of the art Diffusion and Flow-Matching\n(FM) models without having to re-train a task-specific model unlocks a powerful\ntool for solving inverse problems, conditional generation, and controlled\ngeneration in general. In this work we introduce D-Flow, a simple framework for\ncontrolling the generation process by differentiating through the flow,\noptimizing for the source (noise) point. We motivate this framework by our key\nobservation stating that for Diffusion/FM models trained with Gaussian\nprobability paths, differentiating through the generation process projects\ngradient on the data manifold, implicitly injecting the prior into the\noptimization process. We validate our framework on linear and non-linear\ncontrolled generation problems including: image and audio inverse problems and\nconditional molecule generation reaching state of the art performance across\nall.",
        "publication_date": "2024-02-21T18:56:03Z",
        "upvotes": 5
    },
    "2402.13577": {
        "url": "https://arxiv.org/abs/2402.13577",
        "title": "BBA: Bi-Modal Behavioral Alignment for Reasoning with Large\n  Vision-Language Models",
        "authors": [
            "Xueliang Zhao",
            "Xinting Huang",
            "Tingchen Fu",
            "Qintong Li",
            "Shansan Gong",
            "Lemao Liu",
            "Wei Bi",
            "Lingpeng Kong"
        ],
        "abstract": "Multimodal reasoning stands as a pivotal capability for large vision-language\nmodels (LVLMs). The integration with Domain-Specific Languages (DSL), offering\nprecise visual representations, equips these models with the opportunity to\nexecute more accurate reasoning in complex and professional domains. However,\nthe vanilla Chain-of-Thought (CoT) prompting method faces challenges in\neffectively leveraging the unique strengths of visual and DSL representations,\nprimarily due to their differing reasoning mechanisms. Additionally, it often\nfalls short in addressing critical steps in multi-step reasoning tasks. To\nmitigate these challenges, we introduce the \\underline{B}i-Modal\n\\underline{B}ehavioral \\underline{A}lignment (BBA) prompting method, designed\nto maximize the potential of DSL in augmenting complex multi-modal reasoning\ntasks. This method initiates by guiding LVLMs to create separate reasoning\nchains for visual and DSL representations. Subsequently, it aligns these chains\nby addressing any inconsistencies, thus achieving a cohesive integration of\nbehaviors from different modalities. Our experiments demonstrate that BBA\nsubstantially improves the performance of GPT-4V(ision) on geometry problem\nsolving ($28.34\\% \\to 34.22\\%$), chess positional advantage prediction\n($42.08\\% \\to 46.99\\%$) and molecular property prediction ($77.47\\% \\to\n83.52\\%$).",
        "publication_date": "2024-02-21T07:16:29Z",
        "upvotes": 5
    },
    "2402.13720": {
        "url": "https://arxiv.org/abs/2402.13720",
        "title": "Ouroboros: Speculative Decoding with Large Model Enhanced Drafting",
        "authors": [
            "Weilin Zhao",
            "Yuxiang Huang",
            "Xu Han",
            "Chaojun Xiao",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "abstract": "Drafting-then-verifying decoding methods such as speculative decoding are\nwidely adopted training-free methods to accelerate the inference of large\nlanguage models (LLMs). Instead of employing an autoregressive process to\ndecode tokens sequentially, speculative decoding initially creates drafts with\nan efficient small model. Then LLMs are required to conduct verification and\ncorrection in a non-autoregressive fashion to minimize time overhead.\nGenerating longer drafts can lead to even more significant speedups once\nverified, but also incurs substantial trial and error costs if it fails.\nSuffering from the high verification failure probability, existing decoding\nmethods cannot draft too much content for verification at one time, achieving\nsub-optimal inference acceleration. In this paper, we introduce Ouroboros,\nwhich constructs a phrase candidate pool from the verification process of LLMs\nto provide candidates for draft generation of the small model. Thereby,\nOuroboros can further improve the efficiency and effectiveness of the initial\ndrafts. The experimental results on typical text generation tasks show that\nOuroboros achieves speedups of up to 1.9x and 2.8x compared to lookahead\ndecoding and speculative decoding, respectively. The source code of Ouroboros\nis available at https://github.com/thunlp/Ouroboros.",
        "publication_date": "2024-02-21T11:31:28Z",
        "upvotes": 4
    },
    "2402.14658": {
        "url": "https://arxiv.org/abs/2402.14658",
        "title": "OpenCodeInterpreter: Integrating Code Generation with Execution and\n  Refinement",
        "authors": [
            "Tianyu Zheng",
            "Ge Zhang",
            "Tianhao Shen",
            "Xueling Liu",
            "Bill Yuchen Lin",
            "Jie Fu",
            "Wenhu Chen",
            "Xiang Yue"
        ],
        "abstract": "The introduction of large language models has significantly advanced code\ngeneration. However, open-source models often lack the execution capabilities\nand iterative refinement of advanced systems like the GPT-4 Code Interpreter.\nTo address this, we introduce OpenCodeInterpreter, a family of open-source code\nsystems designed for generating, executing, and iteratively refining code.\nSupported by Code-Feedback, a dataset featuring 68K multi-turn interactions,\nOpenCodeInterpreter integrates execution and human feedback for dynamic code\nrefinement. Our comprehensive evaluation of OpenCodeInterpreter across key\nbenchmarks such as HumanEval, MBPP, and their enhanced versions from EvalPlus\nreveals its exceptional performance. Notably, OpenCodeInterpreter-33B achieves\nan accuracy of 83.2 (76.4) on the average (and plus versions) of HumanEval and\nMBPP, closely rivaling GPT-4's 84.2 (76.2) and further elevates to 91.6 (84.6)\nwith synthesized human feedback from GPT-4. OpenCodeInterpreter brings the gap\nbetween open-source code generation models and proprietary systems like GPT-4\nCode Interpreter.",
        "publication_date": "2024-02-22T16:06:23Z",
        "upvotes": 74
    },
    "2402.14083": {
        "url": "https://arxiv.org/abs/2402.14083",
        "title": "Beyond A*: Better Planning with Transformers via Search Dynamics\n  Bootstrapping",
        "authors": [
            "Lucas Lehnert",
            "Sainbayar Sukhbaatar",
            "Paul Mcvay",
            "Michael Rabbat",
            "Yuandong Tian"
        ],
        "abstract": "While Transformers have enabled tremendous progress in various application\nsettings, such architectures still lag behind traditional symbolic planners for\nsolving complex decision making tasks. In this work, we demonstrate how to\ntrain Transformers to solve complex planning tasks and present Searchformer, a\nTransformer model that optimally solves previously unseen Sokoban puzzles 93.7%\nof the time, while using up to 26.8% fewer search steps than standard $A^*$\nsearch. Searchformer is an encoder-decoder Transformer model trained to predict\nthe search dynamics of $A^*$. This model is then fine-tuned via expert\niterations to perform fewer search steps than $A^*$ search while still\ngenerating an optimal plan. In our training method, $A^*$'s search dynamics are\nexpressed as a token sequence outlining when task states are added and removed\ninto the search tree during symbolic planning. In our ablation studies on maze\nnavigation, we find that Searchformer significantly outperforms baselines that\npredict the optimal plan directly with a 5-10$\\times$ smaller model size and a\n10$\\times$ smaller training dataset. We also demonstrate how Searchformer\nscales to larger and more complex decision making tasks like Sokoban with\nimproved percentage of solved tasks and shortened search dynamics.",
        "publication_date": "2024-02-21T19:17:28Z",
        "upvotes": 38
    },
    "2402.14818": {
        "url": "https://arxiv.org/abs/2402.14818",
        "title": "PALO: A Polyglot Large Multimodal Model for 5B People",
        "authors": [
            "Muhammad Maaz",
            "Hanoona Rasheed",
            "Abdelrahman Shaker",
            "Salman Khan",
            "Hisham Cholakal",
            "Rao M. Anwer",
            "Tim Baldwin",
            "Michael Felsberg",
            "Fahad S. Khan"
        ],
        "abstract": "In pursuit of more inclusive Vision-Language Models (VLMs), this study\nintroduces a Large Multilingual Multimodal Model called PALO. PALO offers\nvisual reasoning capabilities in 10 major languages, including English,\nChinese, Hindi, Spanish, French, Arabic, Bengali, Russian, Urdu, and Japanese,\nthat span a total of ~5B people (65% of the world population). Our approach\ninvolves a semi-automated translation approach to adapt the multimodal\ninstruction dataset from English to the target languages using a fine-tuned\nLarge Language Model, thereby ensuring high linguistic fidelity while allowing\nscalability due to minimal manual effort. The incorporation of diverse\ninstruction sets helps us boost overall performance across multiple languages\nespecially those that are underrepresented like Hindi, Arabic, Bengali, and\nUrdu. The resulting models are trained across three scales (1.7B, 7B and 13B\nparameters) to show the generalization and scalability where we observe\nsubstantial improvements compared to strong baselines. We also propose the\nfirst multilingual multimodal benchmark for the forthcoming approaches to\nevaluate their vision-language reasoning capabilities across languages. Code:\nhttps://github.com/mbzuai-oryx/PALO.",
        "publication_date": "2024-02-22T18:59:58Z",
        "upvotes": 21
    },
    "2402.14797": {
        "url": "https://arxiv.org/abs/2402.14797",
        "title": "Snap Video: Scaled Spatiotemporal Transformers for Text-to-Video\n  Synthesis",
        "authors": [
            "Willi Menapace",
            "Aliaksandr Siarohin",
            "Ivan Skorokhodov",
            "Ekaterina Deyneka",
            "Tsai-Shien Chen",
            "Anil Kag",
            "Yuwei Fang",
            "Aleksei Stoliar",
            "Elisa Ricci",
            "Jian Ren",
            "Sergey Tulyakov"
        ],
        "abstract": "Contemporary models for generating images show remarkable quality and\nversatility. Swayed by these advantages, the research community repurposes them\nto generate videos. Since video content is highly redundant, we argue that\nnaively bringing advances of image models to the video generation domain\nreduces motion fidelity, visual quality and impairs scalability. In this work,\nwe build Snap Video, a video-first model that systematically addresses these\nchallenges. To do that, we first extend the EDM framework to take into account\nspatially and temporally redundant pixels and naturally support video\ngeneration. Second, we show that a U-Net - a workhorse behind image generation\n- scales poorly when generating videos, requiring significant computational\noverhead. Hence, we propose a new transformer-based architecture that trains\n3.31 times faster than U-Nets (and is ~4.5 faster at inference). This allows us\nto efficiently train a text-to-video model with billions of parameters for the\nfirst time, reach state-of-the-art results on a number of benchmarks, and\ngenerate videos with substantially higher quality, temporal consistency, and\nmotion complexity. The user studies showed that our model was favored by a\nlarge margin over the most recent methods. See our website at\nhttps://snap-research.github.io/snapvideo/.",
        "publication_date": "2024-02-22T18:55:08Z",
        "upvotes": 18
    },
    "2402.14289": {
        "url": "https://arxiv.org/abs/2402.14289",
        "title": "TinyLLaVA: A Framework of Small-scale Large Multimodal Models",
        "authors": [
            "Baichuan Zhou",
            "Ying Hu",
            "Xi Weng",
            "Junlong Jia",
            "Jie Luo",
            "Xien Liu",
            "Ji Wu",
            "Lei Huang"
        ],
        "abstract": "We present the TinyLLaVA framework that provides a unified perspective in\ndesigning and analyzing the small-scale Large Multimodal Models (LMMs). We\nempirically study the effects of different vision encoders, connection modules,\nlanguage models, training data and training recipes. Our extensive experiments\nshowed that better quality of data combined with better training recipes,\nsmaller LMMs can consistently achieve on-par performances compared to bigger\nLMMs. Under our framework, we train a family of small-scale LMMs. Our best\nmodel, TinyLLaVA-3.1B, achieves better overall performance against existing 7B\nmodels such as LLaVA-1.5 and Qwen-VL. We hope our findings can serve as\nbaselines for future research in terms of data scaling, training setups and\nmodel selections. Our model weights and codes will be made public.",
        "publication_date": "2024-02-22T05:05:30Z",
        "upvotes": 16
    },
    "2402.14327": {
        "url": "https://arxiv.org/abs/2402.14327",
        "title": "Subobject-level Image Tokenization",
        "authors": [
            "Delong Chen",
            "Samuel Cahyawijaya",
            "Jianfeng Liu",
            "Baoyuan Wang",
            "Pascale Fung"
        ],
        "abstract": "Transformer-based vision models typically tokenize images into fixed-size\nsquare patches as input units, which lacks the adaptability to image content\nand overlooks the inherent pixel grouping structure. Inspired by the subword\ntokenization widely adopted in language models, we propose an image tokenizer\nat a subobject level, where the subobjects are represented by semantically\nmeaningful image segments obtained by segmentation models (e.g., segment\nanything models). To implement a learning system based on subobject\ntokenization, we first introduced a Sequence-to-sequence AutoEncoder (SeqAE) to\ncompress subobject segments of varying sizes and shapes into compact embedding\nvectors, then fed the subobject embeddings into a large language model for\nvision language learning. Empirical results demonstrated that our\nsubobject-level tokenization significantly facilitates efficient learning of\ntranslating images into object and attribute descriptions compared to the\ntraditional patch-level tokenization. Codes and models will be open-sourced at\nhttps://github.com/ChenDelong1999/subobjects.",
        "publication_date": "2024-02-22T06:47:44Z",
        "upvotes": 15
    },
    "2402.14034": {
        "url": "https://arxiv.org/abs/2402.14034",
        "title": "AgentScope: A Flexible yet Robust Multi-Agent Platform",
        "authors": [
            "Dawei Gao",
            "Zitao Li",
            "Weirui Kuang",
            "Xuchen Pan",
            "Daoyuan Chen",
            "Zhijian Ma",
            "Bingchen Qian",
            "Liuyi Yao",
            "Lin Zhu",
            "Chen Cheng",
            "Hongzhu Shi",
            "Yaliang Li",
            "Bolin Ding",
            "Jingren Zhou"
        ],
        "abstract": "With the rapid advancement of Large Language Models (LLMs), significant\nprogress has been made in multi-agent applications. However, the complexities\nin coordinating agents' cooperation and LLMs' erratic performance pose notable\nchallenges in developing robust and efficient multi-agent applications. To\ntackle these challenges, we propose AgentScope, a developer-centric multi-agent\nplatform with message exchange as its core communication mechanism. Together\nwith abundant syntactic tools, built-in resources, and user-friendly\ninteractions, our communication mechanism significantly reduces the barriers to\nboth development and understanding. Towards robust and flexible multi-agent\napplication, AgentScope provides both built-in and customizable fault tolerance\nmechanisms while it is also armed with system-level supports for multi-modal\ndata generation, storage and transmission. Additionally, we design an\nactor-based distribution framework, enabling easy conversion between local and\ndistributed deployments and automatic parallel optimization without extra\neffort. With these features, AgentScope empowers developers to build\napplications that fully realize the potential of intelligent agents. We have\nreleased AgentScope at https://github.com/modelscope/agentscope, and hope\nAgentScope invites wider participation and innovation in this fast-moving\nfield.",
        "publication_date": "2024-02-21T04:11:28Z",
        "upvotes": 12
    },
    "2402.14547": {
        "url": "https://arxiv.org/abs/2402.14547",
        "title": "OmniPred: Language Models as Universal Regressors",
        "authors": [
            "Xingyou Song",
            "Oscar Li",
            "Chansoo Lee",
            "Bangding Yang",
            "Daiyi Peng",
            "Sagi Perel",
            "Yutian Chen"
        ],
        "abstract": "Over the broad landscape of experimental design, regression has been a\npowerful tool to accurately predict the outcome metrics of a system or model\ngiven a set of parameters, but has been traditionally restricted to methods\nwhich are only applicable to a specific task. In this paper, we propose\nOmniPred, a framework for training language models as universal end-to-end\nregressors over $(x,y)$ evaluation data from diverse real world experiments.\nUsing data sourced from Google Vizier, one of the largest blackbox optimization\ndatabases in the world, our extensive experiments demonstrate that through only\ntextual representations of mathematical parameters and values, language models\nare capable of very precise numerical regression, and if given the opportunity\nto train over multiple tasks, can significantly outperform traditional\nregression models.",
        "publication_date": "2024-02-22T13:36:53Z",
        "upvotes": 11
    },
    "2402.14261": {
        "url": "https://arxiv.org/abs/2402.14261",
        "title": "Copilot Evaluation Harness: Evaluating LLM-Guided Software Programming",
        "authors": [
            "Anisha Agarwal",
            "Aaron Chan",
            "Shubham Chandel",
            "Jinu Jang",
            "Shaun Miller",
            "Roshanak Zilouchian Moghaddam",
            "Yevhen Mohylevskyy",
            "Neel Sundaresan",
            "Michele Tufano"
        ],
        "abstract": "The integration of Large Language Models (LLMs) into Development Environments\n(IDEs) has become a focal point in modern software development. LLMs such as\nOpenAI GPT-3.5/4 and Code Llama offer the potential to significantly augment\ndeveloper productivity by serving as intelligent, chat-driven programming\nassistants. However, utilizing LLMs out of the box is unlikely to be optimal\nfor any given scenario. Rather, each system requires the LLM to be honed to its\nset of heuristics to ensure the best performance. In this paper, we introduce\nthe Copilot evaluation harness: a set of data and tools for evaluating\nLLM-guided IDE interactions, covering various programming scenarios and\nlanguages. We propose our metrics as a more robust and information-dense\nevaluation than previous state of the art evaluation systems. We design and\ncompute both static and execution based success metrics for scenarios\nencompassing a wide range of developer tasks, including code generation from\nnatural language (generate), documentation generation from code (doc), test\ncase generation (test), bug-fixing (fix), and workspace understanding and query\nresolution (workspace). These success metrics are designed to evaluate the\nperformance of LLMs within a given IDE and its respective parameter space. Our\nlearnings from evaluating three common LLMs using these metrics can inform the\ndevelopment and validation of future scenarios in LLM guided IDEs.",
        "publication_date": "2024-02-22T03:51:34Z",
        "upvotes": 10
    },
    "2402.14167": {
        "url": "https://arxiv.org/abs/2402.14167",
        "title": "T-Stitch: Accelerating Sampling in Pre-Trained Diffusion Models with\n  Trajectory Stitching",
        "authors": [
            "Zizheng Pan",
            "Bohan Zhuang",
            "De-An Huang",
            "Weili Nie",
            "Zhiding Yu",
            "Chaowei Xiao",
            "Jianfei Cai",
            "Anima Anandkumar"
        ],
        "abstract": "Sampling from diffusion probabilistic models (DPMs) is often expensive for\nhigh-quality image generation and typically requires many steps with a large\nmodel. In this paper, we introduce sampling Trajectory Stitching T-Stitch, a\nsimple yet efficient technique to improve the sampling efficiency with little\nor no generation degradation. Instead of solely using a large DPM for the\nentire sampling trajectory, T-Stitch first leverages a smaller DPM in the\ninitial steps as a cheap drop-in replacement of the larger DPM and switches to\nthe larger DPM at a later stage. Our key insight is that different diffusion\nmodels learn similar encodings under the same training data distribution and\nsmaller models are capable of generating good global structures in the early\nsteps. Extensive experiments demonstrate that T-Stitch is training-free,\ngenerally applicable for different architectures, and complements most existing\nfast sampling techniques with flexible speed and quality trade-offs. On DiT-XL,\nfor example, 40% of the early timesteps can be safely replaced with a 10x\nfaster DiT-S without performance drop on class-conditional ImageNet generation.\nWe further show that our method can also be used as a drop-in technique to not\nonly accelerate the popular pretrained stable diffusion (SD) models but also\nimprove the prompt alignment of stylized SD models from the public model zoo.\nCode is released at https://github.com/NVlabs/T-Stitch",
        "publication_date": "2024-02-21T23:08:54Z",
        "upvotes": 8
    },
    "2402.14086": {
        "url": "https://arxiv.org/abs/2402.14086",
        "title": "LexC-Gen: Generating Data for Extremely Low-Resource Languages with\n  Large Language Models and Bilingual Lexicons",
        "authors": [
            "Zheng-Xin Yong",
            "Cristina Menghini",
            "Stephen H. Bach"
        ],
        "abstract": "Data scarcity in low-resource languages can be addressed with word-to-word\ntranslations from labeled task data in high-resource languages using bilingual\nlexicons. However, bilingual lexicons often have limited lexical overlap with\ntask data, which results in poor translation coverage and lexicon utilization.\nWe propose lexicon-conditioned data generation (LexC-Gen), a method that\ngenerates low-resource-language classification task data at scale.\nSpecifically, LexC-Gen first uses high-resource-language words from bilingual\nlexicons to generate lexicon-compatible task data, and then it translates them\ninto low-resource languages with bilingual lexicons via word translation.\nAcross 17 extremely low-resource languages, LexC-Gen generated data is\ncompetitive with expert-translated gold data, and yields on average 5.6 and 8.9\npoints improvement over existing lexicon-based word translation methods on\nsentiment analysis and topic classification tasks respectively. We show that\nconditioning on bilingual lexicons is the key component of LexC-Gen. LexC-Gen\nis also practical -- it only needs a single GPU to generate data at scale. It\nworks well with open-access LLMs, and its cost is one-fifth of the cost of\nGPT4-based multilingual data generation.",
        "publication_date": "2024-02-21T19:20:06Z",
        "upvotes": 8
    },
    "2402.14810": {
        "url": "https://arxiv.org/abs/2402.14810",
        "title": "GeneOH Diffusion: Towards Generalizable Hand-Object Interaction\n  Denoising via Denoising Diffusion",
        "authors": [
            "Xueyi Liu",
            "Li Yi"
        ],
        "abstract": "In this work, we tackle the challenging problem of denoising hand-object\ninteractions (HOI). Given an erroneous interaction sequence, the objective is\nto refine the incorrect hand trajectory to remove interaction artifacts for a\nperceptually realistic sequence. This challenge involves intricate interaction\nnoise, including unnatural hand poses and incorrect hand-object relations,\nalongside the necessity for robust generalization to new interactions and\ndiverse noise patterns. We tackle those challenges through a novel approach,\nGeneOH Diffusion, incorporating two key designs: an innovative contact-centric\nHOI representation named GeneOH and a new domain-generalizable denoising\nscheme. The contact-centric representation GeneOH informatively parameterizes\nthe HOI process, facilitating enhanced generalization across various HOI\nscenarios. The new denoising scheme consists of a canonical denoising model\ntrained to project noisy data samples from a whitened noise space to a clean\ndata manifold and a \"denoising via diffusion\" strategy which can handle input\ntrajectories with various noise patterns by first diffusing them to align with\nthe whitened noise space and cleaning via the canonical denoiser. Extensive\nexperiments on four benchmarks with significant domain variations demonstrate\nthe superior effectiveness of our method. GeneOH Diffusion also shows promise\nfor various downstream applications. Project website:\nhttps://meowuu7.github.io/GeneOH-Diffusion/.",
        "publication_date": "2024-02-22T18:59:21Z",
        "upvotes": 7
    },
    "2402.14792": {
        "url": "https://arxiv.org/abs/2402.14792",
        "title": "Consolidating Attention Features for Multi-view Image Editing",
        "authors": [
            "Or Patashnik",
            "Rinon Gal",
            "Daniel Cohen-Or",
            "Jun-Yan Zhu",
            "Fernando De la Torre"
        ],
        "abstract": "Large-scale text-to-image models enable a wide range of image editing\ntechniques, using text prompts or even spatial controls. However, applying\nthese editing methods to multi-view images depicting a single scene leads to\n3D-inconsistent results. In this work, we focus on spatial control-based\ngeometric manipulations and introduce a method to consolidate the editing\nprocess across various views. We build on two insights: (1) maintaining\nconsistent features throughout the generative process helps attain consistency\nin multi-view editing, and (2) the queries in self-attention layers\nsignificantly influence the image structure. Hence, we propose to improve the\ngeometric consistency of the edited images by enforcing the consistency of the\nqueries. To do so, we introduce QNeRF, a neural radiance field trained on the\ninternal query features of the edited images. Once trained, QNeRF can render\n3D-consistent queries, which are then softly injected back into the\nself-attention layers during generation, greatly improving multi-view\nconsistency. We refine the process through a progressive, iterative method that\nbetter consolidates queries across the diffusion timesteps. We compare our\nmethod to a range of existing techniques and demonstrate that it can achieve\nbetter multi-view consistency and higher fidelity to the input scene. These\nadvantages allow us to train NeRFs with fewer visual artifacts, that are better\naligned with the target geometry.",
        "publication_date": "2024-02-22T18:50:18Z",
        "upvotes": 7
    },
    "2402.14650": {
        "url": "https://arxiv.org/abs/2402.14650",
        "title": "GaussianPro: 3D Gaussian Splatting with Progressive Propagation",
        "authors": [
            "Kai Cheng",
            "Xiaoxiao Long",
            "Kaizhi Yang",
            "Yao Yao",
            "Wei Yin",
            "Yuexin Ma",
            "Wenping Wang",
            "Xuejin Chen"
        ],
        "abstract": "The advent of 3D Gaussian Splatting (3DGS) has recently brought about a\nrevolution in the field of neural rendering, facilitating high-quality\nrenderings at real-time speed. However, 3DGS heavily depends on the initialized\npoint cloud produced by Structure-from-Motion (SfM) techniques. When tackling\nwith large-scale scenes that unavoidably contain texture-less surfaces, the SfM\ntechniques always fail to produce enough points in these surfaces and cannot\nprovide good initialization for 3DGS. As a result, 3DGS suffers from difficult\noptimization and low-quality renderings. In this paper, inspired by classical\nmulti-view stereo (MVS) techniques, we propose GaussianPro, a novel method that\napplies a progressive propagation strategy to guide the densification of the 3D\nGaussians. Compared to the simple split and clone strategies used in 3DGS, our\nmethod leverages the priors of the existing reconstructed geometries of the\nscene and patch matching techniques to produce new Gaussians with accurate\npositions and orientations. Experiments on both large-scale and small-scale\nscenes validate the effectiveness of our method, where our method significantly\nsurpasses 3DGS on the Waymo dataset, exhibiting an improvement of 1.15dB in\nterms of PSNR.",
        "publication_date": "2024-02-22T16:00:20Z",
        "upvotes": 6
    },
    "2402.14590": {
        "url": "https://arxiv.org/abs/2402.14590",
        "title": "Scaling Up LLM Reviews for Google Ads Content Moderation",
        "authors": [
            "Wei Qiao",
            "Tushar Dogra",
            "Otilia Stretcu",
            "Yu-Han Lyu",
            "Tiantian Fang",
            "Dongjin Kwon",
            "Chun-Ta Lu",
            "Enming Luo",
            "Yuan Wang",
            "Chih-Chun Chia",
            "Ariel Fuxman",
            "Fangzhou Wang",
            "Ranjay Krishna",
            "Mehmet Tek"
        ],
        "abstract": "Large language models (LLMs) are powerful tools for content moderation, but\ntheir inference costs and latency make them prohibitive for casual use on large\ndatasets, such as the Google Ads repository. This study proposes a method for\nscaling up LLM reviews for content moderation in Google Ads. First, we use\nheuristics to select candidates via filtering and duplicate removal, and create\nclusters of ads for which we select one representative ad per cluster. We then\nuse LLMs to review only the representative ads. Finally, we propagate the LLM\ndecisions for the representative ads back to their clusters. This method\nreduces the number of reviews by more than 3 orders of magnitude while\nachieving a 2x recall compared to a baseline non-LLM model. The success of this\napproach is a strong function of the representations used in clustering and\nlabel propagation; we found that cross-modal similarity representations yield\nbetter results than uni-modal representations.",
        "publication_date": "2024-02-07T23:47:02Z",
        "upvotes": 6
    },
    "2402.14180": {
        "url": "https://arxiv.org/abs/2402.14180",
        "title": "Linear Transformers are Versatile In-Context Learners",
        "authors": [
            "Max Vladymyrov",
            "Johannes von Oswald",
            "Mark Sandler",
            "Rong Ge"
        ],
        "abstract": "Recent research has demonstrated that transformers, particularly linear\nattention models, implicitly execute gradient-descent-like algorithms on data\nprovided in-context during their forward inference step. However, their\ncapability in handling more complex problems remains unexplored. In this paper,\nwe prove that any linear transformer maintains an implicit linear model and can\nbe interpreted as performing a variant of preconditioned gradient descent. We\nalso investigate the use of linear transformers in a challenging scenario where\nthe training data is corrupted with different levels of noise. Remarkably, we\ndemonstrate that for this problem linear transformers discover an intricate and\nhighly effective optimization algorithm, surpassing or matching in performance\nmany reasonable baselines. We reverse-engineer this algorithm and show that it\nis a novel approach incorporating momentum and adaptive rescaling based on\nnoise levels. Our findings show that even linear transformers possess the\nsurprising ability to discover sophisticated optimization strategies.",
        "publication_date": "2024-02-21T23:45:57Z",
        "upvotes": 5
    },
    "2402.14253": {
        "url": "https://arxiv.org/abs/2402.14253",
        "title": "MVD$^2$: Efficient Multiview 3D Reconstruction for Multiview Diffusion",
        "authors": [
            "Xin-Yang Zheng",
            "Hao Pan",
            "Yu-Xiao Guo",
            "Xin Tong",
            "Yang Liu"
        ],
        "abstract": "As a promising 3D generation technique, multiview diffusion (MVD) has\nreceived a lot of attention due to its advantages in terms of generalizability,\nquality, and efficiency. By finetuning pretrained large image diffusion models\nwith 3D data, the MVD methods first generate multiple views of a 3D object\nbased on an image or text prompt and then reconstruct 3D shapes with multiview\n3D reconstruction. However, the sparse views and inconsistent details in the\ngenerated images make 3D reconstruction challenging. We present MVD$^2$, an\nefficient 3D reconstruction method for multiview diffusion (MVD) images.\nMVD$^2$ aggregates image features into a 3D feature volume by projection and\nconvolution and then decodes volumetric features into a 3D mesh. We train\nMVD$^2$ with 3D shape collections and MVD images prompted by rendered views of\n3D shapes. To address the discrepancy between the generated multiview images\nand ground-truth views of the 3D shapes, we design a simple-yet-efficient\nview-dependent training scheme. MVD$^2$ improves the 3D generation quality of\nMVD and is fast and robust to various MVD methods. After training, it can\nefficiently decode 3D meshes from multiview images within one second. We train\nMVD$^2$ with Zero-123++ and ObjectVerse-LVIS 3D dataset and demonstrate its\nsuperior performance in generating 3D models from multiview images generated by\ndifferent MVD methods, using both synthetic and real images as prompts.",
        "publication_date": "2024-02-22T03:39:48Z",
        "upvotes": 5
    },
    "2402.14795": {
        "url": "https://arxiv.org/abs/2402.14795",
        "title": "CyberDemo: Augmenting Simulated Human Demonstration for Real-World\n  Dexterous Manipulation",
        "authors": [
            "Jun Wang",
            "Yuzhe Qin",
            "Kaiming Kuang",
            "Yigit Korkmaz",
            "Akhilan Gurumoorthy",
            "Hao Su",
            "Xiaolong Wang"
        ],
        "abstract": "We introduce CyberDemo, a novel approach to robotic imitation learning that\nleverages simulated human demonstrations for real-world tasks. By incorporating\nextensive data augmentation in a simulated environment, CyberDemo outperforms\ntraditional in-domain real-world demonstrations when transferred to the real\nworld, handling diverse physical and visual conditions. Regardless of its\naffordability and convenience in data collection, CyberDemo outperforms\nbaseline methods in terms of success rates across various tasks and exhibits\ngeneralizability with previously unseen objects. For example, it can rotate\nnovel tetra-valve and penta-valve, despite human demonstrations only involving\ntri-valves. Our research demonstrates the significant potential of simulated\nhuman demonstrations for real-world dexterous manipulation tasks. More details\ncan be found at https://cyber-demo.github.io",
        "publication_date": "2024-02-22T18:54:32Z",
        "upvotes": 4
    },
    "2402.14194": {
        "url": "https://arxiv.org/abs/2402.14194",
        "title": "BeTAIL: Behavior Transformer Adversarial Imitation Learning from Human\n  Racing Gameplay",
        "authors": [
            "Catherine Weaver",
            "Chen Tang",
            "Ce Hao",
            "Kenta Kawamoto",
            "Masayoshi Tomizuka",
            "Wei Zhan"
        ],
        "abstract": "Imitation learning learns a policy from demonstrations without requiring\nhand-designed reward functions. In many robotic tasks, such as autonomous\nracing, imitated policies must model complex environment dynamics and human\ndecision-making. Sequence modeling is highly effective in capturing intricate\npatterns of motion sequences but struggles to adapt to new environments or\ndistribution shifts that are common in real-world robotics tasks. In contrast,\nAdversarial Imitation Learning (AIL) can mitigate this effect, but struggles\nwith sample inefficiency and handling complex motion patterns. Thus, we propose\nBeTAIL: Behavior Transformer Adversarial Imitation Learning, which combines a\nBehavior Transformer (BeT) policy from human demonstrations with online AIL.\nBeTAIL adds an AIL residual policy to the BeT policy to model the sequential\ndecision-making process of human experts and correct for out-of-distribution\nstates or shifts in environment dynamics. We test BeTAIL on three challenges\nwith expert-level demonstrations of real human gameplay in Gran Turismo Sport.\nOur proposed residual BeTAIL reduces environment interactions and improves\nracing performance and stability, even when the BeT is pretrained on different\ntracks than downstream learning. Videos and code available at:\nhttps://sites.google.com/berkeley.edu/BeTAIL/home.",
        "publication_date": "2024-02-22T00:38:43Z",
        "upvotes": 4
    },
    "2402.14905": {
        "url": "https://arxiv.org/abs/2402.14905",
        "title": "MobileLLM: Optimizing Sub-billion Parameter Language Models for\n  On-Device Use Cases",
        "authors": [
            "Zechun Liu",
            "Changsheng Zhao",
            "Forrest Iandola",
            "Chen Lai",
            "Yuandong Tian",
            "Igor Fedorov",
            "Yunyang Xiong",
            "Ernie Chang",
            "Yangyang Shi",
            "Raghuraman Krishnamoorthi",
            "Liangzhen Lai",
            "Vikas Chandra"
        ],
        "abstract": "This paper addresses the growing need for efficient large language models\n(LLMs) on mobile devices, driven by increasing cloud costs and latency\nconcerns. We focus on designing top-quality LLMs with fewer than a billion\nparameters, a practical choice for mobile deployment. Contrary to prevailing\nbelief emphasizing the pivotal role of data and parameter quantity in\ndetermining model quality, our investigation underscores the significance of\nmodel architecture for sub-billion scale LLMs. Leveraging deep and thin\narchitectures, coupled with embedding sharing and grouped-query attention\nmechanisms, we establish a strong baseline network denoted as MobileLLM, which\nattains a remarkable 2.7%/4.3% accuracy boost over preceding 125M/350M\nstate-of-the-art models. Additionally, we propose an immediate block-wise\nweight sharing approach with no increase in model size and only marginal\nlatency overhead. The resultant models, denoted as MobileLLM-LS, demonstrate a\nfurther accuracy enhancement of 0.7%/0.8% than MobileLLM 125M/350M. Moreover,\nMobileLLM model family shows significant improvements compared to previous\nsub-billion models on chat benchmarks, and demonstrates close correctness to\nLLaMA-v2 7B in API calling tasks, highlighting the capability of small models\nfor common on-device use cases.",
        "publication_date": "2024-02-22T18:58:55Z",
        "upvotes": 75
    },
    "2402.15391": {
        "url": "https://arxiv.org/abs/2402.15391",
        "title": "Genie: Generative Interactive Environments",
        "authors": [
            "Jake Bruce",
            "Michael Dennis",
            "Ashley Edwards",
            "Jack Parker-Holder",
            "Yuge Shi",
            "Edward Hughes",
            "Matthew Lai",
            "Aditi Mavalankar",
            "Richie Steigerwald",
            "Chris Apps",
            "Yusuf Aytar",
            "Sarah Bechtle",
            "Feryal Behbahani",
            "Stephanie Chan",
            "Nicolas Heess",
            "Lucy Gonzalez",
            "Simon Osindero",
            "Sherjil Ozair",
            "Scott Reed",
            "Jingwei Zhang",
            "Konrad Zolna",
            "Jeff Clune",
            "Nando de Freitas",
            "Satinder Singh",
            "Tim Rockt\u00e4schel"
        ],
        "abstract": "We introduce Genie, the first generative interactive environment trained in\nan unsupervised manner from unlabelled Internet videos. The model can be\nprompted to generate an endless variety of action-controllable virtual worlds\ndescribed through text, synthetic images, photographs, and even sketches. At\n11B parameters, Genie can be considered a foundation world model. It is\ncomprised of a spatiotemporal video tokenizer, an autoregressive dynamics\nmodel, and a simple and scalable latent action model. Genie enables users to\nact in the generated environments on a frame-by-frame basis despite training\nwithout any ground-truth action labels or other domain-specific requirements\ntypically found in the world model literature. Further the resulting learned\nlatent action space facilitates training agents to imitate behaviors from\nunseen videos, opening the path for training generalist agents of the future.",
        "publication_date": "2024-02-23T15:47:26Z",
        "upvotes": 67
    },
    "2402.14830": {
        "url": "https://arxiv.org/abs/2402.14830",
        "title": "Orca-Math: Unlocking the potential of SLMs in Grade School Math",
        "authors": [
            "Arindam Mitra",
            "Hamed Khanpour",
            "Corby Rosset",
            "Ahmed Awadallah"
        ],
        "abstract": "Mathematical word problem-solving has long been recognized as a complex task\nfor small language models (SLMs). A recent study hypothesized that the smallest\nmodel size, needed to achieve over 80% accuracy on the GSM8K benchmark, is 34\nbillion parameters. To reach this level of performance with smaller models,\nresearcher often train SLMs to generate Python code or use tools to help avoid\ncalculation errors. Additionally, they employ ensembling, where outputs of up\nto 100 model runs are combined to arrive at a more accurate result. Result\nselection is done using consensus, majority vote or a separate a verifier model\nused in conjunction with the SLM. Ensembling provides a substantial boost in\naccuracy but at a significant cost increase with multiple calls to the model\n(e.g., Phi-GSM uses top-48 to boost the performance from 68.2 to 81.5).\n  In this work, we present Orca-Math, a 7-billion-parameter SLM based on the\nMistral-7B, which achieves 86.81% on GSM8k without the need for multiple model\ncalls or the use of verifiers, code execution or any other external tools. Our\napproach has the following key elements: (1) A high quality synthetic dataset\nof 200K math problems created using a multi-agent setup where agents\ncollaborate to create the data, (2) An iterative learning techniques that\nenables the SLM to practice solving problems, receive feedback on its solutions\nand learn from preference pairs incorporating the SLM solutions and the\nfeedback. When trained with Supervised Fine-Tuning alone, Orca-Math achieves\n81.50% on GSM8k pass@1 metric. With iterative preference learning, Orca-Math\nachieves 86.81% pass@1. Orca-Math surpasses the performance of significantly\nlarger models such as LLAMA-2-70B, WizardMath-70B, Gemini-Pro, ChatGPT-3.5. It\nalso significantly outperforms other smaller models while using much smaller\ndata (hundreds of thousands vs. millions of problems).",
        "publication_date": "2024-02-16T23:44:38Z",
        "upvotes": 23
    },
    "2402.15000": {
        "url": "https://arxiv.org/abs/2402.15000",
        "title": "Divide-or-Conquer? Which Part Should You Distill Your LLM?",
        "authors": [
            "Zhuofeng Wu",
            "He Bai",
            "Aonan Zhang",
            "Jiatao Gu",
            "VG Vinod Vydiswaran",
            "Navdeep Jaitly",
            "Yizhe Zhang"
        ],
        "abstract": "Recent methods have demonstrated that Large Language Models (LLMs) can solve\nreasoning tasks better when they are encouraged to solve subtasks of the main\ntask first. In this paper we devise a similar strategy that breaks down\nreasoning tasks into a problem decomposition phase and a problem solving phase\nand show that the strategy is able to outperform a single stage solution.\nFurther, we hypothesize that the decomposition should be easier to distill into\na smaller model compared to the problem solving because the latter requires\nlarge amounts of domain knowledge while the former only requires learning\ngeneral problem solving strategies. We propose methods to distill these two\ncapabilities and evaluate their impact on reasoning outcomes and inference\ncost. We find that we can distill the problem decomposition phase and at the\nsame time achieve good generalization across tasks, datasets, and models.\nHowever, it is harder to distill the problem solving capability without losing\nperformance and the resulting distilled model struggles with generalization.\nThese results indicate that by using smaller, distilled problem decomposition\nmodels in combination with problem solving LLMs we can achieve reasoning with\ncost-efficient inference and local adaptation.",
        "publication_date": "2024-02-22T22:28:46Z",
        "upvotes": 22
    },
    "2402.14904": {
        "url": "https://arxiv.org/abs/2402.14904",
        "title": "Watermarking Makes Language Models Radioactive",
        "authors": [
            "Tom Sander",
            "Pierre Fernandez",
            "Alain Durmus",
            "Matthijs Douze",
            "Teddy Furon"
        ],
        "abstract": "This paper investigates the radioactivity of LLM-generated texts, i.e.\nwhether it is possible to detect that such input was used as training data.\nConventional methods like membership inference can carry out this detection\nwith some level of accuracy. We show that watermarked training data leaves\ntraces easier to detect and much more reliable than membership inference. We\nlink the contamination level to the watermark robustness, its proportion in the\ntraining set, and the fine-tuning process. We notably demonstrate that training\non watermarked synthetic instructions can be detected with high confidence\n(p-value < 1e-5) even when as little as 5% of training text is watermarked.\nThus, LLM watermarking, originally designed for detecting machine-generated\ntext, gives the ability to easily identify if the outputs of a watermarked LLM\nwere used to fine-tune another LLM.",
        "publication_date": "2024-02-22T18:55:22Z",
        "upvotes": 21
    },
    "2402.15319": {
        "url": "https://arxiv.org/abs/2402.15319",
        "title": "GPTVQ: The Blessing of Dimensionality for LLM Quantization",
        "authors": [
            "Mart van Baalen",
            "Andrey Kuzmin",
            "Markus Nagel",
            "Peter Couperus",
            "Cedric Bastoul",
            "Eric Mahurin",
            "Tijmen Blankevoort",
            "Paul Whatmough"
        ],
        "abstract": "In this work we show that the size versus accuracy trade-off of neural\nnetwork quantization can be significantly improved by increasing the\nquantization dimensionality. We propose the GPTVQ method, a new fast method for\npost-training vector quantization (VQ) that scales well to Large Language\nModels (LLMs). Our method interleaves quantization of one or more columns with\nupdates to the remaining unquantized weights, using information from the\nHessian of the per-layer output reconstruction MSE. Quantization codebooks are\ninitialized using an efficient data-aware version of the EM algorithm. The\ncodebooks are then updated, and further compressed by using integer\nquantization and SVD-based compression. GPTVQ establishes a new state-of-the\nart in the size vs accuracy trade-offs on a wide range of LLMs such as Llama-v2\nand Mistral. Furthermore, our method is efficient: on a single H100 it takes\nbetween 3 and 11 hours to process a Llamav2-70B model, depending on\nquantization setting. Lastly, with on-device timings for VQ decompression on a\nmobile CPU we show that VQ leads to improved latency compared to using a 4-bit\ninteger format.",
        "publication_date": "2024-02-23T13:39:16Z",
        "upvotes": 19
    },
    "2402.15504": {
        "url": "https://arxiv.org/abs/2402.15504",
        "title": "Gen4Gen: Generative Data Pipeline for Generative Multi-Concept\n  Composition",
        "authors": [
            "Chun-Hsiao Yeh",
            "Ta-Ying Cheng",
            "He-Yen Hsieh",
            "Chuan-En Lin",
            "Yi Ma",
            "Andrew Markham",
            "Niki Trigoni",
            "H. T. Kung",
            "Yubei Chen"
        ],
        "abstract": "Recent text-to-image diffusion models are able to learn and synthesize images\ncontaining novel, personalized concepts (e.g., their own pets or specific\nitems) with just a few examples for training. This paper tackles two\ninterconnected issues within this realm of personalizing text-to-image\ndiffusion models. First, current personalization techniques fail to reliably\nextend to multiple concepts -- we hypothesize this to be due to the mismatch\nbetween complex scenes and simple text descriptions in the pre-training dataset\n(e.g., LAION). Second, given an image containing multiple personalized\nconcepts, there lacks a holistic metric that evaluates performance on not just\nthe degree of resemblance of personalized concepts, but also whether all\nconcepts are present in the image and whether the image accurately reflects the\noverall text description. To address these issues, we introduce Gen4Gen, a\nsemi-automated dataset creation pipeline utilizing generative models to combine\npersonalized concepts into complex compositions along with text-descriptions.\nUsing this, we create a dataset called MyCanvas, that can be used to benchmark\nthe task of multi-concept personalization. In addition, we design a\ncomprehensive metric comprising two scores (CP-CLIP and TI-CLIP) for better\nquantifying the performance of multi-concept, personalized text-to-image\ndiffusion methods. We provide a simple baseline built on top of Custom\nDiffusion with empirical prompting strategies for future researchers to\nevaluate on MyCanvas. We show that by improving data quality and prompting\nstrategies, we can significantly increase multi-concept personalized image\ngeneration quality, without requiring any modifications to model architecture\nor training algorithms.",
        "publication_date": "2024-02-23T18:55:09Z",
        "upvotes": 19
    },
    "2402.15220": {
        "url": "https://arxiv.org/abs/2402.15220",
        "title": "ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and\n  Two-Phase Partition",
        "authors": [
            "Lu Ye",
            "Ze Tao",
            "Yong Huang",
            "Yang Li"
        ],
        "abstract": "Self-attention is an essential component of large language models(LLMs) but a\nsignificant source of inference latency for long sequences. In multi-tenant\nLLMs serving scenarios, the compute and memory operation cost of self-attention\ncan be optimized by using the probability that multiple LLM requests have\nshared system prompts in prefixes. In this paper, we introduce ChunkAttention,\na prefix-aware self-attention module that can detect matching prompt prefixes\nacross multiple requests and share their key/value tensors in memory at runtime\nto improve the memory utilization of KV cache. This is achieved by breaking\nmonolithic key/value tensors into smaller chunks and structuring them into the\nauxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,\nwe design an efficient self-attention kernel, where a two-phase partition\nalgorithm is implemented to improve the data locality during self-attention\ncomputation in the presence of shared system prompts. Experiments show that\nChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$\ncompared to the start-of-the-art implementation, with the length of the system\nprompt ranging from 1024 to 4096.",
        "publication_date": "2024-02-23T09:29:19Z",
        "upvotes": 18
    },
    "2402.14848": {
        "url": "https://arxiv.org/abs/2402.14848",
        "title": "Same Task, More Tokens: the Impact of Input Length on the Reasoning\n  Performance of Large Language Models",
        "authors": [
            "Mosh Levy",
            "Alon Jacoby",
            "Yoav Goldberg"
        ],
        "abstract": "This paper explores the impact of extending input lengths on the capabilities\nof Large Language Models (LLMs). Despite LLMs advancements in recent times,\ntheir performance consistency across different input lengths is not well\nunderstood. We investigate this aspect by introducing a novel QA reasoning\nframework, specifically designed to assess the impact of input length. We\nisolate the effect of input length using multiple versions of the same sample,\neach being extended with padding of different lengths, types and locations. Our\nfindings show a notable degradation in LLMs' reasoning performance at much\nshorter input lengths than their technical maximum. We show that the\ndegradation trend appears in every version of our dataset, although at\ndifferent intensities. Additionally, our study reveals that traditional\nperplexity metrics do not correlate with performance of LLMs' in long input\nreasoning tasks. We analyse our results and identify failure modes that can\nserve as useful guides for future research, potentially informing strategies to\naddress the limitations observed in LLMs.",
        "publication_date": "2024-02-19T16:04:53Z",
        "upvotes": 17
    },
    "2402.15491": {
        "url": "https://arxiv.org/abs/2402.15491",
        "title": "API-BLEND: A Comprehensive Corpora for Training and Benchmarking API\n  LLMs",
        "authors": [
            "Kinjal Basu",
            "Ibrahim Abdelaziz",
            "Subhajit Chaudhury",
            "Soham Dan",
            "Maxwell Crouse",
            "Asim Munawar",
            "Sadhana Kumaravel",
            "Vinod Muthusamy",
            "Pavan Kapanipathi",
            "Luis A. Lastras"
        ],
        "abstract": "There is a growing need for Large Language Models (LLMs) to effectively use\ntools and external Application Programming Interfaces (APIs) to plan and\ncomplete tasks. As such, there is tremendous interest in methods that can\nacquire sufficient quantities of train and test data that involve calls to\ntools / APIs. Two lines of research have emerged as the predominant strategies\nfor addressing this challenge. The first has focused on synthetic data\ngeneration techniques, while the second has involved curating task-adjacent\ndatasets which can be transformed into API / Tool-based tasks. In this paper,\nwe focus on the task of identifying, curating, and transforming existing\ndatasets and, in turn, introduce API-BLEND, a large corpora for training and\nsystematic testing of tool-augmented LLMs. The datasets mimic real-world\nscenarios involving API-tasks such as API / tool detection, slot filling, and\nsequencing of the detected APIs. We demonstrate the utility of the API-BLEND\ndataset for both training and benchmarking purposes.",
        "publication_date": "2024-02-23T18:30:49Z",
        "upvotes": 13
    },
    "2402.15509": {
        "url": "https://arxiv.org/abs/2402.15509",
        "title": "Seamless Human Motion Composition with Blended Positional Encodings",
        "authors": [
            "German Barquero",
            "Sergio Escalera",
            "Cristina Palmero"
        ],
        "abstract": "Conditional human motion generation is an important topic with many\napplications in virtual reality, gaming, and robotics. While prior works have\nfocused on generating motion guided by text, music, or scenes, these typically\nresult in isolated motions confined to short durations. Instead, we address the\ngeneration of long, continuous sequences guided by a series of varying textual\ndescriptions. In this context, we introduce FlowMDM, the first diffusion-based\nmodel that generates seamless Human Motion Compositions (HMC) without any\npostprocessing or redundant denoising steps. For this, we introduce the Blended\nPositional Encodings, a technique that leverages both absolute and relative\npositional encodings in the denoising chain. More specifically, global motion\ncoherence is recovered at the absolute stage, whereas smooth and realistic\ntransitions are built at the relative stage. As a result, we achieve\nstate-of-the-art results in terms of accuracy, realism, and smoothness on the\nBabel and HumanML3D datasets. FlowMDM excels when trained with only a single\ndescription per motion sequence thanks to its Pose-Centric Cross-ATtention,\nwhich makes it robust against varying text descriptions at inference time.\nFinally, to address the limitations of existing HMC metrics, we propose two new\nmetrics: the Peak Jerk and the Area Under the Jerk, to detect abrupt\ntransitions.",
        "publication_date": "2024-02-23T18:59:40Z",
        "upvotes": 12
    },
    "2402.15021": {
        "url": "https://arxiv.org/abs/2402.15021",
        "title": "CLoVe: Encoding Compositional Language in Contrastive Vision-Language\n  Models",
        "authors": [
            "Santiago Castro",
            "Amir Ziai",
            "Avneesh Saluja",
            "Zhuoning Yuan",
            "Rada Mihalcea"
        ],
        "abstract": "Recent years have witnessed a significant increase in the performance of\nVision and Language tasks. Foundational Vision-Language Models (VLMs), such as\nCLIP, have been leveraged in multiple settings and demonstrated remarkable\nperformance across several tasks. Such models excel at object-centric\nrecognition yet learn text representations that seem invariant to word order,\nfailing to compose known concepts in novel ways. However, no evidence exists\nthat any VLM, including large-scale single-stream models such as GPT-4V,\nidentifies compositions successfully. In this paper, we introduce a framework\nto significantly improve the ability of existing models to encode compositional\nlanguage, with over 10% absolute improvement on compositionality benchmarks,\nwhile maintaining or improving the performance on standard object-recognition\nand retrieval benchmarks. Our code and pre-trained models are publicly\navailable at https://github.com/netflix/clove.",
        "publication_date": "2024-02-22T23:42:25Z",
        "upvotes": 11
    },
    "2402.15506": {
        "url": "https://arxiv.org/abs/2402.15506",
        "title": "AgentOhana: Design Unified Data and Training Pipeline for Effective\n  Agent Learning",
        "authors": [
            "Jianguo Zhang",
            "Tian Lan",
            "Rithesh Murthy",
            "Zhiwei Liu",
            "Weiran Yao",
            "Juntao Tan",
            "Thai Hoang",
            "Liangwei Yang",
            "Yihao Feng",
            "Zuxin Liu",
            "Tulika Awalgaonkar",
            "Juan Carlos Niebles",
            "Silvio Savarese",
            "Shelby Heinecke",
            "Huan Wang",
            "Caiming Xiong"
        ],
        "abstract": "Autonomous agents powered by large language models (LLMs) have garnered\nsignificant research attention. However, fully harnessing the potential of LLMs\nfor agent-based tasks presents inherent challenges due to the heterogeneous\nnature of diverse data sources featuring multi-turn trajectories. In this\npaper, we introduce \\textbf{AgentOhana} as a comprehensive solution to address\nthese challenges. \\textit{AgentOhana} aggregates agent trajectories from\ndistinct environments, spanning a wide array of scenarios. It meticulously\nstandardizes and unifies these trajectories into a consistent format,\nstreamlining the creation of a generic data loader optimized for agent\ntraining. Leveraging the data unification, our training pipeline maintains\nequilibrium across different data sources and preserves independent randomness\nacross devices during dataset partitioning and model training. Additionally, we\npresent \\textbf{xLAM-v0.1}, a large action model tailored for AI agents, which\ndemonstrates exceptional performance across various benchmarks. Begin the\nexploration at \\url{https://github.com/SalesforceAIResearch/xLAM}.",
        "publication_date": "2024-02-23T18:56:26Z",
        "upvotes": 10
    },
    "2402.16153": {
        "url": "https://arxiv.org/abs/2402.16153",
        "title": "ChatMusician: Understanding and Generating Music Intrinsically with LLM",
        "authors": [
            "Ruibin Yuan",
            "Hanfeng Lin",
            "Yi Wang",
            "Zeyue Tian",
            "Shangda Wu",
            "Tianhao Shen",
            "Ge Zhang",
            "Yuhang Wu",
            "Cong Liu",
            "Ziya Zhou",
            "Ziyang Ma",
            "Liumeng Xue",
            "Ziyu Wang",
            "Qin Liu",
            "Tianyu Zheng",
            "Yizhi Li",
            "Yinghao Ma",
            "Yiming Liang",
            "Xiaowei Chi",
            "Ruibo Liu",
            "Zili Wang",
            "Pengfei Li",
            "Jingcheng Wu",
            "Chenghua Lin",
            "Qifeng Liu",
            "Tao Jiang",
            "Wenhao Huang",
            "Wenhu Chen",
            "Emmanouil Benetos",
            "Jie Fu",
            "Gus Xia",
            "Roger Dannenberg",
            "Wei Xue",
            "Shiyin Kang",
            "Yike Guo"
        ],
        "abstract": "While Large Language Models (LLMs) demonstrate impressive capabilities in\ntext generation, we find that their ability has yet to be generalized to music,\nhumanity's creative language. We introduce ChatMusician, an open-source LLM\nthat integrates intrinsic musical abilities. It is based on continual\npre-training and finetuning LLaMA2 on a text-compatible music representation,\nABC notation, and the music is treated as a second language. ChatMusician can\nunderstand and generate music with a pure text tokenizer without any external\nmulti-modal neural structures or tokenizers. Interestingly, endowing musical\nabilities does not harm language abilities, even achieving a slightly higher\nMMLU score. Our model is capable of composing well-structured, full-length\nmusic, conditioned on texts, chords, melodies, motifs, musical forms, etc,\nsurpassing GPT-4 baseline. On our meticulously curated college-level music\nunderstanding benchmark, MusicTheoryBench, ChatMusician surpasses LLaMA2 and\nGPT-3.5 on zero-shot setting by a noticeable margin. Our work reveals that LLMs\ncan be an excellent compressor for music, but there remains significant\nterritory to be conquered. We release our 4B token music-language corpora\nMusicPile, the collected MusicTheoryBench, code, model and demo in GitHub.",
        "publication_date": "2024-02-25T17:19:41Z",
        "upvotes": 52
    },
    "2402.16819": {
        "url": "https://arxiv.org/abs/2402.16819",
        "title": "Nemotron-4 15B Technical Report",
        "authors": [
            "Jupinder Parmar",
            "Shrimai Prabhumoye",
            "Joseph Jennings",
            "Mostofa Patwary",
            "Sandeep Subramanian",
            "Dan Su",
            "Chen Zhu",
            "Deepak Narayanan",
            "Aastha Jhunjhunwala",
            "Ayush Dattagupta",
            "Vibhu Jawa",
            "Jiwei Liu",
            "Ameya Mahabaleshwarkar",
            "Osvald Nitski",
            "Annika Brundyn",
            "James Maki",
            "Miguel Martinez",
            "Jiaxuan You",
            "John Kamalu",
            "Patrick LeGresley",
            "Denys Fridman",
            "Jared Casper",
            "Ashwath Aithal",
            "Oleksii Kuchaiev",
            "Mohammad Shoeybi",
            "Jonathan Cohen",
            "Bryan Catanzaro"
        ],
        "abstract": "We introduce Nemotron-4 15B, a 15-billion-parameter large multilingual\nlanguage model trained on 8 trillion text tokens. Nemotron-4 15B demonstrates\nstrong performance when assessed on English, multilingual, and coding tasks: it\noutperforms all existing similarly-sized open models on 4 out of 7 downstream\nevaluation areas and achieves competitive performance to the leading open\nmodels in the remaining ones. Specifically, Nemotron-4 15B exhibits the best\nmultilingual capabilities of all similarly-sized models, even outperforming\nmodels over four times larger and those explicitly specialized for multilingual\ntasks.",
        "publication_date": "2024-02-26T18:43:45Z",
        "upvotes": 40
    },
    "2402.16107": {
        "url": "https://arxiv.org/abs/2402.16107",
        "title": "FuseChat: Knowledge Fusion of Chat Models",
        "authors": [
            "Fanqi Wan",
            "Ziyi Yang",
            "Longguang Zhong",
            "Xiaojun Quan",
            "Xinting Huang",
            "Wei Bi"
        ],
        "abstract": "While training large language models (LLMs) from scratch can indeed lead to\nmodels with distinct capabilities and strengths, this approach incurs\nsubstantial costs and may lead to potential redundancy in competencies. An\nalternative strategy is to combine existing LLMs into a more robust LLM,\nthereby diminishing the necessity for expensive pre-training. However, due to\nthe diverse architectures of LLMs, direct parameter blending proves to be\nunfeasible. Recently, \\textsc{FuseLLM} introduced the concept of knowledge\nfusion to transfer the collective knowledge of multiple structurally varied\nLLMs into a target LLM through lightweight continual training. In this report,\nwe extend the scalability and flexibility of the \\textsc{FuseLLM} framework to\nrealize the fusion of chat LLMs, resulting in \\textsc{FuseChat}.\n\\textsc{FuseChat} comprises two main stages. Firstly, we undertake knowledge\nfusion for structurally and scale-varied source LLMs to derive multiple target\nLLMs of identical structure and size via lightweight fine-tuning. Then, these\ntarget LLMs are merged within the parameter space, wherein we propose a novel\nmethod for determining the merging weights based on the variation ratio of\nparameter matrices before and after fine-tuning. We validate our approach using\nthree prominent chat LLMs with diverse architectures and scales, namely\n\\texttt{NH2-Mixtral-8x7B}, \\texttt{NH2-Solar-10.7B}, and\n\\texttt{OpenChat-3.5-7B}. Experimental results spanning various chat domains\ndemonstrate the superiority of \\texttt{\\textsc{FuseChat}-7B} across a broad\nspectrum of chat LLMs at 7B and 34B scales, even surpassing \\texttt{GPT-3.5\n(March)} and approaching \\texttt{Mixtral-8x7B-Instruct}. Our code, model\nweights, and data are openly accessible at\n\\url{https://github.com/fanqiwan/FuseLLM}.",
        "publication_date": "2024-02-25T15:11:58Z",
        "upvotes": 34
    },
    "2402.15627": {
        "url": "https://arxiv.org/abs/2402.15627",
        "title": "MegaScale: Scaling Large Language Model Training to More Than 10,000\n  GPUs",
        "authors": [
            "Ziheng Jiang",
            "Haibin Lin",
            "Yinmin Zhong",
            "Qi Huang",
            "Yangrui Chen",
            "Zhi Zhang",
            "Yanghua Peng",
            "Xiang Li",
            "Cong Xie",
            "Shibiao Nong",
            "Yulu Jia",
            "Sun He",
            "Hongmin Chen",
            "Zhihao Bai",
            "Qi Hou",
            "Shipeng Yan",
            "Ding Zhou",
            "Yiyao Sheng",
            "Zhuo Jiang",
            "Haohan Xu",
            "Haoran Wei",
            "Zhang Zhang",
            "Pengfei Nie",
            "Leqi Zou",
            "Sida Zhao",
            "Liang Xiang",
            "Zherui Liu",
            "Zhe Li",
            "Xiaoying Jia",
            "Jianxi Ye",
            "Xin Jin",
            "Xin Liu"
        ],
        "abstract": "We present the design, implementation and engineering experience in building\nand deploying MegaScale, a production system for training large language models\n(LLMs) at the scale of more than 10,000 GPUs. Training LLMs at this scale\nbrings unprecedented challenges to training efficiency and stability. We take a\nfull-stack approach that co-designs the algorithmic and system components\nacross model block and optimizer design, computation and communication\noverlapping, operator optimization, data pipeline, and network performance\ntuning. Maintaining high efficiency throughout the training process (i.e.,\nstability) is an important consideration in production given the long extent of\nLLM training jobs. Many hard stability issues only emerge at large scale, and\nin-depth observability is the key to address them. We develop a set of\ndiagnosis tools to monitor system components and events deep in the stack,\nidentify root causes, and derive effective techniques to achieve fault\ntolerance and mitigate stragglers. MegaScale achieves 55.2% Model FLOPs\nUtilization (MFU) when training a 175B LLM model on 12,288 GPUs, improving the\nMFU by 1.34x compared to Megatron-LM. We share our operational experience in\nidentifying and fixing failures and stragglers. We hope by articulating the\nproblems and sharing our experience from a systems perspective, this work can\ninspire future LLM systems research.",
        "publication_date": "2024-02-23T22:10:59Z",
        "upvotes": 28
    },
    "2402.16843": {
        "url": "https://arxiv.org/abs/2402.16843",
        "title": "Multi-LoRA Composition for Image Generation",
        "authors": [
            "Ming Zhong",
            "Yelong Shen",
            "Shuohang Wang",
            "Yadong Lu",
            "Yizhu Jiao",
            "Siru Ouyang",
            "Donghan Yu",
            "Jiawei Han",
            "Weizhu Chen"
        ],
        "abstract": "Low-Rank Adaptation (LoRA) is extensively utilized in text-to-image models\nfor the accurate rendition of specific elements like distinct characters or\nunique styles in generated images. Nonetheless, existing methods face\nchallenges in effectively composing multiple LoRAs, especially as the number of\nLoRAs to be integrated grows, thus hindering the creation of complex imagery.\nIn this paper, we study multi-LoRA composition through a decoding-centric\nperspective. We present two training-free methods: LoRA Switch, which\nalternates between different LoRAs at each denoising step, and LoRA Composite,\nwhich simultaneously incorporates all LoRAs to guide more cohesive image\nsynthesis. To evaluate the proposed approaches, we establish ComposLoRA, a new\ncomprehensive testbed as part of this research. It features a diverse range of\nLoRA categories with 480 composition sets. Utilizing an evaluation framework\nbased on GPT-4V, our findings demonstrate a clear improvement in performance\nwith our methods over the prevalent baseline, particularly evident when\nincreasing the number of LoRAs in a composition.",
        "publication_date": "2024-02-26T18:59:18Z",
        "upvotes": 28
    },
    "2402.16671": {
        "url": "https://arxiv.org/abs/2402.16671",
        "title": "StructLM: Towards Building Generalist Models for Structured Knowledge\n  Grounding",
        "authors": [
            "Alex Zhuang",
            "Ge Zhang",
            "Tianyu Zheng",
            "Xinrun Du",
            "Junjie Wang",
            "Weiming Ren",
            "Stephen W. Huang",
            "Jie Fu",
            "Xiang Yue",
            "Wenhu Chen"
        ],
        "abstract": "Structured data sources, such as tables, graphs, and databases, are\nubiquitous knowledge sources. Despite the demonstrated capabilities of large\nlanguage models (LLMs) on plain text, their proficiency in interpreting and\nutilizing structured data remains limited. Our investigation reveals a notable\ndeficiency in LLMs' ability to process structured data, e.g., ChatGPT lags\nbehind state-of-the-art (SoTA) model by an average of 35%. To augment the\nStructured Knowledge Grounding (SKG) capabilities in LLMs, we have developed a\ncomprehensive instruction tuning dataset comprising 1.1 million examples.\nUtilizing this dataset, we train a series of models, referred to as StructLM,\nbased on the Code-LLaMA architecture, ranging from 7B to 34B parameters. Our\nStructLM series surpasses task-specific models on 14 out of 18 evaluated\ndatasets and establishes new SoTA achievements on 7 SKG tasks. Furthermore,\nStructLM demonstrates strong generalization across 6 novel held-out SKG tasks,\noutperforming TableLlama by an average of 35\\% and Flan-UL2 20B by an average\nof 10\\%. Contrary to expectations, we observe that scaling model size offers\nmarginal benefits, with StructLM-34B showing only slight improvements over\nStructLM-7B. This suggests that structured knowledge grounding is still a\nchallenging task and requires more innovative design to push to a new level.",
        "publication_date": "2024-02-26T15:47:01Z",
        "upvotes": 26
    },
    "2402.16840": {
        "url": "https://arxiv.org/abs/2402.16840",
        "title": "MobiLlama: Towards Accurate and Lightweight Fully Transparent GPT",
        "authors": [
            "Omkar Thawakar",
            "Ashmal Vayani",
            "Salman Khan",
            "Hisham Cholakal",
            "Rao M. Anwer",
            "Michael Felsberg",
            "Tim Baldwin",
            "Eric P. Xing",
            "Fahad Shahbaz Khan"
        ],
        "abstract": "\"Bigger the better\" has been the predominant trend in recent Large Language\nModels (LLMs) development. However, LLMs do not suit well for scenarios that\nrequire on-device processing, energy efficiency, low memory footprint, and\nresponse efficiency. These requisites are crucial for privacy, security, and\nsustainable deployment. This paper explores the \"less is more\" paradigm by\naddressing the challenge of designing accurate yet efficient Small Language\nModels (SLMs) for resource constrained devices. Our primary contribution is the\nintroduction of an accurate and fully transparent open-source 0.5 billion\n(0.5B) parameter SLM, named MobiLlama, catering to the specific needs of\nresource-constrained computing with an emphasis on enhanced performance with\nreduced resource demands. MobiLlama is a SLM design that initiates from a\nlarger model and applies a careful parameter sharing scheme to reduce both the\npre-training and the deployment cost. Our work strives to not only bridge the\ngap in open-source SLMs but also ensures full transparency, where complete\ntraining data pipeline, training code, model weights, and over 300 checkpoints\nalong with evaluation codes is available at :\nhttps://github.com/mbzuai-oryx/MobiLlama.",
        "publication_date": "2024-02-26T18:59:03Z",
        "upvotes": 23
    },
    "2402.16837": {
        "url": "https://arxiv.org/abs/2402.16837",
        "title": "Do Large Language Models Latently Perform Multi-Hop Reasoning?",
        "authors": [
            "Sohee Yang",
            "Elena Gribovskaya",
            "Nora Kassner",
            "Mor Geva",
            "Sebastian Riedel"
        ],
        "abstract": "We study whether Large Language Models (LLMs) latently perform multi-hop\nreasoning with complex prompts such as \"The mother of the singer of\n'Superstition' is\". We look for evidence of a latent reasoning pathway where an\nLLM (1) latently identifies \"the singer of 'Superstition'\" as Stevie Wonder,\nthe bridge entity, and (2) uses its knowledge of Stevie Wonder's mother to\ncomplete the prompt. We analyze these two hops individually and consider their\nco-occurrence as indicative of latent multi-hop reasoning. For the first hop,\nwe test if changing the prompt to indirectly mention the bridge entity instead\nof any other entity increases the LLM's internal recall of the bridge entity.\nFor the second hop, we test if increasing this recall causes the LLM to better\nutilize what it knows about the bridge entity. We find strong evidence of\nlatent multi-hop reasoning for the prompts of certain relation types, with the\nreasoning pathway used in more than 80% of the prompts. However, the\nutilization is highly contextual, varying across different types of prompts.\nAlso, on average, the evidence for the second hop and the full multi-hop\ntraversal is rather moderate and only substantial for the first hop. Moreover,\nwe find a clear scaling trend with increasing model size for the first hop of\nreasoning but not for the second hop. Our experimental findings suggest\npotential challenges and opportunities for future development and applications\nof LLMs.",
        "publication_date": "2024-02-26T18:57:54Z",
        "upvotes": 23
    },
    "2402.16822": {
        "url": "https://arxiv.org/abs/2402.16822",
        "title": "Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts",
        "authors": [
            "Mikayel Samvelyan",
            "Sharath Chandra Raparthy",
            "Andrei Lupu",
            "Eric Hambro",
            "Aram H. Markosyan",
            "Manish Bhatt",
            "Yuning Mao",
            "Minqi Jiang",
            "Jack Parker-Holder",
            "Jakob Foerster",
            "Tim Rockt\u00e4schel",
            "Roberta Raileanu"
        ],
        "abstract": "As large language models (LLMs) become increasingly prevalent across many\nreal-world applications, understanding and enhancing their robustness to user\ninputs is of paramount importance. Existing methods for identifying adversarial\nprompts tend to focus on specific domains, lack diversity, or require extensive\nhuman annotations. To address these limitations, we present Rainbow Teaming, a\nnovel approach for producing a diverse collection of adversarial prompts.\nRainbow Teaming casts adversarial prompt generation as a quality-diversity\nproblem, and uses open-ended search to generate prompts that are both effective\nand diverse. It can uncover a model's vulnerabilities across a broad range of\ndomains including, in this paper, safety, question answering, and\ncybersecurity. We also demonstrate that fine-tuning on synthetic data generated\nby Rainbow Teaming improves the safety of state-of-the-art LLMs without hurting\ntheir general capabilities and helpfulness, paving the path to open-ended\nself-improvement.",
        "publication_date": "2024-02-26T18:47:27Z",
        "upvotes": 15
    },
    "2402.16641": {
        "url": "https://arxiv.org/abs/2402.16641",
        "title": "Towards Open-ended Visual Quality Comparison",
        "authors": [
            "Haoning Wu",
            "Hanwei Zhu",
            "Zicheng Zhang",
            "Erli Zhang",
            "Chaofeng Chen",
            "Liang Liao",
            "Chunyi Li",
            "Annan Wang",
            "Wenxiu Sun",
            "Qiong Yan",
            "Xiaohong Liu",
            "Guangtao Zhai",
            "Shiqi Wang",
            "Weisi Lin"
        ],
        "abstract": "Comparative settings (e.g. pairwise choice, listwise ranking) have been\nadopted by a wide range of subjective studies for image quality assessment\n(IQA), as it inherently standardizes the evaluation criteria across different\nobservers and offer more clear-cut responses. In this work, we extend the edge\nof emerging large multi-modality models (LMMs) to further advance visual\nquality comparison into open-ended settings, that 1) can respond to open-range\nquestions on quality comparison; 2) can provide detailed reasonings beyond\ndirect answers. To this end, we propose the Co-Instruct. To train this\nfirst-of-its-kind open-source open-ended visual quality comparer, we collect\nthe Co-Instruct-562K dataset, from two sources: (a) LLM-merged single image\nquality description, (b) GPT-4V \"teacher\" responses on unlabeled data.\nFurthermore, to better evaluate this setting, we propose the MICBench, the\nfirst benchmark on multi-image comparison for LMMs. We demonstrate that\nCo-Instruct not only achieves in average 30% higher accuracy than\nstate-of-the-art open-source LMMs, but also outperforms GPT-4V (its teacher),\non both existing related benchmarks and the proposed MICBench. Our model is\npublished at https://huggingface.co/q-future/co-instruct.",
        "publication_date": "2024-02-26T15:10:56Z",
        "upvotes": 15
    },
    "2402.17764": {
        "url": "https://arxiv.org/abs/2402.17764",
        "title": "The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits",
        "authors": [
            "Shuming Ma",
            "Hongyu Wang",
            "Lingxiao Ma",
            "Lei Wang",
            "Wenhui Wang",
            "Shaohan Huang",
            "Li Dong",
            "Ruiping Wang",
            "Jilong Xue",
            "Furu Wei"
        ],
        "abstract": "Recent research, such as BitNet, is paving the way for a new era of 1-bit\nLarge Language Models (LLMs). In this work, we introduce a 1-bit LLM variant,\nnamely BitNet b1.58, in which every single parameter (or weight) of the LLM is\nternary {-1, 0, 1}. It matches the full-precision (i.e., FP16 or BF16)\nTransformer LLM with the same model size and training tokens in terms of both\nperplexity and end-task performance, while being significantly more\ncost-effective in terms of latency, memory, throughput, and energy consumption.\nMore profoundly, the 1.58-bit LLM defines a new scaling law and recipe for\ntraining new generations of LLMs that are both high-performance and\ncost-effective. Furthermore, it enables a new computation paradigm and opens\nthe door for designing specific hardware optimized for 1-bit LLMs.",
        "publication_date": "2024-02-27T18:56:19Z",
        "upvotes": 553
    },
    "2402.17485": {
        "url": "https://arxiv.org/abs/2402.17485",
        "title": "EMO: Emote Portrait Alive -- Generating Expressive Portrait Videos with\n  Audio2Video Diffusion Model under Weak Conditions",
        "authors": [
            "Linrui Tian",
            "Qi Wang",
            "Bang Zhang",
            "Liefeng Bo"
        ],
        "abstract": "In this work, we tackle the challenge of enhancing the realism and\nexpressiveness in talking head video generation by focusing on the dynamic and\nnuanced relationship between audio cues and facial movements. We identify the\nlimitations of traditional techniques that often fail to capture the full\nspectrum of human expressions and the uniqueness of individual facial styles.\nTo address these issues, we propose EMO, a novel framework that utilizes a\ndirect audio-to-video synthesis approach, bypassing the need for intermediate\n3D models or facial landmarks. Our method ensures seamless frame transitions\nand consistent identity preservation throughout the video, resulting in highly\nexpressive and lifelike animations. Experimental results demonsrate that EMO is\nable to produce not only convincing speaking videos but also singing videos in\nvarious styles, significantly outperforming existing state-of-the-art\nmethodologies in terms of expressiveness and realism.",
        "publication_date": "2024-02-27T13:10:11Z",
        "upvotes": 176
    },
    "2402.17177": {
        "url": "https://arxiv.org/abs/2402.17177",
        "title": "Sora: A Review on Background, Technology, Limitations, and Opportunities\n  of Large Vision Models",
        "authors": [
            "Yixin Liu",
            "Kai Zhang",
            "Yuan Li",
            "Zhiling Yan",
            "Chujie Gao",
            "Ruoxi Chen",
            "Zhengqing Yuan",
            "Yue Huang",
            "Hanchi Sun",
            "Jianfeng Gao",
            "Lifang He",
            "Lichao Sun"
        ],
        "abstract": "Sora is a text-to-video generative AI model, released by OpenAI in February\n2024. The model is trained to generate videos of realistic or imaginative\nscenes from text instructions and show potential in simulating the physical\nworld. Based on public technical reports and reverse engineering, this paper\npresents a comprehensive review of the model's background, related\ntechnologies, applications, remaining challenges, and future directions of\ntext-to-video AI models. We first trace Sora's development and investigate the\nunderlying technologies used to build this \"world simulator\". Then, we describe\nin detail the applications and potential impact of Sora in multiple industries\nranging from film-making and education to marketing. We discuss the main\nchallenges and limitations that need to be addressed to widely deploy Sora,\nsuch as ensuring safe and unbiased video generation. Lastly, we discuss the\nfuture development of Sora and video generation models in general, and how\nadvancements in the field could enable new ways of human-AI interaction,\nboosting productivity and creativity of video generation.",
        "publication_date": "2024-02-27T03:30:58Z",
        "upvotes": 84
    },
    "2402.17193": {
        "url": "https://arxiv.org/abs/2402.17193",
        "title": "When Scaling Meets LLM Finetuning: The Effect of Data, Model and\n  Finetuning Method",
        "authors": [
            "Biao Zhang",
            "Zhongtao Liu",
            "Colin Cherry",
            "Orhan Firat"
        ],
        "abstract": "While large language models (LLMs) often adopt finetuning to unlock their\ncapabilities for downstream applications, our understanding on the inductive\nbiases (especially the scaling properties) of different finetuning methods is\nstill limited. To fill this gap, we conduct systematic experiments studying\nwhether and how different scaling factors, including LLM model size,\npretraining data size, new finetuning parameter size and finetuning data size,\naffect the finetuning performance. We consider two types of finetuning --\nfull-model tuning (FMT) and parameter efficient tuning (PET, including prompt\ntuning and LoRA), and explore their scaling behaviors in the data-limited\nregime where the LLM model size substantially outweighs the finetuning data\nsize. Based on two sets of pretrained bilingual LLMs from 1B to 16B and\nexperiments on bilingual machine translation and multilingual summarization\nbenchmarks, we find that 1) LLM finetuning follows a powerbased multiplicative\njoint scaling law between finetuning data size and each other scaling factor;\n2) LLM finetuning benefits more from LLM model scaling than pretraining data\nscaling, and PET parameter scaling is generally ineffective; and 3) the optimal\nfinetuning method is highly task- and finetuning data-dependent. We hope our\nfindings could shed light on understanding, selecting and developing LLM\nfinetuning methods.",
        "publication_date": "2024-02-27T04:18:49Z",
        "upvotes": 23
    },
    "2402.17412": {
        "url": "https://arxiv.org/abs/2402.17412",
        "title": "DiffuseKronA: A Parameter Efficient Fine-tuning Method for Personalized\n  Diffusion Models",
        "authors": [
            "Shyam Marjit",
            "Harshit Singh",
            "Nityanand Mathur",
            "Sayak Paul",
            "Chia-Mu Yu",
            "Pin-Yu Chen"
        ],
        "abstract": "In the realm of subject-driven text-to-image (T2I) generative models, recent\ndevelopments like DreamBooth and BLIP-Diffusion have led to impressive results\nyet encounter limitations due to their intensive fine-tuning demands and\nsubstantial parameter requirements. While the low-rank adaptation (LoRA) module\nwithin DreamBooth offers a reduction in trainable parameters, it introduces a\npronounced sensitivity to hyperparameters, leading to a compromise between\nparameter efficiency and the quality of T2I personalized image synthesis.\nAddressing these constraints, we introduce \\textbf{\\textit{DiffuseKronA}}, a\nnovel Kronecker product-based adaptation module that not only significantly\nreduces the parameter count by 35\\% and 99.947\\% compared to LoRA-DreamBooth\nand the original DreamBooth, respectively, but also enhances the quality of\nimage synthesis. Crucially, \\textit{DiffuseKronA} mitigates the issue of\nhyperparameter sensitivity, delivering consistent high-quality generations\nacross a wide range of hyperparameters, thereby diminishing the necessity for\nextensive fine-tuning. Furthermore, a more controllable decomposition makes\n\\textit{DiffuseKronA} more interpretable and even can achieve up to a 50\\%\nreduction with results comparable to LoRA-Dreambooth. Evaluated against diverse\nand complex input images and text prompts, \\textit{DiffuseKronA} consistently\noutperforms existing models, producing diverse images of higher quality with\nimproved fidelity and a more accurate color distribution of objects, all the\nwhile upholding exceptional parameter efficiency, thus presenting a substantial\nadvancement in the field of T2I generative modeling. Our project page,\nconsisting of links to the code, and pre-trained checkpoints, is available at\nhttps://diffusekrona.github.io/.",
        "publication_date": "2024-02-27T11:05:34Z",
        "upvotes": 21
    },
    "2402.17553": {
        "url": "https://arxiv.org/abs/2402.17553",
        "title": "OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist\n  Autonomous Agents for Desktop and Web",
        "authors": [
            "Raghav Kapoor",
            "Yash Parag Butala",
            "Melisa Russak",
            "Jing Yu Koh",
            "Kiran Kamble",
            "Waseem Alshikh",
            "Ruslan Salakhutdinov"
        ],
        "abstract": "For decades, human-computer interaction has fundamentally been manual. Even\ntoday, almost all productive work done on the computer necessitates human input\nat every step. Autonomous virtual agents represent an exciting step in\nautomating many of these menial tasks. Virtual agents would empower users with\nlimited technical proficiency to harness the full possibilities of computer\nsystems. They could also enable the efficient streamlining of numerous computer\ntasks, ranging from calendar management to complex travel bookings, with\nminimal human intervention. In this paper, we introduce OmniACT, the\nfirst-of-a-kind dataset and benchmark for assessing an agent's capability to\ngenerate executable programs to accomplish computer tasks. Our scope extends\nbeyond traditional web automation, covering a diverse range of desktop\napplications. The dataset consists of fundamental tasks such as \"Play the next\nsong\", as well as longer horizon tasks such as \"Send an email to John Doe\nmentioning the time and place to meet\". Specifically, given a pair of screen\nimage and a visually-grounded natural language task, the goal is to generate a\nscript capable of fully executing the task. We run several strong baseline\nlanguage model agents on our benchmark. The strongest baseline, GPT-4, performs\nthe best on our benchmark However, its performance level still reaches only 15%\nof the human proficiency in generating executable scripts capable of completing\nthe task, demonstrating the challenge of our task for conventional web agents.\nOur benchmark provides a platform to measure and evaluate the progress of\nlanguage model agents in automating computer tasks and motivates future work\ntowards building multimodal models that bridge large language models and the\nvisual grounding of computer screens.",
        "publication_date": "2024-02-27T14:47:53Z",
        "upvotes": 21
    },
    "2402.17759": {
        "url": "https://arxiv.org/abs/2402.17759",
        "title": "Towards Optimal Learning of Language Models",
        "authors": [
            "Yuxian Gu",
            "Li Dong",
            "Yaru Hao",
            "Qingxiu Dong",
            "Minlie Huang",
            "Furu Wei"
        ],
        "abstract": "This work studies the general principles of improving the learning of\nlanguage models (LMs), which aims at reducing the necessary training steps for\nachieving superior performance. Specifically, we present a theory for the\noptimal learning of LMs. We first propose an objective that optimizes LM\nlearning by maximizing the data compression ratio in an\n\"LM-training-as-lossless-compression\" view. Then, we derive a theorem, named\nLearning Law, to reveal the properties of the dynamics in the optimal learning\nprocess under our objective. The theorem is then validated by experiments on a\nlinear classification and a real-world language modeling task. Finally, we\nempirically verify that the optimal learning of LMs essentially stems from the\nimprovement of the coefficients in the scaling law of LMs, indicating great\npromise and significance for designing practical learning acceleration methods.\nOur code can be found at https://aka.ms/LearningLaw.",
        "publication_date": "2024-02-27T18:52:19Z",
        "upvotes": 16
    },
    "2402.17139": {
        "url": "https://arxiv.org/abs/2402.17139",
        "title": "Video as the New Language for Real-World Decision Making",
        "authors": [
            "Sherry Yang",
            "Jacob Walker",
            "Jack Parker-Holder",
            "Yilun Du",
            "Jake Bruce",
            "Andre Barreto",
            "Pieter Abbeel",
            "Dale Schuurmans"
        ],
        "abstract": "Both text and video data are abundant on the internet and support large-scale\nself-supervised learning through next token or frame prediction. However, they\nhave not been equally leveraged: language models have had significant\nreal-world impact, whereas video generation has remained largely limited to\nmedia entertainment. Yet video data captures important information about the\nphysical world that is difficult to express in language. To address this gap,\nwe discuss an under-appreciated opportunity to extend video generation to solve\ntasks in the real world. We observe how, akin to language, video can serve as a\nunified interface that can absorb internet knowledge and represent diverse\ntasks. Moreover, we demonstrate how, like language models, video generation can\nserve as planners, agents, compute engines, and environment simulators through\ntechniques such as in-context learning, planning and reinforcement learning. We\nidentify major impact opportunities in domains such as robotics, self-driving,\nand science, supported by recent work that demonstrates how such advanced\ncapabilities in video generation are plausibly within reach. Lastly, we\nidentify key challenges in video generation that mitigate progress. Addressing\nthese challenges will enable video generation models to demonstrate unique\nvalue alongside language models in a wider array of AI applications.",
        "publication_date": "2024-02-27T02:05:29Z",
        "upvotes": 16
    },
    "2402.17753": {
        "url": "https://arxiv.org/abs/2402.17753",
        "title": "Evaluating Very Long-Term Conversational Memory of LLM Agents",
        "authors": [
            "Adyasha Maharana",
            "Dong-Ho Lee",
            "Sergey Tulyakov",
            "Mohit Bansal",
            "Francesco Barbieri",
            "Yuwei Fang"
        ],
        "abstract": "Existing works on long-term open-domain dialogues focus on evaluating model\nresponses within contexts spanning no more than five chat sessions. Despite\nadvancements in long-context large language models (LLMs) and retrieval\naugmented generation (RAG) techniques, their efficacy in very long-term\ndialogues remains unexplored. To address this research gap, we introduce a\nmachine-human pipeline to generate high-quality, very long-term dialogues by\nleveraging LLM-based agent architectures and grounding their dialogues on\npersonas and temporal event graphs. Moreover, we equip each agent with the\ncapability of sharing and reacting to images. The generated conversations are\nverified and edited by human annotators for long-range consistency and\ngrounding to the event graphs. Using this pipeline, we collect LoCoMo, a\ndataset of very long-term conversations, each encompassing 300 turns and 9K\ntokens on avg., over up to 35 sessions. Based on LoCoMo, we present a\ncomprehensive evaluation benchmark to measure long-term memory in models,\nencompassing question answering, event summarization, and multi-modal dialogue\ngeneration tasks. Our experimental results indicate that LLMs exhibit\nchallenges in understanding lengthy conversations and comprehending long-range\ntemporal and causal dynamics within dialogues. Employing strategies like\nlong-context LLMs or RAG can offer improvements but these models still\nsubstantially lag behind human performance.",
        "publication_date": "2024-02-27T18:42:31Z",
        "upvotes": 15
    },
    "2402.17403": {
        "url": "https://arxiv.org/abs/2402.17403",
        "title": "Sora Generates Videos with Stunning Geometrical Consistency",
        "authors": [
            "Xuanyi Li",
            "Daquan Zhou",
            "Chenxu Zhang",
            "Shaodong Wei",
            "Qibin Hou",
            "Ming-Ming Cheng"
        ],
        "abstract": "The recently developed Sora model [1] has exhibited remarkable capabilities\nin video generation, sparking intense discussions regarding its ability to\nsimulate real-world phenomena. Despite its growing popularity, there is a lack\nof established metrics to evaluate its fidelity to real-world physics\nquantitatively. In this paper, we introduce a new benchmark that assesses the\nquality of the generated videos based on their adherence to real-world physics\nprinciples. We employ a method that transforms the generated videos into 3D\nmodels, leveraging the premise that the accuracy of 3D reconstruction is\nheavily contingent on the video quality. From the perspective of 3D\nreconstruction, we use the fidelity of the geometric constraints satisfied by\nthe constructed 3D models as a proxy to gauge the extent to which the generated\nvideos conform to real-world physics rules. Project page:\nhttps://sora-geometrical-consistency.github.io/",
        "publication_date": "2024-02-27T10:49:05Z",
        "upvotes": 14
    },
    "2402.17463": {
        "url": "https://arxiv.org/abs/2402.17463",
        "title": "Training-Free Long-Context Scaling of Large Language Models",
        "authors": [
            "Chenxin An",
            "Fei Huang",
            "Jun Zhang",
            "Shansan Gong",
            "Xipeng Qiu",
            "Chang Zhou",
            "Lingpeng Kong"
        ],
        "abstract": "The ability of Large Language Models (LLMs) to process and generate coherent\ntext is markedly weakened when the number of input tokens exceeds their\npretraining length. Given the expensive overhead of finetuning large-scale\nmodels with longer sequences, we propose Dual Chunk Attention (DCA), which\nenables Llama2 70B to support context windows of more than 100k tokens without\ncontinual training. By decomposing the attention computation for long sequences\ninto chunk-based modules, DCA manages to effectively capture the relative\npositional information of tokens within the same chunk (Intra-Chunk) and across\ndistinct chunks (Inter-Chunk), as well as integrates seamlessly with Flash\nAttention. In addition to its impressive extrapolation capability, DCA achieves\nperformance on practical long-context tasks that is comparable to or even\nbetter than that of finetuned models. When compared with proprietary models,\nour training-free 70B model attains 94% of the performance of gpt-3.5-16k,\nindicating it is a viable open-source alternative. All code and data used in\nthis work are released at \\url{https://github.com/HKUNLP/ChunkLlama}.",
        "publication_date": "2024-02-27T12:39:23Z",
        "upvotes": 13
    },
    "2402.17723": {
        "url": "https://arxiv.org/abs/2402.17723",
        "title": "Seeing and Hearing: Open-domain Visual-Audio Generation with Diffusion\n  Latent Aligners",
        "authors": [
            "Yazhou Xing",
            "Yingqing He",
            "Zeyue Tian",
            "Xintao Wang",
            "Qifeng Chen"
        ],
        "abstract": "Video and audio content creation serves as the core technique for the movie\nindustry and professional users. Recently, existing diffusion-based methods\ntackle video and audio generation separately, which hinders the technique\ntransfer from academia to industry. In this work, we aim at filling the gap,\nwith a carefully designed optimization-based framework for cross-visual-audio\nand joint-visual-audio generation. We observe the powerful generation ability\nof off-the-shelf video or audio generation models. Thus, instead of training\nthe giant models from scratch, we propose to bridge the existing strong models\nwith a shared latent representation space. Specifically, we propose a\nmultimodality latent aligner with the pre-trained ImageBind model. Our latent\naligner shares a similar core as the classifier guidance that guides the\ndiffusion denoising process during inference time. Through carefully designed\noptimization strategy and loss functions, we show the superior performance of\nour method on joint video-audio generation, visual-steered audio generation,\nand audio-steered visual generation tasks. The project website can be found at\nhttps://yzxing87.github.io/Seeing-and-Hearing/",
        "publication_date": "2024-02-27T17:57:04Z",
        "upvotes": 13
    },
    "2402.16936": {
        "url": "https://arxiv.org/abs/2402.16936",
        "title": "Disentangled 3D Scene Generation with Layout Learning",
        "authors": [
            "Dave Epstein",
            "Ben Poole",
            "Ben Mildenhall",
            "Alexei A. Efros",
            "Aleksander Holynski"
        ],
        "abstract": "We introduce a method to generate 3D scenes that are disentangled into their\ncomponent objects. This disentanglement is unsupervised, relying only on the\nknowledge of a large pretrained text-to-image model. Our key insight is that\nobjects can be discovered by finding parts of a 3D scene that, when rearranged\nspatially, still produce valid configurations of the same scene. Concretely,\nour method jointly optimizes multiple NeRFs from scratch - each representing\nits own object - along with a set of layouts that composite these objects into\nscenes. We then encourage these composited scenes to be in-distribution\naccording to the image generator. We show that despite its simplicity, our\napproach successfully generates 3D scenes decomposed into individual objects,\nenabling new capabilities in text-to-3D content creation. For results and an\ninteractive demo, see our project page at https://dave.ml/layoutlearning/",
        "publication_date": "2024-02-26T18:54:15Z",
        "upvotes": 10
    },
    "2402.17245": {
        "url": "https://arxiv.org/abs/2402.17245",
        "title": "Playground v2.5: Three Insights towards Enhancing Aesthetic Quality in\n  Text-to-Image Generation",
        "authors": [
            "Daiqing Li",
            "Aleks Kamko",
            "Ehsan Akhgari",
            "Ali Sabet",
            "Linmiao Xu",
            "Suhail Doshi"
        ],
        "abstract": "In this work, we share three insights for achieving state-of-the-art\naesthetic quality in text-to-image generative models. We focus on three\ncritical aspects for model improvement: enhancing color and contrast, improving\ngeneration across multiple aspect ratios, and improving human-centric fine\ndetails. First, we delve into the significance of the noise schedule in\ntraining a diffusion model, demonstrating its profound impact on realism and\nvisual fidelity. Second, we address the challenge of accommodating various\naspect ratios in image generation, emphasizing the importance of preparing a\nbalanced bucketed dataset. Lastly, we investigate the crucial role of aligning\nmodel outputs with human preferences, ensuring that generated images resonate\nwith human perceptual expectations. Through extensive analysis and experiments,\nPlayground v2.5 demonstrates state-of-the-art performance in terms of aesthetic\nquality under various conditions and aspect ratios, outperforming both\nwidely-used open-source models like SDXL and Playground v2, and closed-source\ncommercial systems such as DALLE 3 and Midjourney v5.2. Our model is\nopen-source, and we hope the development of Playground v2.5 provides valuable\nguidelines for researchers aiming to elevate the aesthetic quality of\ndiffusion-based image generation models.",
        "publication_date": "2024-02-27T06:31:52Z",
        "upvotes": 10
    },
    "2402.17427": {
        "url": "https://arxiv.org/abs/2402.17427",
        "title": "VastGaussian: Vast 3D Gaussians for Large Scene Reconstruction",
        "authors": [
            "Jiaqi Lin",
            "Zhihao Li",
            "Xiao Tang",
            "Jianzhuang Liu",
            "Shiyong Liu",
            "Jiayue Liu",
            "Yangdi Lu",
            "Xiaofei Wu",
            "Songcen Xu",
            "Youliang Yan",
            "Wenming Yang"
        ],
        "abstract": "Existing NeRF-based methods for large scene reconstruction often have\nlimitations in visual quality and rendering speed. While the recent 3D Gaussian\nSplatting works well on small-scale and object-centric scenes, scaling it up to\nlarge scenes poses challenges due to limited video memory, long optimization\ntime, and noticeable appearance variations. To address these challenges, we\npresent VastGaussian, the first method for high-quality reconstruction and\nreal-time rendering on large scenes based on 3D Gaussian Splatting. We propose\na progressive partitioning strategy to divide a large scene into multiple\ncells, where the training cameras and point cloud are properly distributed with\nan airspace-aware visibility criterion. These cells are merged into a complete\nscene after parallel optimization. We also introduce decoupled appearance\nmodeling into the optimization process to reduce appearance variations in the\nrendered images. Our approach outperforms existing NeRF-based methods and\nachieves state-of-the-art results on multiple large scene datasets, enabling\nfast optimization and high-fidelity real-time rendering.",
        "publication_date": "2024-02-27T11:40:50Z",
        "upvotes": 9
    },
    "2402.19173": {
        "url": "https://arxiv.org/abs/2402.19173",
        "title": "StarCoder 2 and The Stack v2: The Next Generation",
        "authors": [
            "Anton Lozhkov",
            "Raymond Li",
            "Loubna Ben Allal",
            "Federico Cassano",
            "Joel Lamy-Poirier",
            "Nouamane Tazi",
            "Ao Tang",
            "Dmytro Pykhtar",
            "Jiawei Liu",
            "Yuxiang Wei",
            "Tianyang Liu",
            "Max Tian",
            "Denis Kocetkov",
            "Arthur Zucker",
            "Younes Belkada",
            "Zijian Wang",
            "Qian Liu",
            "Dmitry Abulkhanov",
            "Indraneil Paul",
            "Zhuang Li",
            "Wen-Ding Li",
            "Megan Risdal",
            "Jia Li",
            "Jian Zhu",
            "Terry Yue Zhuo",
            "Evgenii Zheltonozhskii",
            "Nii Osae Osae Dade",
            "Wenhao Yu",
            "Lucas Krau\u00df",
            "Naman Jain",
            "Yixuan Su",
            "Xuanli He",
            "Manan Dey",
            "Edoardo Abati",
            "Yekun Chai",
            "Niklas Muennighoff",
            "Xiangru Tang",
            "Muhtasham Oblokulov",
            "Christopher Akiki",
            "Marc Marone",
            "Chenghao Mou",
            "Mayank Mishra",
            "Alex Gu",
            "Binyuan Hui",
            "Tri Dao",
            "Armel Zebaze",
            "Olivier Dehaene",
            "Nicolas Patry",
            "Canwen Xu",
            "Julian McAuley",
            "Han Hu",
            "Torsten Scholak",
            "Sebastien Paquet",
            "Jennifer Robinson",
            "Carolyn Jane Anderson",
            "Nicolas Chapados",
            "Mostofa Patwary",
            "Nima Tajbakhsh",
            "Yacine Jernite",
            "Carlos Mu\u00f1oz Ferrandis",
            "Lingming Zhang",
            "Sean Hughes",
            "Thomas Wolf",
            "Arjun Guha",
            "Leandro von Werra",
            "Harm de Vries"
        ],
        "abstract": "The BigCode project, an open-scientific collaboration focused on the\nresponsible development of Large Language Models for Code (Code LLMs),\nintroduces StarCoder2. In partnership with Software Heritage (SWH), we build\nThe Stack v2 on top of the digital commons of their source code archive.\nAlongside the SWH repositories spanning 619 programming languages, we carefully\nselect other high-quality data sources, such as GitHub pull requests, Kaggle\nnotebooks, and code documentation. This results in a training set that is 4x\nlarger than the first StarCoder dataset. We train StarCoder2 models with 3B,\n7B, and 15B parameters on 3.3 to 4.3 trillion tokens and thoroughly evaluate\nthem on a comprehensive set of Code LLM benchmarks. We find that our small\nmodel, StarCoder2-3B, outperforms other Code LLMs of similar size on most\nbenchmarks, and also outperforms StarCoderBase-15B. Our large model,\nStarCoder2- 15B, significantly outperforms other models of comparable size. In\naddition, it matches or outperforms CodeLlama-34B, a model more than twice its\nsize. Although DeepSeekCoder- 33B is the best-performing model at code\ncompletion for high-resource languages, we find that StarCoder2-15B outperforms\nit on math and code reasoning benchmarks, as well as several low-resource\nlanguages. We make the model weights available under an OpenRAIL license and\nensure full transparency regarding the training data by releasing the SoftWare\nHeritage persistent IDentifiers (SWHIDs) of the source code data.",
        "publication_date": "2024-02-29T13:53:35Z",
        "upvotes": 117
    },
    "2402.19155": {
        "url": "https://arxiv.org/abs/2402.19155",
        "title": "Beyond Language Models: Byte Models are Digital World Simulators",
        "authors": [
            "Shangda Wu",
            "Xu Tan",
            "Zili Wang",
            "Rui Wang",
            "Xiaobing Li",
            "Maosong Sun"
        ],
        "abstract": "Traditional deep learning often overlooks bytes, the basic units of the\ndigital world, where all forms of information and operations are encoded and\nmanipulated in binary format. Inspired by the success of next token prediction\nin natural language processing, we introduce bGPT, a model with next byte\nprediction to simulate the digital world. bGPT matches specialized models in\nperformance across various modalities, including text, audio, and images, and\noffers new possibilities for predicting, simulating, and diagnosing algorithm\nor hardware behaviour. It has almost flawlessly replicated the process of\nconverting symbolic music data, achieving a low error rate of 0.0011 bits per\nbyte in converting ABC notation to MIDI format. In addition, bGPT demonstrates\nexceptional capabilities in simulating CPU behaviour, with an accuracy\nexceeding 99.99% in executing various operations. Leveraging next byte\nprediction, models like bGPT can directly learn from vast binary data,\neffectively simulating the intricate patterns of the digital world.",
        "publication_date": "2024-02-29T13:38:07Z",
        "upvotes": 44
    },
    "2402.19427": {
        "url": "https://arxiv.org/abs/2402.19427",
        "title": "Griffin: Mixing Gated Linear Recurrences with Local Attention for\n  Efficient Language Models",
        "authors": [
            "Soham De",
            "Samuel L. Smith",
            "Anushan Fernando",
            "Aleksandar Botev",
            "George Cristian-Muraru",
            "Albert Gu",
            "Ruba Haroun",
            "Leonard Berrada",
            "Yutian Chen",
            "Srivatsan Srinivasan",
            "Guillaume Desjardins",
            "Arnaud Doucet",
            "David Budden",
            "Yee Whye Teh",
            "Razvan Pascanu",
            "Nando De Freitas",
            "Caglar Gulcehre"
        ],
        "abstract": "Recurrent neural networks (RNNs) have fast inference and scale efficiently on\nlong sequences, but they are difficult to train and hard to scale. We propose\nHawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that\nmixes gated linear recurrences with local attention. Hawk exceeds the reported\nperformance of Mamba on downstream tasks, while Griffin matches the performance\nof Llama-2 despite being trained on over 6 times fewer tokens. We also show\nthat Griffin can extrapolate on sequences significantly longer than those seen\nduring training. Our models match the hardware efficiency of Transformers\nduring training, and during inference they have lower latency and significantly\nhigher throughput. We scale Griffin up to 14B parameters, and explain how to\nshard our models for efficient distributed training.",
        "publication_date": "2024-02-29T18:24:46Z",
        "upvotes": 40
    },
    "2402.19479": {
        "url": "https://arxiv.org/abs/2402.19479",
        "title": "Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers",
        "authors": [
            "Tsai-Shien Chen",
            "Aliaksandr Siarohin",
            "Willi Menapace",
            "Ekaterina Deyneka",
            "Hsiang-wei Chao",
            "Byung Eun Jeon",
            "Yuwei Fang",
            "Hsin-Ying Lee",
            "Jian Ren",
            "Ming-Hsuan Yang",
            "Sergey Tulyakov"
        ],
        "abstract": "The quality of the data and annotation upper-bounds the quality of a\ndownstream model. While there exist large text corpora and image-text pairs,\nhigh-quality video-text data is much harder to collect. First of all, manual\nlabeling is more time-consuming, as it requires an annotator to watch an entire\nvideo. Second, videos have a temporal dimension, consisting of several scenes\nstacked together, and showing multiple actions. Accordingly, to establish a\nvideo dataset with high-quality captions, we propose an automatic approach\nleveraging multimodal inputs, such as textual video description, subtitles, and\nindividual video frames. Specifically, we curate 3.8M high-resolution videos\nfrom the publicly available HD-VILA-100M dataset. We then split them into\nsemantically consistent video clips, and apply multiple cross-modality teacher\nmodels to obtain captions for each video. Next, we finetune a retrieval model\non a small subset where the best caption of each video is manually selected and\nthen employ the model in the whole dataset to select the best caption as the\nannotation. In this way, we get 70M videos paired with high-quality text\ncaptions. We dub the dataset as Panda-70M. We show the value of the proposed\ndataset on three downstream tasks: video captioning, video and text retrieval,\nand text-driven video generation. The models trained on the proposed data score\nsubstantially better on the majority of metrics across all the tasks.",
        "publication_date": "2024-02-29T18:59:50Z",
        "upvotes": 30
    },
    "2402.19469": {
        "url": "https://arxiv.org/abs/2402.19469",
        "title": "Humanoid Locomotion as Next Token Prediction",
        "authors": [
            "Ilija Radosavovic",
            "Bike Zhang",
            "Baifeng Shi",
            "Jathushan Rajasegaran",
            "Sarthak Kamat",
            "Trevor Darrell",
            "Koushil Sreenath",
            "Jitendra Malik"
        ],
        "abstract": "We cast real-world humanoid control as a next token prediction problem, akin\nto predicting the next word in language. Our model is a causal transformer\ntrained via autoregressive prediction of sensorimotor trajectories. To account\nfor the multi-modal nature of the data, we perform prediction in a\nmodality-aligned way, and for each input token predict the next token from the\nsame modality. This general formulation enables us to leverage data with\nmissing modalities, like video trajectories without actions. We train our model\non a collection of simulated trajectories coming from prior neural network\npolicies, model-based controllers, motion capture data, and YouTube videos of\nhumans. We show that our model enables a full-sized humanoid to walk in San\nFrancisco zero-shot. Our model can transfer to the real world even when trained\non only 27 hours of walking data, and can generalize to commands not seen\nduring training like walking backward. These findings suggest a promising path\ntoward learning challenging real-world control tasks by generative modeling of\nsensorimotor trajectories.",
        "publication_date": "2024-02-29T18:57:37Z",
        "upvotes": 25
    },
    "2402.18796": {
        "url": "https://arxiv.org/abs/2402.18796",
        "title": "MOSAIC: A Modular System for Assistive and Interactive Cooking",
        "authors": [
            "Huaxiaoyue Wang",
            "Kushal Kedia",
            "Juntao Ren",
            "Rahma Abdullah",
            "Atiksh Bhardwaj",
            "Angela Chao",
            "Kelly Y Chen",
            "Nathaniel Chin",
            "Prithwish Dan",
            "Xinyi Fan",
            "Gonzalo Gonzalez-Pumariega",
            "Aditya Kompella",
            "Maximus Adrian Pace",
            "Yash Sharma",
            "Xiangwan Sun",
            "Neha Sunkara",
            "Sanjiban Choudhury"
        ],
        "abstract": "We present MOSAIC, a modular architecture for home robots to perform complex\ncollaborative tasks, such as cooking with everyday users. MOSAIC tightly\ncollaborates with humans, interacts with users using natural language,\ncoordinates multiple robots, and manages an open vocabulary of everyday\nobjects. At its core, MOSAIC employs modularity: it leverages multiple\nlarge-scale pre-trained models for general tasks like language and image\nrecognition, while using streamlined modules designed for task-specific\ncontrol. We extensively evaluate MOSAIC on 60 end-to-end trials where two\nrobots collaborate with a human user to cook a combination of 6 recipes. We\nalso extensively test individual modules with 180 episodes of visuomotor\npicking, 60 episodes of human motion forecasting, and 46 online user\nevaluations of the task planner. We show that MOSAIC is able to efficiently\ncollaborate with humans by running the overall system end-to-end with a real\nhuman user, completing 68.3% (41/60) collaborative cooking trials of 6\ndifferent recipes with a subtask completion rate of 91.6%. Finally, we discuss\nthe limitations of the current system and exciting open challenges in this\ndomain. The project's website is at https://portal-cornell.github.io/MOSAIC/",
        "publication_date": "2024-02-29T01:56:41Z",
        "upvotes": 22
    },
    "2402.19481": {
        "url": "https://arxiv.org/abs/2402.19481",
        "title": "DistriFusion: Distributed Parallel Inference for High-Resolution\n  Diffusion Models",
        "authors": [
            "Muyang Li",
            "Tianle Cai",
            "Jiaxin Cao",
            "Qinsheng Zhang",
            "Han Cai",
            "Junjie Bai",
            "Yangqing Jia",
            "Ming-Yu Liu",
            "Kai Li",
            "Song Han"
        ],
        "abstract": "Diffusion models have achieved great success in synthesizing high-quality\nimages. However, generating high-resolution images with diffusion models is\nstill challenging due to the enormous computational costs, resulting in a\nprohibitive latency for interactive applications. In this paper, we propose\nDistriFusion to tackle this problem by leveraging parallelism across multiple\nGPUs. Our method splits the model input into multiple patches and assigns each\npatch to a GPU. However, naively implementing such an algorithm breaks the\ninteraction between patches and loses fidelity, while incorporating such an\ninteraction will incur tremendous communication overhead. To overcome this\ndilemma, we observe the high similarity between the input from adjacent\ndiffusion steps and propose displaced patch parallelism, which takes advantage\nof the sequential nature of the diffusion process by reusing the pre-computed\nfeature maps from the previous timestep to provide context for the current\nstep. Therefore, our method supports asynchronous communication, which can be\npipelined by computation. Extensive experiments show that our method can be\napplied to recent Stable Diffusion XL with no quality degradation and achieve\nup to a 6.1$\\times$ speedup on eight NVIDIA A100s compared to one. Our code is\npublicly available at https://github.com/mit-han-lab/distrifuser.",
        "publication_date": "2024-02-29T18:59:58Z",
        "upvotes": 16
    },
    "2402.18668": {
        "url": "https://arxiv.org/abs/2402.18668",
        "title": "Simple linear attention language models balance the recall-throughput\n  tradeoff",
        "authors": [
            "Simran Arora",
            "Sabri Eyuboglu",
            "Michael Zhang",
            "Aman Timalsina",
            "Silas Alberti",
            "Dylan Zinsley",
            "James Zou",
            "Atri Rudra",
            "Christopher R\u00e9"
        ],
        "abstract": "Recent work has shown that attention-based language models excel at recall,\nthe ability to ground generations in tokens previously seen in context.\nHowever, the efficiency of attention-based models is bottle-necked during\ninference by the KV-cache's aggressive memory consumption. In this work, we\nexplore whether we can improve language model efficiency (e.g. by reducing\nmemory consumption) without compromising on recall. By applying experiments and\ntheory to a broad set of architectures, we identify a key tradeoff between a\nmodel's state size and recall ability. We show that efficient alternatives to\nattention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but\nstruggle at recall. We propose BASED a simple architecture combining linear and\nsliding window attention. By varying BASED window size and linear attention\nfeature dimension, we can dial the state size and traverse the pareto frontier\nof the recall-memory tradeoff curve, recovering the full quality of attention\non one end and the small state size of attention-alternatives on the other. We\ntrain language models up to 1.3b parameters and show that BASED matches the\nstrongest sub-quadratic models (e.g. Mamba) in perplexity and outperforms them\non real-world recall-intensive tasks by 6.22 accuracy points. Implementations\nof linear attention are often less efficient than optimized standard attention\nimplementations. To make BASED competitive, we develop IO-aware algorithms that\nenable 24x higher throughput on language generation than FlashAttention-2, when\ngenerating 1024 tokens using 1.3b parameter models. Code for this work is\nprovided at: https://github.com/HazyResearch/based.",
        "publication_date": "2024-02-28T19:28:27Z",
        "upvotes": 16
    },
    "2402.18734": {
        "url": "https://arxiv.org/abs/2402.18734",
        "title": "Priority Sampling of Large Language Models for Compilers",
        "authors": [
            "Dejan Grubisic",
            "Chris Cummins",
            "Volker Seeker",
            "Hugh Leather"
        ],
        "abstract": "Large language models show great potential in generating and optimizing code.\nWidely used sampling methods such as Nucleus Sampling increase the diversity of\ngeneration but often produce repeated samples for low temperatures and\nincoherent samples for high temperatures. Furthermore, the temperature\ncoefficient has to be tuned for each task, limiting its usability. We present\nPriority Sampling, a simple and deterministic sampling technique that produces\nunique samples ordered by the model's confidence. Each new sample expands the\nunexpanded token with the highest probability in the augmented search tree.\nAdditionally, Priority Sampling supports generation based on regular expression\nthat provides a controllable and structured exploration process. Priority\nSampling outperforms Nucleus Sampling for any number of samples, boosting the\nperformance of the original model from 2.87% to 5% improvement over -Oz.\nMoreover, it outperforms the autotuner used for the generation of labels for\nthe training of the original model in just 30 samples.",
        "publication_date": "2024-02-28T22:27:49Z",
        "upvotes": 15
    },
    "2402.18842": {
        "url": "https://arxiv.org/abs/2402.18842",
        "title": "ViewFusion: Towards Multi-View Consistency via Interpolated Denoising",
        "authors": [
            "Xianghui Yang",
            "Yan Zuo",
            "Sameera Ramasinghe",
            "Loris Bazzani",
            "Gil Avraham",
            "Anton van den Hengel"
        ],
        "abstract": "Novel-view synthesis through diffusion models has demonstrated remarkable\npotential for generating diverse and high-quality images. Yet, the independent\nprocess of image generation in these prevailing methods leads to challenges in\nmaintaining multiple-view consistency. To address this, we introduce\nViewFusion, a novel, training-free algorithm that can be seamlessly integrated\ninto existing pre-trained diffusion models. Our approach adopts an\nauto-regressive method that implicitly leverages previously generated views as\ncontext for the next view generation, ensuring robust multi-view consistency\nduring the novel-view generation process. Through a diffusion process that\nfuses known-view information via interpolated denoising, our framework\nsuccessfully extends single-view conditioned models to work in multiple-view\nconditional settings without any additional fine-tuning. Extensive experimental\nresults demonstrate the effectiveness of ViewFusion in generating consistent\nand detailed novel views.",
        "publication_date": "2024-02-29T04:21:38Z",
        "upvotes": 13
    },
    "2402.19159": {
        "url": "https://arxiv.org/abs/2402.19159",
        "title": "Trajectory Consistency Distillation",
        "authors": [
            "Jianbin Zheng",
            "Minghui Hu",
            "Zhongyi Fan",
            "Chaoyue Wang",
            "Changxing Ding",
            "Dacheng Tao",
            "Tat-Jen Cham"
        ],
        "abstract": "Latent Consistency Model (LCM) extends the Consistency Model to the latent\nspace and leverages the guided consistency distillation technique to achieve\nimpressive performance in accelerating text-to-image synthesis. However, we\nobserved that LCM struggles to generate images with both clarity and detailed\nintricacy. To address this limitation, we initially delve into and elucidate\nthe underlying causes. Our investigation identifies that the primary issue\nstems from errors in three distinct areas. Consequently, we introduce\nTrajectory Consistency Distillation (TCD), which encompasses trajectory\nconsistency function and strategic stochastic sampling. The trajectory\nconsistency function diminishes the distillation errors by broadening the scope\nof the self-consistency boundary condition and endowing the TCD with the\nability to accurately trace the entire trajectory of the Probability Flow ODE.\nAdditionally, strategic stochastic sampling is specifically designed to\ncircumvent the accumulated errors inherent in multi-step consistency sampling,\nwhich is meticulously tailored to complement the TCD model. Experiments\ndemonstrate that TCD not only significantly enhances image quality at low NFEs\nbut also yields more detailed results compared to the teacher model at high\nNFEs.",
        "publication_date": "2024-02-29T13:44:14Z",
        "upvotes": 11
    },
    "2403.00522": {
        "url": "https://arxiv.org/abs/2403.00522",
        "title": "VisionLLaMA: A Unified LLaMA Interface for Vision Tasks",
        "authors": [
            "Xiangxiang Chu",
            "Jianlin Su",
            "Bo Zhang",
            "Chunhua Shen"
        ],
        "abstract": "Large language models are built on top of a transformer-based architecture to\nprocess textual inputs. For example, the LLaMA stands out among many\nopen-source implementations. Can the same transformer be used to process 2D\nimages? In this paper, we answer this question by unveiling a LLaMA-like vision\ntransformer in plain and pyramid forms, termed VisionLLaMA, which is tailored\nfor this purpose. VisionLLaMA is a unified and generic modelling framework for\nsolving most vision tasks. We extensively evaluate its effectiveness using\ntypical pre-training paradigms in a good portion of downstream tasks of image\nperception and especially image generation. In many cases, VisionLLaMA have\nexhibited substantial gains over the previous state-of-the-art vision\ntransformers. We believe that VisionLLaMA can serve as a strong new baseline\nmodel for vision generation and understanding. Our code will be released at\nhttps://github.com/Meituan-AutoML/VisionLLaMA.",
        "publication_date": "2024-03-01T13:30:51Z",
        "upvotes": 40
    },
    "2403.00504": {
        "url": "https://arxiv.org/abs/2403.00504",
        "title": "Learning and Leveraging World Models in Visual Representation Learning",
        "authors": [
            "Quentin Garrido",
            "Mahmoud Assran",
            "Nicolas Ballas",
            "Adrien Bardes",
            "Laurent Najman",
            "Yann LeCun"
        ],
        "abstract": "Joint-Embedding Predictive Architecture (JEPA) has emerged as a promising\nself-supervised approach that learns by leveraging a world model. While\npreviously limited to predicting missing parts of an input, we explore how to\ngeneralize the JEPA prediction task to a broader set of corruptions. We\nintroduce Image World Models, an approach that goes beyond masked image\nmodeling and learns to predict the effect of global photometric transformations\nin latent space. We study the recipe of learning performant IWMs and show that\nit relies on three key aspects: conditioning, prediction difficulty, and\ncapacity. Additionally, we show that the predictive world model learned by IWM\ncan be adapted through finetuning to solve diverse tasks; a fine-tuned IWM\nworld model matches or surpasses the performance of previous self-supervised\nmethods. Finally, we show that learning with an IWM allows one to control the\nabstraction level of the learned representations, learning invariant\nrepresentations such as contrastive methods, or equivariant representations\nsuch as masked image modelling.",
        "publication_date": "2024-03-01T13:05:38Z",
        "upvotes": 24
    },
    "2403.00071": {
        "url": "https://arxiv.org/abs/2403.00071",
        "title": "Resonance RoPE: Improving Context Length Generalization of Large\n  Language Models",
        "authors": [
            "Suyuchen Wang",
            "Ivan Kobyzev",
            "Peng Lu",
            "Mehdi Rezagholizadeh",
            "Bang Liu"
        ],
        "abstract": "This paper addresses the challenge of train-short-test-long (TSTL) scenarios\nin Large Language Models (LLMs) equipped with Rotary Position Embedding (RoPE),\nwhere models pre-trained on shorter sequences face difficulty with\nout-of-distribution (OOD) token positions in longer sequences. We introduce\nResonance RoPE, a novel approach designed to narrow the generalization gap in\nTSTL scenarios by refining the interpolation of RoPE features for OOD\npositions, significantly improving the model performance without additional\nonline computational costs. Furthermore, we present PosGen, a new synthetic\nbenchmark specifically designed for fine-grained behavior analysis in TSTL\nscenarios, aiming to isolate the constantly increasing difficulty of token\ngeneration on long contexts from the challenges of recognizing new token\npositions. Our experiments on synthetic tasks show that after applying\nResonance RoPE, Transformers recognize OOD position better and more robustly.\nOur extensive LLM experiments also show superior performance after applying\nResonance RoPE to the current state-of-the-art RoPE scaling method, YaRN, on\nboth upstream language modeling tasks and a variety of downstream long-text\napplications.",
        "publication_date": "2024-02-29T19:02:03Z",
        "upvotes": 18
    },
    "2403.00745": {
        "url": "https://arxiv.org/abs/2403.00745",
        "title": "AtP*: An efficient and scalable method for localizing LLM behaviour to\n  components",
        "authors": [
            "J\u00e1nos Kram\u00e1r",
            "Tom Lieberum",
            "Rohin Shah",
            "Neel Nanda"
        ],
        "abstract": "Activation Patching is a method of directly computing causal attributions of\nbehavior to model components. However, applying it exhaustively requires a\nsweep with cost scaling linearly in the number of model components, which can\nbe prohibitively expensive for SoTA Large Language Models (LLMs). We\ninvestigate Attribution Patching (AtP), a fast gradient-based approximation to\nActivation Patching and find two classes of failure modes of AtP which lead to\nsignificant false negatives. We propose a variant of AtP called AtP*, with two\nchanges to address these failure modes while retaining scalability. We present\nthe first systematic study of AtP and alternative methods for faster activation\npatching and show that AtP significantly outperforms all other investigated\nmethods, with AtP* providing further significant improvement. Finally, we\nprovide a method to bound the probability of remaining false negatives of AtP*\nestimates.",
        "publication_date": "2024-03-01T18:43:51Z",
        "upvotes": 8
    },
    "2403.00483": {
        "url": "https://arxiv.org/abs/2403.00483",
        "title": "RealCustom: Narrowing Real Text Word for Real-Time Open-Domain\n  Text-to-Image Customization",
        "authors": [
            "Mengqi Huang",
            "Zhendong Mao",
            "Mingcong Liu",
            "Qian He",
            "Yongdong Zhang"
        ],
        "abstract": "Text-to-image customization, which aims to synthesize text-driven images for\nthe given subjects, has recently revolutionized content creation. Existing\nworks follow the pseudo-word paradigm, i.e., represent the given subjects as\npseudo-words and then compose them with the given text. However, the inherent\nentangled influence scope of pseudo-words with the given text results in a\ndual-optimum paradox, i.e., the similarity of the given subjects and the\ncontrollability of the given text could not be optimal simultaneously. We\npresent RealCustom that, for the first time, disentangles similarity from\ncontrollability by precisely limiting subject influence to relevant parts only,\nachieved by gradually narrowing real text word from its general connotation to\nthe specific subject and using its cross-attention to distinguish relevance.\nSpecifically, RealCustom introduces a novel \"train-inference\" decoupled\nframework: (1) during training, RealCustom learns general alignment between\nvisual conditions to original textual conditions by a novel adaptive scoring\nmodule to adaptively modulate influence quantity; (2) during inference, a novel\nadaptive mask guidance strategy is proposed to iteratively update the influence\nscope and influence quantity of the given subjects to gradually narrow the\ngeneration of the real text word. Comprehensive experiments demonstrate the\nsuperior real-time customization ability of RealCustom in the open domain,\nachieving both unprecedented similarity of the given subjects and\ncontrollability of the given text for the first time. The project page is\nhttps://corleone-huang.github.io/realcustom/.",
        "publication_date": "2024-03-01T12:12:09Z",
        "upvotes": 8
    },
    "2403.01779": {
        "url": "https://arxiv.org/abs/2403.01779",
        "title": "OOTDiffusion: Outfitting Fusion based Latent Diffusion for Controllable\n  Virtual Try-on",
        "authors": [
            "Yuhao Xu",
            "Tao Gu",
            "Weifeng Chen",
            "Chengcai Chen"
        ],
        "abstract": "We present OOTDiffusion, a novel network architecture for realistic and\ncontrollable image-based virtual try-on (VTON). We leverage the power of\npretrained latent diffusion models, designing an outfitting UNet to learn the\ngarment detail features. Without a redundant warping process, the garment\nfeatures are precisely aligned with the target human body via the proposed\noutfitting fusion in the self-attention layers of the denoising UNet. In order\nto further enhance the controllability, we introduce outfitting dropout to the\ntraining process, which enables us to adjust the strength of the garment\nfeatures through classifier-free guidance. Our comprehensive experiments on the\nVITON-HD and Dress Code datasets demonstrate that OOTDiffusion efficiently\ngenerates high-quality try-on results for arbitrary human and garment images,\nwhich outperforms other VTON methods in both realism and controllability,\nindicating an impressive breakthrough in virtual try-on. Our source code is\navailable at https://github.com/levihsu/OOTDiffusion.",
        "publication_date": "2024-03-04T07:17:44Z",
        "upvotes": 24
    },
    "2403.01422": {
        "url": "https://arxiv.org/abs/2403.01422",
        "title": "MovieLLM: Enhancing Long Video Understanding with AI-Generated Movies",
        "authors": [
            "Zhende Song",
            "Chenchen Wang",
            "Jiamu Sheng",
            "Chi Zhang",
            "Gang Yu",
            "Jiayuan Fan",
            "Tao Chen"
        ],
        "abstract": "The development of multimodal models has marked a significant step forward in\nhow machines understand videos. These models have shown promise in analyzing\nshort video clips. However, when it comes to longer formats like movies, they\noften fall short. The main hurdles are the lack of high-quality, diverse video\ndata and the intensive work required to collect or annotate such data. In the\nface of these challenges, we propose MovieLLM, a novel framework designed to\ncreate synthetic, high-quality data for long videos. This framework leverages\nthe power of GPT-4 and text-to-image models to generate detailed scripts and\ncorresponding visuals. Our approach stands out for its flexibility and\nscalability, making it a superior alternative to traditional data collection\nmethods. Our extensive experiments validate that the data produced by MovieLLM\nsignificantly improves the performance of multimodal models in understanding\ncomplex video narratives, overcoming the limitations of existing datasets\nregarding scarcity and bias.",
        "publication_date": "2024-03-03T07:43:39Z",
        "upvotes": 23
    },
    "2403.01800": {
        "url": "https://arxiv.org/abs/2403.01800",
        "title": "AtomoVideo: High Fidelity Image-to-Video Generation",
        "authors": [
            "Litong Gong",
            "Yiran Zhu",
            "Weijie Li",
            "Xiaoyang Kang",
            "Biao Wang",
            "Tiezheng Ge",
            "Bo Zheng"
        ],
        "abstract": "Recently, video generation has achieved significant rapid development based\non superior text-to-image generation techniques. In this work, we propose a\nhigh fidelity framework for image-to-video generation, named AtomoVideo. Based\non multi-granularity image injection, we achieve higher fidelity of the\ngenerated video to the given image. In addition, thanks to high quality\ndatasets and training strategies, we achieve greater motion intensity while\nmaintaining superior temporal consistency and stability. Our architecture\nextends flexibly to the video frame prediction task, enabling long sequence\nprediction through iterative generation. Furthermore, due to the design of\nadapter training, our approach can be well combined with existing personalized\nmodels and controllable modules. By quantitatively and qualitatively\nevaluation, AtomoVideo achieves superior results compared to popular methods,\nmore examples can be found on our project website:\nhttps://atomo-video.github.io/.",
        "publication_date": "2024-03-04T07:41:50Z",
        "upvotes": 18
    },
    "2403.01487": {
        "url": "https://arxiv.org/abs/2403.01487",
        "title": "InfiMM-HD: A Leap Forward in High-Resolution Multimodal Understanding",
        "authors": [
            "Haogeng Liu",
            "Quanzeng You",
            "Xiaotian Han",
            "Yiqi Wang",
            "Bohan Zhai",
            "Yongfei Liu",
            "Yunzhe Tao",
            "Huaibo Huang",
            "Ran He",
            "Hongxia Yang"
        ],
        "abstract": "Multimodal Large Language Models (MLLMs) have experienced significant\nadvancements recently. Nevertheless, challenges persist in the accurate\nrecognition and comprehension of intricate details within high-resolution\nimages. Despite being indispensable for the development of robust MLLMs, this\narea remains underinvestigated. To tackle this challenge, our work introduces\nInfiMM-HD, a novel architecture specifically designed for processing images of\ndifferent resolutions with low computational overhead. This innovation\nfacilitates the enlargement of MLLMs to higher-resolution capabilities.\nInfiMM-HD incorporates a cross-attention module and visual windows to reduce\ncomputation costs. By integrating this architectural design with a four-stage\ntraining pipeline, our model attains improved visual perception efficiently and\ncost-effectively. Empirical study underscores the robustness and effectiveness\nof InfiMM-HD, opening new avenues for exploration in related areas. Codes and\nmodels can be found at https://huggingface.co/Infi-MM/infimm-hd",
        "publication_date": "2024-03-03T11:39:41Z",
        "upvotes": 14
    },
    "2403.00818": {
        "url": "https://arxiv.org/abs/2403.00818",
        "title": "DenseMamba: State Space Models with Dense Hidden Connection for\n  Efficient Large Language Models",
        "authors": [
            "Wei He",
            "Kai Han",
            "Yehui Tang",
            "Chengcheng Wang",
            "Yujie Yang",
            "Tianyu Guo",
            "Yunhe Wang"
        ],
        "abstract": "Large language models (LLMs) face a daunting challenge due to the excessive\ncomputational and memory requirements of the commonly used Transformer\narchitecture. While state space model (SSM) is a new type of foundational\nnetwork architecture offering lower computational complexity, their performance\nhas yet to fully rival that of Transformers. This paper introduces DenseSSM, a\nnovel approach to enhance the flow of hidden information between layers in\nSSMs. By selectively integrating shallowlayer hidden states into deeper layers,\nDenseSSM retains fine-grained information crucial for the final output. Dense\nconnections enhanced DenseSSM still maintains the training parallelizability\nand inference efficiency. The proposed method can be widely applicable to\nvarious SSM types like RetNet and Mamba. With similar model size, DenseSSM\nachieves significant improvements, exemplified by DenseRetNet outperforming the\noriginal RetNet with up to 5% accuracy improvement on public benchmarks. code\nis avalaible at https://github.com/WailordHe/DenseSSM",
        "publication_date": "2024-02-26T09:21:59Z",
        "upvotes": 12
    },
    "2403.02084": {
        "url": "https://arxiv.org/abs/2403.02084",
        "title": "ResAdapter: Domain Consistent Resolution Adapter for Diffusion Models",
        "authors": [
            "Jiaxiang Cheng",
            "Pan Xie",
            "Xin Xia",
            "Jiashi Li",
            "Jie Wu",
            "Yuxi Ren",
            "Huixia Li",
            "Xuefeng Xiao",
            "Min Zheng",
            "Lean Fu"
        ],
        "abstract": "Recent advancement in text-to-image models (e.g., Stable Diffusion) and\ncorresponding personalized technologies (e.g., DreamBooth and LoRA) enables\nindividuals to generate high-quality and imaginative images. However, they\noften suffer from limitations when generating images with resolutions outside\nof their trained domain. To overcome this limitation, we present the Resolution\nAdapter (ResAdapter), a domain-consistent adapter designed for diffusion models\nto generate images with unrestricted resolutions and aspect ratios. Unlike\nother multi-resolution generation methods that process images of static\nresolution with complex post-process operations, ResAdapter directly generates\nimages with the dynamical resolution. Especially, after learning a deep\nunderstanding of pure resolution priors, ResAdapter trained on the general\ndataset, generates resolution-free images with personalized diffusion models\nwhile preserving their original style domain. Comprehensive experiments\ndemonstrate that ResAdapter with only 0.5M can process images with flexible\nresolutions for arbitrary diffusion models. More extended experiments\ndemonstrate that ResAdapter is compatible with other modules (e.g., ControlNet,\nIP-Adapter and LCM-LoRA) for image generation across a broad range of\nresolutions, and can be integrated into other multi-resolution model (e.g.,\nElasticDiffusion) for efficiently generating higher-resolution images. Project\nlink is https://res-adapter.github.io",
        "publication_date": "2024-03-04T14:36:56Z",
        "upvotes": 10
    },
    "2403.02151": {
        "url": "https://arxiv.org/abs/2403.02151",
        "title": "TripoSR: Fast 3D Object Reconstruction from a Single Image",
        "authors": [
            "Dmitry Tochilkin",
            "David Pankratz",
            "Zexiang Liu",
            "Zixuan Huang",
            "Adam Letts",
            "Yangguang Li",
            "Ding Liang",
            "Christian Laforte",
            "Varun Jampani",
            "Yan-Pei Cao"
        ],
        "abstract": "This technical report introduces TripoSR, a 3D reconstruction model\nleveraging transformer architecture for fast feed-forward 3D generation,\nproducing 3D mesh from a single image in under 0.5 seconds. Building upon the\nLRM network architecture, TripoSR integrates substantial improvements in data\nprocessing, model design, and training techniques. Evaluations on public\ndatasets show that TripoSR exhibits superior performance, both quantitatively\nand qualitatively, compared to other open-source alternatives. Released under\nthe MIT license, TripoSR is intended to empower researchers, developers, and\ncreatives with the latest advancements in 3D generative AI.",
        "publication_date": "2024-03-04T16:00:56Z",
        "upvotes": 8
    },
    "2403.01823": {
        "url": "https://arxiv.org/abs/2403.01823",
        "title": "RT-H: Action Hierarchies Using Language",
        "authors": [
            "Suneel Belkhale",
            "Tianli Ding",
            "Ted Xiao",
            "Pierre Sermanet",
            "Quon Vuong",
            "Jonathan Tompson",
            "Yevgen Chebotar",
            "Debidatta Dwibedi",
            "Dorsa Sadigh"
        ],
        "abstract": "Language provides a way to break down complex concepts into digestible\npieces. Recent works in robot imitation learning use language-conditioned\npolicies that predict actions given visual observations and the high-level task\nspecified in language. These methods leverage the structure of natural language\nto share data between semantically similar tasks (e.g., \"pick coke can\" and\n\"pick an apple\") in multi-task datasets. However, as tasks become more\nsemantically diverse (e.g., \"pick coke can\" and \"pour cup\"), sharing data\nbetween tasks becomes harder, so learning to map high-level tasks to actions\nrequires much more demonstration data. To bridge tasks and actions, our insight\nis to teach the robot the language of actions, describing low-level motions\nwith more fine-grained phrases like \"move arm forward\". Predicting these\nlanguage motions as an intermediate step between tasks and actions forces the\npolicy to learn the shared structure of low-level motions across seemingly\ndisparate tasks. Furthermore, a policy that is conditioned on language motions\ncan easily be corrected during execution through human-specified language\nmotions. This enables a new paradigm for flexible policies that can learn from\nhuman intervention in language. Our method RT-H builds an action hierarchy\nusing language motions: it first learns to predict language motions, and\nconditioned on this and the high-level task, it predicts actions, using visual\ncontext at all stages. We show that RT-H leverages this language-action\nhierarchy to learn policies that are more robust and flexible by effectively\ntapping into multi-task datasets. We show that these policies not only allow\nfor responding to language interventions, but can also learn from such\ninterventions and outperform methods that learn from teleoperated\ninterventions. Our website and videos are found at\nhttps://rt-hierarchy.github.io.",
        "publication_date": "2024-03-04T08:16:11Z",
        "upvotes": 7
    },
    "2403.01807": {
        "url": "https://arxiv.org/abs/2403.01807",
        "title": "ViewDiff: 3D-Consistent Image Generation with Text-to-Image Models",
        "authors": [
            "Lukas H\u00f6llein",
            "Alja\u017e Bo\u017ei\u010d",
            "Norman M\u00fcller",
            "David Novotny",
            "Hung-Yu Tseng",
            "Christian Richardt",
            "Michael Zollh\u00f6fer",
            "Matthias Nie\u00dfner"
        ],
        "abstract": "3D asset generation is getting massive amounts of attention, inspired by the\nrecent success of text-guided 2D content creation. Existing text-to-3D methods\nuse pretrained text-to-image diffusion models in an optimization problem or\nfine-tune them on synthetic data, which often results in non-photorealistic 3D\nobjects without backgrounds. In this paper, we present a method that leverages\npretrained text-to-image models as a prior, and learn to generate multi-view\nimages in a single denoising process from real-world data. Concretely, we\npropose to integrate 3D volume-rendering and cross-frame-attention layers into\neach block of the existing U-Net network of the text-to-image model. Moreover,\nwe design an autoregressive generation that renders more 3D-consistent images\nat any viewpoint. We train our model on real-world datasets of objects and\nshowcase its capabilities to generate instances with a variety of high-quality\nshapes and textures in authentic surroundings. Compared to the existing\nmethods, the results generated by our method are consistent, and have favorable\nvisual quality (-30% FID, -37% KID).",
        "publication_date": "2024-03-04T07:57:05Z",
        "upvotes": 7
    },
    "2403.02338": {
        "url": "https://arxiv.org/abs/2403.02338",
        "title": "Twisting Lids Off with Two Hands",
        "authors": [
            "Toru Lin",
            "Zhao-Heng Yin",
            "Haozhi Qi",
            "Pieter Abbeel",
            "Jitendra Malik"
        ],
        "abstract": "Manipulating objects with two multi-fingered hands has been a long-standing\nchallenge in robotics, attributed to the contact-rich nature of many\nmanipulation tasks and the complexity inherent in coordinating a\nhigh-dimensional bimanual system. In this work, we consider the problem of\ntwisting lids of various bottle-like objects with two hands, and demonstrate\nthat policies trained in simulation using deep reinforcement learning can be\neffectively transferred to the real world. With novel engineering insights into\nphysical modeling, real-time perception, and reward design, the policy\ndemonstrates generalization capabilities across a diverse set of unseen\nobjects, showcasing dynamic and dexterous behaviors. Our findings serve as\ncompelling evidence that deep reinforcement learning combined with sim-to-real\ntransfer remains a promising approach for addressing manipulation problems of\nunprecedented complexity.",
        "publication_date": "2024-03-04T18:59:30Z",
        "upvotes": 5
    },
    "2403.01444": {
        "url": "https://arxiv.org/abs/2403.01444",
        "title": "3DGStream: On-the-Fly Training of 3D Gaussians for Efficient Streaming\n  of Photo-Realistic Free-Viewpoint Videos",
        "authors": [
            "Jiakai Sun",
            "Han Jiao",
            "Guangyuan Li",
            "Zhanjie Zhang",
            "Lei Zhao",
            "Wei Xing"
        ],
        "abstract": "Constructing photo-realistic Free-Viewpoint Videos (FVVs) of dynamic scenes\nfrom multi-view videos remains a challenging endeavor. Despite the remarkable\nadvancements achieved by current neural rendering techniques, these methods\ngenerally require complete video sequences for offline training and are not\ncapable of real-time rendering. To address these constraints, we introduce\n3DGStream, a method designed for efficient FVV streaming of real-world dynamic\nscenes. Our method achieves fast on-the-fly per-frame reconstruction within 12\nseconds and real-time rendering at 200 FPS. Specifically, we utilize 3D\nGaussians (3DGs) to represent the scene. Instead of the na\\\"ive approach of\ndirectly optimizing 3DGs per-frame, we employ a compact Neural Transformation\nCache (NTC) to model the translations and rotations of 3DGs, markedly reducing\nthe training time and storage required for each FVV frame. Furthermore, we\npropose an adaptive 3DG addition strategy to handle emerging objects in dynamic\nscenes. Experiments demonstrate that 3DGStream achieves competitive performance\nin terms of rendering speed, image quality, training time, and model storage\nwhen compared with state-of-the-art methods.",
        "publication_date": "2024-03-03T08:42:40Z",
        "upvotes": 4
    },
    "2403.03163": {
        "url": "https://arxiv.org/abs/2403.03163",
        "title": "Design2Code: How Far Are We From Automating Front-End Engineering?",
        "authors": [
            "Chenglei Si",
            "Yanzhe Zhang",
            "Zhengyuan Yang",
            "Ruibo Liu",
            "Diyi Yang"
        ],
        "abstract": "Generative AI has made rapid advancements in recent years, achieving\nunprecedented capabilities in multimodal understanding and code generation.\nThis can enable a new paradigm of front-end development, in which multimodal\nLLMs might directly convert visual designs into code implementations. In this\nwork, we formalize this as a Design2Code task and conduct comprehensive\nbenchmarking. Specifically, we manually curate a benchmark of 484 diverse\nreal-world webpages as test cases and develop a set of automatic evaluation\nmetrics to assess how well current multimodal LLMs can generate the code\nimplementations that directly render into the given reference webpages, given\nthe screenshots as input. We also complement automatic metrics with\ncomprehensive human evaluations. We develop a suite of multimodal prompting\nmethods and show their effectiveness on GPT-4V and Gemini Pro Vision. We\nfurther finetune an open-source Design2Code-18B model that successfully matches\nthe performance of Gemini Pro Vision. Both human evaluation and automatic\nmetrics show that GPT-4V performs the best on this task compared to other\nmodels. Moreover, annotators think GPT-4V generated webpages can replace the\noriginal reference webpages in 49% of cases in terms of visual appearance and\ncontent; and perhaps surprisingly, in 64% of cases GPT-4V generated webpages\nare considered better than the original reference webpages. Our fine-grained\nbreak-down metrics indicate that open-source models mostly lag in recalling\nvisual elements from the input webpages and in generating correct layout\ndesigns, while aspects like text content and coloring can be drastically\nimproved with proper finetuning.",
        "publication_date": "2024-03-05T17:56:27Z",
        "upvotes": 92
    },
    "2403.03206": {
        "url": "https://arxiv.org/abs/2403.03206",
        "title": "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis",
        "authors": [
            "Patrick Esser",
            "Sumith Kulal",
            "Andreas Blattmann",
            "Rahim Entezari",
            "Jonas M\u00fcller",
            "Harry Saini",
            "Yam Levi",
            "Dominik Lorenz",
            "Axel Sauer",
            "Frederic Boesel",
            "Dustin Podell",
            "Tim Dockhorn",
            "Zion English",
            "Kyle Lacey",
            "Alex Goodwin",
            "Yannik Marek",
            "Robin Rombach"
        ],
        "abstract": "Diffusion models create data from noise by inverting the forward paths of\ndata towards noise and have emerged as a powerful generative modeling technique\nfor high-dimensional, perceptual data such as images and videos. Rectified flow\nis a recent generative model formulation that connects data and noise in a\nstraight line. Despite its better theoretical properties and conceptual\nsimplicity, it is not yet decisively established as standard practice. In this\nwork, we improve existing noise sampling techniques for training rectified flow\nmodels by biasing them towards perceptually relevant scales. Through a\nlarge-scale study, we demonstrate the superior performance of this approach\ncompared to established diffusion formulations for high-resolution\ntext-to-image synthesis. Additionally, we present a novel transformer-based\narchitecture for text-to-image generation that uses separate weights for the\ntwo modalities and enables a bidirectional flow of information between image\nand text tokens, improving text comprehension, typography, and human preference\nratings. We demonstrate that this architecture follows predictable scaling\ntrends and correlates lower validation loss to improved text-to-image synthesis\nas measured by various metrics and human evaluations. Our largest models\noutperform state-of-the-art models, and we will make our experimental data,\ncode, and model weights publicly available.",
        "publication_date": "2024-03-05T18:45:39Z",
        "upvotes": 38
    },
    "2403.03100": {
        "url": "https://arxiv.org/abs/2403.03100",
        "title": "NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and\n  Diffusion Models",
        "authors": [
            "Zeqian Ju",
            "Yuancheng Wang",
            "Kai Shen",
            "Xu Tan",
            "Detai Xin",
            "Dongchao Yang",
            "Yanqing Liu",
            "Yichong Leng",
            "Kaitao Song",
            "Siliang Tang",
            "Zhizheng Wu",
            "Tao Qin",
            "Xiang-Yang Li",
            "Wei Ye",
            "Shikun Zhang",
            "Jiang Bian",
            "Lei He",
            "Jinyu Li",
            "Sheng Zhao"
        ],
        "abstract": "While recent large-scale text-to-speech (TTS) models have achieved\nsignificant progress, they still fall short in speech quality, similarity, and\nprosody. Considering speech intricately encompasses various attributes (e.g.,\ncontent, prosody, timbre, and acoustic details) that pose significant\nchallenges for generation, a natural idea is to factorize speech into\nindividual subspaces representing different attributes and generate them\nindividually. Motivated by it, we propose NaturalSpeech 3, a TTS system with\nnovel factorized diffusion models to generate natural speech in a zero-shot\nway. Specifically, 1) we design a neural codec with factorized vector\nquantization (FVQ) to disentangle speech waveform into subspaces of content,\nprosody, timbre, and acoustic details; 2) we propose a factorized diffusion\nmodel to generate attributes in each subspace following its corresponding\nprompt. With this factorization design, NaturalSpeech 3 can effectively and\nefficiently model intricate speech with disentangled subspaces in a\ndivide-and-conquer way. Experiments show that NaturalSpeech 3 outperforms the\nstate-of-the-art TTS systems on quality, similarity, prosody, and\nintelligibility, and achieves on-par quality with human recordings.\nFurthermore, we achieve better performance by scaling to 1B parameters and 200K\nhours of training data.",
        "publication_date": "2024-03-05T16:35:25Z",
        "upvotes": 30
    },
    "2403.02677": {
        "url": "https://arxiv.org/abs/2403.02677",
        "title": "Finetuned Multimodal Language Models Are High-Quality Image-Text Data\n  Filters",
        "authors": [
            "Weizhi Wang",
            "Khalil Mrini",
            "Linjie Yang",
            "Sateesh Kumar",
            "Yu Tian",
            "Xifeng Yan",
            "Heng Wang"
        ],
        "abstract": "We propose a novel framework for filtering image-text data by leveraging\nfine-tuned Multimodal Language Models (MLMs). Our approach outperforms\npredominant filtering methods (e.g., CLIPScore) via integrating the recent\nadvances in MLMs. We design four distinct yet complementary metrics to\nholistically measure the quality of image-text data. A new pipeline is\nestablished to construct high-quality instruction data for fine-tuning MLMs as\ndata filters. Comparing with CLIPScore, our MLM filters produce more precise\nand comprehensive scores that directly improve the quality of filtered data and\nboost the performance of pre-trained models. We achieve significant\nimprovements over CLIPScore on popular foundation models (i.e., CLIP and BLIP2)\nand various downstream tasks. Our MLM filter can generalize to different models\nand tasks, and be used as a drop-in replacement for CLIPScore. An additional\nablation study is provided to verify our design choices for the MLM filter.",
        "publication_date": "2024-03-05T06:05:15Z",
        "upvotes": 16
    },
    "2403.02545": {
        "url": "https://arxiv.org/abs/2403.02545",
        "title": "Wukong: Towards a Scaling Law for Large-Scale Recommendation",
        "authors": [
            "Buyun Zhang",
            "Liang Luo",
            "Yuxin Chen",
            "Jade Nie",
            "Xi Liu",
            "Daifeng Guo",
            "Yanli Zhao",
            "Shen Li",
            "Yuchen Hao",
            "Yantao Yao",
            "Guna Lakshminarayanan",
            "Ellie Dingqiao Wen",
            "Jongsoo Park",
            "Maxim Naumov",
            "Wenlin Chen"
        ],
        "abstract": "Scaling laws play an instrumental role in the sustainable improvement in\nmodel quality. Unfortunately, recommendation models to date do not exhibit such\nlaws similar to those observed in the domain of large language models, due to\nthe inefficiencies of their upscaling mechanisms. This limitation poses\nsignificant challenges in adapting these models to increasingly more complex\nreal-world datasets. In this paper, we propose an effective network\narchitecture based purely on stacked factorization machines, and a synergistic\nupscaling strategy, collectively dubbed Wukong, to establish a scaling law in\nthe domain of recommendation. Wukong's unique design makes it possible to\ncapture diverse, any-order of interactions simply through taller and wider\nlayers. We conducted extensive evaluations on six public datasets, and our\nresults demonstrate that Wukong consistently outperforms state-of-the-art\nmodels quality-wise. Further, we assessed Wukong's scalability on an internal,\nlarge-scale dataset. The results show that Wukong retains its superiority in\nquality over state-of-the-art models, while holding the scaling law across two\norders of magnitude in model complexity, extending beyond 100 Gflop or\nequivalently up to Large Language Model (GPT-3) training compute scale, where\nprior arts fall short.",
        "publication_date": "2024-03-04T23:40:20Z",
        "upvotes": 15
    },
    "2403.02884": {
        "url": "https://arxiv.org/abs/2403.02884",
        "title": "MathScale: Scaling Instruction Tuning for Mathematical Reasoning",
        "authors": [
            "Zhengyang Tang",
            "Xingxing Zhang",
            "Benyou Wang",
            "Furu Wei"
        ],
        "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in\nproblem-solving. However, their proficiency in solving mathematical problems\nremains inadequate. We propose MathScale, a simple and scalable method to\ncreate high-quality mathematical reasoning data using frontier LLMs (e.g., {\\tt\nGPT-3.5}). Inspired by the cognitive mechanism in human mathematical learning,\nit first extracts topics and knowledge points from seed math questions and then\nbuild a concept graph, which is subsequently used to generate new math\nquestions. MathScale exhibits effective scalability along the size axis of the\nmath dataset that we generate. As a result, we create a mathematical reasoning\ndataset (MathScaleQA) containing two million math question-answer pairs. To\nevaluate mathematical reasoning abilities of LLMs comprehensively, we construct\n{\\sc MwpBench}, a benchmark of Math Word Problems, which is a collection of ten\ndatasets (including GSM8K and MATH) covering K-12, college, and competition\nlevel math problems. We apply MathScaleQA to fine-tune open-source LLMs (e.g.,\nLLaMA-2 and Mistral), resulting in significantly improved capabilities in\nmathematical reasoning. Evaluated on {\\sc MwpBench}, MathScale-7B achieves\nstate-of-the-art performance across all datasets, surpassing its best peers of\nequivalent size by 42.9\\% in micro average accuracy and 43.7\\% in macro average\naccuracy, respectively.",
        "publication_date": "2024-03-05T11:42:59Z",
        "upvotes": 14
    },
    "2403.02775": {
        "url": "https://arxiv.org/abs/2403.02775",
        "title": "EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs",
        "authors": [
            "Hanlin Tang",
            "Yifu Sun",
            "Decheng Wu",
            "Kai Liu",
            "Jianchen Zhu",
            "Zhanhui Kang"
        ],
        "abstract": "Large language models (LLMs) have proven to be very superior to conventional\nmethods in various tasks. However, their expensive computations and high memory\nrequirements are prohibitive for deployment. Model quantization is an effective\nmethod for reducing this overhead. The problem is that in most previous works,\nthe quantized model was calibrated using few samples from the training data,\nwhich might affect the generalization of the quantized LLMs to unknown cases\nand tasks. Hence in this work, we explore an important question: Can we design\na data-independent quantization method for LLMs to guarantee its generalization\nperformance? In this work, we propose EasyQuant, a training-free and\ndata-independent weight-only quantization algorithm for LLMs. Our observation\nindicates that two factors: outliers in the weight and quantization ranges, are\nessential for reducing the quantization error. Therefore, in EasyQuant, we\nleave the outliers (less than 1%) unchanged and optimize the quantization range\nto reduce the reconstruction error. With these methods, we surprisingly find\nthat EasyQuant achieves comparable performance to the original model. Since\nEasyQuant does not depend on any training data, the generalization performance\nof quantized LLMs is safely guaranteed. Moreover, EasyQuant can be implemented\nin parallel so that the quantized model could be attained in a few minutes even\nfor LLMs over 100B. To our best knowledge, we are the first work that achieves\nalmost lossless quantization performance for LLMs under a data-independent\nsetting and our algorithm runs over 10 times faster than the data-dependent\nmethods.",
        "publication_date": "2024-03-05T08:45:30Z",
        "upvotes": 11
    },
    "2403.03194": {
        "url": "https://arxiv.org/abs/2403.03194",
        "title": "MAGID: An Automated Pipeline for Generating Synthetic Multi-modal\n  Datasets",
        "authors": [
            "Hossein Aboutalebi",
            "Hwanjun Song",
            "Yusheng Xie",
            "Arshit Gupta",
            "Justin Sun",
            "Hang Su",
            "Igor Shalyminov",
            "Nikolaos Pappas",
            "Siffi Singh",
            "Saab Mansour"
        ],
        "abstract": "Development of multimodal interactive systems is hindered by the lack of\nrich, multimodal (text, images) conversational data, which is needed in large\nquantities for LLMs. Previous approaches augment textual dialogues with\nretrieved images, posing privacy, diversity, and quality constraints. In this\nwork, we introduce \\textbf{M}ultimodal \\textbf{A}ugmented \\textbf{G}enerative\n\\textbf{I}mages \\textbf{D}ialogues (MAGID), a framework to augment text-only\ndialogues with diverse and high-quality images. Subsequently, a diffusion model\nis applied to craft corresponding images, ensuring alignment with the\nidentified text. Finally, MAGID incorporates an innovative feedback loop\nbetween an image description generation module (textual LLM) and image quality\nmodules (addressing aesthetics, image-text matching, and safety), that work in\ntandem to generate high-quality and multi-modal dialogues. We compare MAGID to\nother SOTA baselines on three dialogue datasets, using automated and human\nevaluation. Our results show that MAGID is comparable to or better than\nbaselines, with significant improvements in human evaluation, especially\nagainst retrieval baselines where the image database is small.",
        "publication_date": "2024-03-05T18:31:28Z",
        "upvotes": 10
    },
    "2403.02626": {
        "url": "https://arxiv.org/abs/2403.02626",
        "title": "Modeling Collaborator: Enabling Subjective Vision Classification With\n  Minimal Human Effort via LLM Tool-Use",
        "authors": [
            "Imad Eddine Toubal",
            "Aditya Avinash",
            "Neil Gordon Alldrin",
            "Jan Dlabal",
            "Wenlei Zhou",
            "Enming Luo",
            "Otilia Stretcu",
            "Hao Xiong",
            "Chun-Ta Lu",
            "Howard Zhou",
            "Ranjay Krishna",
            "Ariel Fuxman",
            "Tom Duerig"
        ],
        "abstract": "From content moderation to wildlife conservation, the number of applications\nthat require models to recognize nuanced or subjective visual concepts is\ngrowing. Traditionally, developing classifiers for such concepts requires\nsubstantial manual effort measured in hours, days, or even months to identify\nand annotate data needed for training. Even with recently proposed Agile\nModeling techniques, which enable rapid bootstrapping of image classifiers,\nusers are still required to spend 30 minutes or more of monotonous, repetitive\ndata labeling just to train a single classifier. Drawing on Fiske's Cognitive\nMiser theory, we propose a new framework that alleviates manual effort by\nreplacing human labeling with natural language interactions, reducing the total\neffort required to define a concept by an order of magnitude: from labeling\n2,000 images to only 100 plus some natural language interactions. Our framework\nleverages recent advances in foundation models, both large language models and\nvision-language models, to carve out the concept space through conversation and\nby automatically labeling training data points. Most importantly, our framework\neliminates the need for crowd-sourced annotations. Moreover, our framework\nultimately produces lightweight classification models that are deployable in\ncost-sensitive scenarios. Across 15 subjective concepts and across 2 public\nimage classification datasets, our trained models outperform traditional Agile\nModeling as well as state-of-the-art zero-shot classification models like\nALIGN, CLIP, CuPL, and large visual question-answering models like PaLI-X.",
        "publication_date": "2024-03-05T03:34:11Z",
        "upvotes": 9
    },
    "2403.03003": {
        "url": "https://arxiv.org/abs/2403.03003",
        "title": "Feast Your Eyes: Mixture-of-Resolution Adaptation for Multimodal Large\n  Language Models",
        "authors": [
            "Gen Luo",
            "Yiyi Zhou",
            "Yuxin Zhang",
            "Xiawu Zheng",
            "Xiaoshuai Sun",
            "Rongrong Ji"
        ],
        "abstract": "Despite remarkable progress, existing multimodal large language models\n(MLLMs) are still inferior in granular visual recognition. Contrary to previous\nworks, we study this problem from the perspective of image resolution, and\nreveal that a combination of low- and high-resolution visual features can\neffectively mitigate this shortcoming. Based on this observation, we propose a\nnovel and efficient method for MLLMs, termed Mixture-of-Resolution Adaptation\n(MRA). In particular, MRA adopts two visual pathways for images with different\nresolutions, where high-resolution visual information is embedded into the\nlow-resolution pathway via the novel mixture-of-resolution adapters\n(MR-Adapters). This design also greatly reduces the input sequence length of\nMLLMs. To validate MRA, we apply it to a recent MLLM called LLaVA, and term the\nnew model LLaVA-HR. We conduct extensive experiments on 11 vision-language (VL)\ntasks, which show that LLaVA-HR outperforms existing MLLMs on 8 VL tasks, e.g.,\n+9.4% on TextVQA. More importantly, both training and inference of LLaVA-HR\nremain efficient with MRA, e.g., 20 training hours and 3$\\times$ inference\nspeed than LLaVA-1.5. Source codes are released at:\nhttps://github.com/luogen1996/LLaVA-HR.",
        "publication_date": "2024-03-05T14:31:24Z",
        "upvotes": 8
    },
    "2403.02709": {
        "url": "https://arxiv.org/abs/2403.02709",
        "title": "RT-Sketch: Goal-Conditioned Imitation Learning from Hand-Drawn Sketches",
        "authors": [
            "Priya Sundaresan",
            "Quan Vuong",
            "Jiayuan Gu",
            "Peng Xu",
            "Ted Xiao",
            "Sean Kirmani",
            "Tianhe Yu",
            "Michael Stark",
            "Ajinkya Jain",
            "Karol Hausman",
            "Dorsa Sadigh",
            "Jeannette Bohg",
            "Stefan Schaal"
        ],
        "abstract": "Natural language and images are commonly used as goal representations in\ngoal-conditioned imitation learning (IL). However, natural language can be\nambiguous and images can be over-specified. In this work, we propose hand-drawn\nsketches as a modality for goal specification in visual imitation learning.\nSketches are easy for users to provide on the fly like language, but similar to\nimages they can also help a downstream policy to be spatially-aware and even go\nbeyond images to disambiguate task-relevant from task-irrelevant objects. We\npresent RT-Sketch, a goal-conditioned policy for manipulation that takes a\nhand-drawn sketch of the desired scene as input, and outputs actions. We train\nRT-Sketch on a dataset of paired trajectories and corresponding synthetically\ngenerated goal sketches. We evaluate this approach on six manipulation skills\ninvolving tabletop object rearrangements on an articulated countertop.\nExperimentally we find that RT-Sketch is able to perform on a similar level to\nimage or language-conditioned agents in straightforward settings, while\nachieving greater robustness when language goals are ambiguous or visual\ndistractors are present. Additionally, we show that RT-Sketch has the capacity\nto interpret and act upon sketches with varied levels of specificity, ranging\nfrom minimal line drawings to detailed, colored drawings. For supplementary\nmaterial and videos, please refer to our website: http://rt-sketch.github.io.",
        "publication_date": "2024-03-05T07:00:46Z",
        "upvotes": 6
    },
    "2403.02460": {
        "url": "https://arxiv.org/abs/2403.02460",
        "title": "MagicClay: Sculpting Meshes With Generative Neural Fields",
        "authors": [
            "Amir Barda",
            "Vladimir G. Kim",
            "Noam Aigerman",
            "Amit H. Bermano",
            "Thibault Groueix"
        ],
        "abstract": "The recent developments in neural fields have brought phenomenal capabilities\nto the field of shape generation, but they lack crucial properties, such as\nincremental control - a fundamental requirement for artistic work. Triangular\nmeshes, on the other hand, are the representation of choice for most geometry\nrelated tasks, offering efficiency and intuitive control, but do not lend\nthemselves to neural optimization. To support downstream tasks, previous art\ntypically proposes a two-step approach, where first a shape is generated using\nneural fields, and then a mesh is extracted for further processing. Instead, in\nthis paper we introduce a hybrid approach that maintains both a mesh and a\nSigned Distance Field (SDF) representations consistently. Using this\nrepresentation, we introduce MagicClay - an artist friendly tool for sculpting\nregions of a mesh according to textual prompts while keeping other regions\nuntouched. Our framework carefully and efficiently balances consistency between\nthe representations and regularizations in every step of the shape\noptimization; Relying on the mesh representation, we show how to render the SDF\nat higher resolutions and faster. In addition, we employ recent work in\ndifferentiable mesh reconstruction to adaptively allocate triangles in the mesh\nwhere required, as indicated by the SDF. Using an implemented prototype, we\ndemonstrate superior generated geometry compared to the state-of-the-art, and\nnovel consistent control, allowing sequential prompt-based edits to the same\nmesh for the first time.",
        "publication_date": "2024-03-04T20:20:14Z",
        "upvotes": 6
    },
    "2403.02827": {
        "url": "https://arxiv.org/abs/2403.02827",
        "title": "Tuning-Free Noise Rectification for High Fidelity Image-to-Video\n  Generation",
        "authors": [
            "Weijie Li",
            "Litong Gong",
            "Yiran Zhu",
            "Fanda Fan",
            "Biao Wang",
            "Tiezheng Ge",
            "Bo Zheng"
        ],
        "abstract": "Image-to-video (I2V) generation tasks always suffer from keeping high\nfidelity in the open domains. Traditional image animation techniques primarily\nfocus on specific domains such as faces or human poses, making them difficult\nto generalize to open domains. Several recent I2V frameworks based on diffusion\nmodels can generate dynamic content for open domain images but fail to maintain\nfidelity. We found that two main factors of low fidelity are the loss of image\ndetails and the noise prediction biases during the denoising process. To this\nend, we propose an effective method that can be applied to mainstream video\ndiffusion models. This method achieves high fidelity based on supplementing\nmore precise image information and noise rectification. Specifically, given a\nspecified image, our method first adds noise to the input image latent to keep\nmore details, then denoises the noisy latent with proper rectification to\nalleviate the noise prediction biases. Our method is tuning-free and\nplug-and-play. The experimental results demonstrate the effectiveness of our\napproach in improving the fidelity of generated videos. For more image-to-video\ngenerated results, please refer to the project website:\nhttps://noise-rectification.github.io.",
        "publication_date": "2024-03-05T09:57:47Z",
        "upvotes": 5
    },
    "2403.03507": {
        "url": "https://arxiv.org/abs/2403.03507",
        "title": "GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection",
        "authors": [
            "Jiawei Zhao",
            "Zhenyu Zhang",
            "Beidi Chen",
            "Zhangyang Wang",
            "Anima Anandkumar",
            "Yuandong Tian"
        ],
        "abstract": "Training Large Language Models (LLMs) presents significant memory challenges,\npredominantly due to the growing size of weights and optimizer states. Common\nmemory-reduction approaches, such as low-rank adaptation (LoRA), add a\ntrainable low-rank matrix to the frozen pre-trained weight in each layer,\nreducing trainable parameters and optimizer states. However, such approaches\ntypically underperform training with full-rank weights in both pre-training and\nfine-tuning stages since they limit the parameter search to a low-rank subspace\nand alter the training dynamics, and further, may require full-rank warm start.\nIn this work, we propose Gradient Low-Rank Projection (GaLore), a training\nstrategy that allows full-parameter learning but is more memory-efficient than\ncommon low-rank adaptation methods such as LoRA. Our approach reduces memory\nusage by up to 65.5% in optimizer states while maintaining both efficiency and\nperformance for pre-training on LLaMA 1B and 7B architectures with C4 dataset\nwith up to 19.7B tokens, and on fine-tuning RoBERTa on GLUE tasks. Our 8-bit\nGaLore further reduces optimizer memory by up to 82.5% and total training\nmemory by 63.3%, compared to a BF16 baseline. Notably, we demonstrate, for the\nfirst time, the feasibility of pre-training a 7B model on consumer GPUs with\n24GB memory (e.g., NVIDIA RTX 4090) without model parallel, checkpointing, or\noffloading strategies.",
        "publication_date": "2024-03-06T07:29:57Z",
        "upvotes": 166
    },
    "2403.03883": {
        "url": "https://arxiv.org/abs/2403.03883",
        "title": "SaulLM-7B: A pioneering Large Language Model for Law",
        "authors": [
            "Pierre Colombo",
            "Telmo Pessoa Pires",
            "Malik Boudiaf",
            "Dominic Culver",
            "Rui Melo",
            "Caio Corro",
            "Andre F. T. Martins",
            "Fabrizio Esposito",
            "Vera L\u00facia Raposo",
            "Sofia Morgado",
            "Michael Desa"
        ],
        "abstract": "In this paper, we introduce SaulLM-7B, a large language model (LLM) tailored\nfor the legal domain. With 7 billion parameters, SaulLM-7B is the first LLM\ndesigned explicitly for legal text comprehension and generation. Leveraging the\nMistral 7B architecture as its foundation, SaulLM-7B is trained on an English\nlegal corpus of over 30 billion tokens. SaulLM-7B exhibits state-of-the-art\nproficiency in understanding and processing legal documents. Additionally, we\npresent a novel instructional fine-tuning method that leverages legal datasets\nto further enhance SaulLM-7B's performance in legal tasks. SaulLM-7B is\nreleased under the MIT License.",
        "publication_date": "2024-03-06T17:42:16Z",
        "upvotes": 63
    },
    "2403.03853": {
        "url": "https://arxiv.org/abs/2403.03853",
        "title": "ShortGPT: Layers in Large Language Models are More Redundant Than You\n  Expect",
        "authors": [
            "Xin Men",
            "Mingyu Xu",
            "Qingyu Zhang",
            "Bingning Wang",
            "Hongyu Lin",
            "Yaojie Lu",
            "Xianpei Han",
            "Weipeng Chen"
        ],
        "abstract": "As Large Language Models (LLMs) continue to advance in performance, their\nsize has escalated significantly, with current LLMs containing billions or even\ntrillions of parameters. However, in this study, we discovered that many layers\nof LLMs exhibit high similarity, and some layers play a negligible role in\nnetwork functionality. Based on this observation, we define a metric called\nBlock Influence (BI) to gauge the significance of each layer in LLMs. We then\npropose a straightforward pruning approach: layer removal, in which we directly\ndelete the redundant layers in LLMs based on their BI scores. Experiments\ndemonstrate that our method, which we call ShortGPT, significantly outperforms\nprevious state-of-the-art (SOTA) methods in model pruning. Moreover, ShortGPT\nis orthogonal to quantization-like methods, enabling further reduction in\nparameters and computation. The ability to achieve better results through\nsimple layer removal, as opposed to more complex pruning techniques, suggests a\nhigh degree of redundancy in the model architecture.",
        "publication_date": "2024-03-06T17:04:18Z",
        "upvotes": 61
    },
    "2403.03870": {
        "url": "https://arxiv.org/abs/2403.03870",
        "title": "Learning to Decode Collaboratively with Multiple Language Models",
        "authors": [
            "Shannon Zejiang Shen",
            "Hunter Lang",
            "Bailin Wang",
            "Yoon Kim",
            "David Sontag"
        ],
        "abstract": "We propose a method to teach multiple large language models (LLM) to\ncollaborate by interleaving their generations at the token level. We model the\ndecision of which LLM generates the next token as a latent variable. By\noptimizing the marginal likelihood of a training set under our latent variable\nmodel, the base LLM automatically learns when to generate itself and when to\ncall on one of the ``assistant'' language models to generate, all without\ndirect supervision. Token-level collaboration during decoding allows for a\nfusion of each model's expertise in a manner tailored to the specific task at\nhand. Our collaborative decoding is especially useful in cross-domain settings\nwhere a generalist base LLM learns to invoke domain expert models. On\ninstruction-following, domain-specific QA, and reasoning tasks, we show that\nthe performance of the joint system exceeds that of the individual models.\nThrough qualitative analysis of the learned latent decisions, we show models\ntrained with our method exhibit several interesting collaboration patterns,\ne.g., template-filling. Our code is available at\nhttps://github.com/clinicalml/co-llm.",
        "publication_date": "2024-03-06T17:23:28Z",
        "upvotes": 17
    },
    "2403.03346": {
        "url": "https://arxiv.org/abs/2403.03346",
        "title": "Enhancing Vision-Language Pre-training with Rich Supervisions",
        "authors": [
            "Yuan Gao",
            "Kunyu Shi",
            "Pengkai Zhu",
            "Edouard Belval",
            "Oren Nuriel",
            "Srikar Appalaraju",
            "Shabnam Ghadar",
            "Vijay Mahadevan",
            "Zhuowen Tu",
            "Stefano Soatto"
        ],
        "abstract": "We propose Strongly Supervised pre-training with ScreenShots (S4) - a novel\npre-training paradigm for Vision-Language Models using data from large-scale\nweb screenshot rendering. Using web screenshots unlocks a treasure trove of\nvisual and textual cues that are not present in using image-text pairs. In S4,\nwe leverage the inherent tree-structured hierarchy of HTML elements and the\nspatial localization to carefully design 10 pre-training tasks with large scale\nannotated data. These tasks resemble downstream tasks across different domains\nand the annotations are cheap to obtain. We demonstrate that, compared to\ncurrent screenshot pre-training objectives, our innovative pre-training method\nsignificantly enhances performance of image-to-text model in nine varied and\npopular downstream tasks - up to 76.1% improvements on Table Detection, and at\nleast 1% on Widget Captioning.",
        "publication_date": "2024-03-05T22:14:58Z",
        "upvotes": 12
    },
    "2403.03950": {
        "url": "https://arxiv.org/abs/2403.03950",
        "title": "Stop Regressing: Training Value Functions via Classification for\n  Scalable Deep RL",
        "authors": [
            "Jesse Farebrother",
            "Jordi Orbay",
            "Quan Vuong",
            "Adrien Ali Ta\u00efga",
            "Yevgen Chebotar",
            "Ted Xiao",
            "Alex Irpan",
            "Sergey Levine",
            "Pablo Samuel Castro",
            "Aleksandra Faust",
            "Aviral Kumar",
            "Rishabh Agarwal"
        ],
        "abstract": "Value functions are a central component of deep reinforcement learning (RL).\nThese functions, parameterized by neural networks, are trained using a mean\nsquared error regression objective to match bootstrapped target values.\nHowever, scaling value-based RL methods that use regression to large networks,\nsuch as high-capacity Transformers, has proven challenging. This difficulty is\nin stark contrast to supervised learning: by leveraging a cross-entropy\nclassification loss, supervised methods have scaled reliably to massive\nnetworks. Observing this discrepancy, in this paper, we investigate whether the\nscalability of deep RL can also be improved simply by using classification in\nplace of regression for training value functions. We demonstrate that value\nfunctions trained with categorical cross-entropy significantly improves\nperformance and scalability in a variety of domains. These include: single-task\nRL on Atari 2600 games with SoftMoEs, multi-task RL on Atari with large-scale\nResNets, robotic manipulation with Q-transformers, playing Chess without\nsearch, and a language-agent Wordle task with high-capacity Transformers,\nachieving state-of-the-art results on these domains. Through careful analysis,\nwe show that the benefits of categorical cross-entropy primarily stem from its\nability to mitigate issues inherent to value-based RL, such as noisy targets\nand non-stationarity. Overall, we argue that a simple shift to training value\nfunctions with categorical cross-entropy can yield substantial improvements in\nthe scalability of deep RL at little-to-no cost.",
        "publication_date": "2024-03-06T18:55:47Z",
        "upvotes": 11
    },
    "2403.03956": {
        "url": "https://arxiv.org/abs/2403.03956",
        "title": "Backtracing: Retrieving the Cause of the Query",
        "authors": [
            "Rose E. Wang",
            "Pawan Wirawarn",
            "Omar Khattab",
            "Noah Goodman",
            "Dorottya Demszky"
        ],
        "abstract": "Many online content portals allow users to ask questions to supplement their\nunderstanding (e.g., of lectures). While information retrieval (IR) systems may\nprovide answers for such user queries, they do not directly assist content\ncreators -- such as lecturers who want to improve their content -- identify\nsegments that _caused_ a user to ask those questions. We introduce the task of\nbacktracing, in which systems retrieve the text segment that most likely caused\na user query. We formalize three real-world domains for which backtracing is\nimportant in improving content delivery and communication: understanding the\ncause of (a) student confusion in the Lecture domain, (b) reader curiosity in\nthe News Article domain, and (c) user emotion in the Conversation domain. We\nevaluate the zero-shot performance of popular information retrieval methods and\nlanguage modeling methods, including bi-encoder, re-ranking and\nlikelihood-based methods and ChatGPT. While traditional IR systems retrieve\nsemantically relevant information (e.g., details on \"projection matrices\" for a\nquery \"does projecting multiple times still lead to the same point?\"), they\noften miss the causally relevant context (e.g., the lecturer states \"projecting\ntwice gets me the same answer as one projection\"). Our results show that there\nis room for improvement on backtracing and it requires new retrieval\napproaches. We hope our benchmark serves to improve future retrieval systems\nfor backtracing, spawning systems that refine content generation and identify\nlinguistic triggers influencing user queries. Our code and data are\nopen-sourced: https://github.com/rosewang2008/backtracing.",
        "publication_date": "2024-03-06T18:59:02Z",
        "upvotes": 10
    },
    "2403.03954": {
        "url": "https://arxiv.org/abs/2403.03954",
        "title": "3D Diffusion Policy: Generalizable Visuomotor Policy Learning via Simple\n  3D Representations",
        "authors": [
            "Yanjie Ze",
            "Gu Zhang",
            "Kangning Zhang",
            "Chenyuan Hu",
            "Muhan Wang",
            "Huazhe Xu"
        ],
        "abstract": "Imitation learning provides an efficient way to teach robots dexterous\nskills; however, learning complex skills robustly and generalizablely usually\nconsumes large amounts of human demonstrations. To tackle this challenging\nproblem, we present 3D Diffusion Policy (DP3), a novel visual imitation\nlearning approach that incorporates the power of 3D visual representations into\ndiffusion policies, a class of conditional action generative models. The core\ndesign of DP3 is the utilization of a compact 3D visual representation,\nextracted from sparse point clouds with an efficient point encoder. In our\nexperiments involving 72 simulation tasks, DP3 successfully handles most tasks\nwith just 10 demonstrations and surpasses baselines with a 24.2% relative\nimprovement. In 4 real robot tasks, DP3 demonstrates precise control with a\nhigh success rate of 85%, given only 40 demonstrations of each task, and shows\nexcellent generalization abilities in diverse aspects, including space,\nviewpoint, appearance, and instance. Interestingly, in real robot experiments,\nDP3 rarely violates safety requirements, in contrast to baseline methods which\nfrequently do, necessitating human intervention. Our extensive evaluation\nhighlights the critical importance of 3D representations in real-world robot\nlearning. Videos, code, and data are available on\nhttps://3d-diffusion-policy.github.io .",
        "publication_date": "2024-03-06T18:58:49Z",
        "upvotes": 10
    },
    "2403.03234": {
        "url": "https://arxiv.org/abs/2403.03234",
        "title": "Caduceus: Bi-Directional Equivariant Long-Range DNA Sequence Modeling",
        "authors": [
            "Yair Schiff",
            "Chia-Hsiang Kao",
            "Aaron Gokaslan",
            "Tri Dao",
            "Albert Gu",
            "Volodymyr Kuleshov"
        ],
        "abstract": "Large-scale sequence modeling has sparked rapid advances that now extend into\nbiology and genomics. However, modeling genomic sequences introduces challenges\nsuch as the need to model long-range token interactions, the effects of\nupstream and downstream regions of the genome, and the reverse complementarity\n(RC) of DNA. Here, we propose an architecture motivated by these challenges\nthat builds off the long-range Mamba block, and extends it to a BiMamba\ncomponent that supports bi-directionality, and to a MambaDNA block that\nadditionally supports RC equivariance. We use MambaDNA as the basis of\nCaduceus, the first family of RC equivariant bi-directional long-range DNA\nlanguage models, and we introduce pre-training and fine-tuning strategies that\nyield Caduceus DNA foundation models. Caduceus outperforms previous long-range\nmodels on downstream benchmarks; on a challenging long-range variant effect\nprediction task, Caduceus exceeds the performance of 10x larger models that do\nnot leverage bi-directionality or equivariance.",
        "publication_date": "2024-03-05T01:42:51Z",
        "upvotes": 9
    },
    "2403.04652": {
        "url": "https://arxiv.org/abs/2403.04652",
        "title": "Yi: Open Foundation Models by 01.AI",
        "authors": [
            "01. AI",
            ":",
            "Alex Young",
            "Bei Chen",
            "Chao Li",
            "Chengen Huang",
            "Ge Zhang",
            "Guanwei Zhang",
            "Heng Li",
            "Jiangcheng Zhu",
            "Jianqun Chen",
            "Jing Chang",
            "Kaidong Yu",
            "Peng Liu",
            "Qiang Liu",
            "Shawn Yue",
            "Senbin Yang",
            "Shiming Yang",
            "Tao Yu",
            "Wen Xie",
            "Wenhao Huang",
            "Xiaohui Hu",
            "Xiaoyi Ren",
            "Xinyao Niu",
            "Pengcheng Nie",
            "Yuchi Xu",
            "Yudong Liu",
            "Yue Wang",
            "Yuxuan Cai",
            "Zhenyu Gu",
            "Zhiyuan Liu",
            "Zonghong Dai"
        ],
        "abstract": "We introduce the Yi model family, a series of language and multimodal models\nthat demonstrate strong multi-dimensional capabilities. The Yi model family is\nbased on 6B and 34B pretrained language models, then we extend them to chat\nmodels, 200K long context models, depth-upscaled models, and vision-language\nmodels. Our base models achieve strong performance on a wide range of\nbenchmarks like MMLU, and our finetuned chat models deliver strong human\npreference rate on major evaluation platforms like AlpacaEval and Chatbot\nArena. Building upon our scalable super-computing infrastructure and the\nclassical transformer architecture, we attribute the performance of Yi models\nprimarily to its data quality resulting from our data-engineering efforts. For\npretraining, we construct 3.1 trillion tokens of English and Chinese corpora\nusing a cascaded data deduplication and quality filtering pipeline. For\nfinetuning, we polish a small scale (less than 10K) instruction dataset over\nmultiple iterations such that every single instance has been verified directly\nby our machine learning engineers. For vision-language, we combine the chat\nlanguage model with a vision transformer encoder and train the model to align\nvisual representations to the semantic space of the language model. We further\nextend the context length to 200K through lightweight continual pretraining and\ndemonstrate strong needle-in-a-haystack retrieval performance. We show that\nextending the depth of the pretrained checkpoint through continual pretraining\nfurther improves performance. We believe that given our current results,\ncontinuing to scale up model parameters using thoroughly optimized data will\nlead to even stronger frontier models.",
        "publication_date": "2024-03-07T16:52:49Z",
        "upvotes": 57
    },
    "2403.04642": {
        "url": "https://arxiv.org/abs/2403.04642",
        "title": "Teaching Large Language Models to Reason with Reinforcement Learning",
        "authors": [
            "Alex Havrilla",
            "Yuqing Du",
            "Sharath Chandra Raparthy",
            "Christoforos Nalmpantis",
            "Jane Dwivedi-Yu",
            "Maksym Zhuravinskyi",
            "Eric Hambro",
            "Sainbayar Sukhbaatar",
            "Roberta Raileanu"
        ],
        "abstract": "Reinforcement Learning from Human Feedback (\\textbf{RLHF}) has emerged as a\ndominant approach for aligning LLM outputs with human preferences. Inspired by\nthe success of RLHF, we study the performance of multiple algorithms that learn\nfrom feedback (Expert Iteration, Proximal Policy Optimization (\\textbf{PPO}),\nReturn-Conditioned RL) on improving LLM reasoning capabilities. We investigate\nboth sparse and dense rewards provided to the LLM both heuristically and via a\nlearned reward model. We additionally start from multiple model sizes and\ninitializations both with and without supervised fine-tuning (\\textbf{SFT})\ndata. Overall, we find all algorithms perform comparably, with Expert Iteration\nperforming best in most cases. Surprisingly, we find the sample complexity of\nExpert Iteration is similar to that of PPO, requiring at most on the order of\n$10^6$ samples to converge from a pretrained checkpoint. We investigate why\nthis is the case, concluding that during RL training models fail to explore\nsignificantly beyond solutions already produced by SFT models. Additionally, we\ndiscuss a trade off between maj@1 and pass@96 metric performance during SFT\ntraining and how conversely RL training improves both simultaneously. We then\nconclude by discussing the implications of our findings for RLHF and the future\nrole of RL in LLM fine-tuning.",
        "publication_date": "2024-03-07T16:36:29Z",
        "upvotes": 42
    },
    "2403.04132": {
        "url": "https://arxiv.org/abs/2403.04132",
        "title": "Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference",
        "authors": [
            "Wei-Lin Chiang",
            "Lianmin Zheng",
            "Ying Sheng",
            "Anastasios Nikolas Angelopoulos",
            "Tianle Li",
            "Dacheng Li",
            "Hao Zhang",
            "Banghua Zhu",
            "Michael Jordan",
            "Joseph E. Gonzalez",
            "Ion Stoica"
        ],
        "abstract": "Large Language Models (LLMs) have unlocked new capabilities and applications;\nhowever, evaluating the alignment with human preferences still poses\nsignificant challenges. To address this issue, we introduce Chatbot Arena, an\nopen platform for evaluating LLMs based on human preferences. Our methodology\nemploys a pairwise comparison approach and leverages input from a diverse user\nbase through crowdsourcing. The platform has been operational for several\nmonths, amassing over 240K votes. This paper describes the platform, analyzes\nthe data we have collected so far, and explains the tried-and-true statistical\nmethods we are using for efficient and accurate evaluation and ranking of\nmodels. We confirm that the crowdsourced questions are sufficiently diverse and\ndiscriminating and that the crowdsourced human votes are in good agreement with\nthose of expert raters. These analyses collectively establish a robust\nfoundation for the credibility of Chatbot Arena. Because of its unique value\nand openness, Chatbot Arena has emerged as one of the most referenced LLM\nleaderboards, widely cited by leading LLM developers and companies. Our demo is\npublicly available at \\url{https://chat.lmsys.org}.",
        "publication_date": "2024-03-07T01:22:38Z",
        "upvotes": 38
    },
    "2403.04692": {
        "url": "https://arxiv.org/abs/2403.04692",
        "title": "PixArt-\u03a3: Weak-to-Strong Training of Diffusion Transformer for 4K\n  Text-to-Image Generation",
        "authors": [
            "Junsong Chen",
            "Chongjian Ge",
            "Enze Xie",
            "Yue Wu",
            "Lewei Yao",
            "Xiaozhe Ren",
            "Zhongdao Wang",
            "Ping Luo",
            "Huchuan Lu",
            "Zhenguo Li"
        ],
        "abstract": "In this paper, we introduce PixArt-\\Sigma, a Diffusion Transformer\nmodel~(DiT) capable of directly generating images at 4K resolution.\nPixArt-\\Sigma represents a significant advancement over its predecessor,\nPixArt-\\alpha, offering images of markedly higher fidelity and improved\nalignment with text prompts. A key feature of PixArt-\\Sigma is its training\nefficiency. Leveraging the foundational pre-training of PixArt-\\alpha, it\nevolves from the `weaker' baseline to a `stronger' model via incorporating\nhigher quality data, a process we term \"weak-to-strong training\". The\nadvancements in PixArt-\\Sigma are twofold: (1) High-Quality Training Data:\nPixArt-\\Sigma incorporates superior-quality image data, paired with more\nprecise and detailed image captions. (2) Efficient Token Compression: we\npropose a novel attention module within the DiT framework that compresses both\nkeys and values, significantly improving efficiency and facilitating\nultra-high-resolution image generation. Thanks to these improvements,\nPixArt-\\Sigma achieves superior image quality and user prompt adherence\ncapabilities with significantly smaller model size (0.6B parameters) than\nexisting text-to-image diffusion models, such as SDXL (2.6B parameters) and SD\nCascade (5.1B parameters). Moreover, PixArt-\\Sigma's capability to generate 4K\nimages supports the creation of high-resolution posters and wallpapers,\nefficiently bolstering the production of high-quality visual content in\nindustries such as film and gaming.",
        "publication_date": "2024-03-07T17:41:37Z",
        "upvotes": 32
    },
    "2403.04437": {
        "url": "https://arxiv.org/abs/2403.04437",
        "title": "StableDrag: Stable Dragging for Point-based Image Editing",
        "authors": [
            "Yutao Cui",
            "Xiaotong Zhao",
            "Guozhen Zhang",
            "Shengming Cao",
            "Kai Ma",
            "Limin Wang"
        ],
        "abstract": "Point-based image editing has attracted remarkable attention since the\nemergence of DragGAN. Recently, DragDiffusion further pushes forward the\ngenerative quality via adapting this dragging technique to diffusion models.\nDespite these great success, this dragging scheme exhibits two major drawbacks,\nnamely inaccurate point tracking and incomplete motion supervision, which may\nresult in unsatisfactory dragging outcomes. To tackle these issues, we build a\nstable and precise drag-based editing framework, coined as StableDrag, by\ndesigning a discirminative point tracking method and a confidence-based latent\nenhancement strategy for motion supervision. The former allows us to precisely\nlocate the updated handle points, thereby boosting the stability of long-range\nmanipulation, while the latter is responsible for guaranteeing the optimized\nlatent as high-quality as possible across all the manipulation steps. Thanks to\nthese unique designs, we instantiate two types of image editing models\nincluding StableDrag-GAN and StableDrag-Diff, which attains more stable\ndragging performance, through extensive qualitative experiments and\nquantitative assessment on DragBench.",
        "publication_date": "2024-03-07T12:11:02Z",
        "upvotes": 21
    },
    "2403.04746": {
        "url": "https://arxiv.org/abs/2403.04746",
        "title": "LLMs in the Imaginarium: Tool Learning through Simulated Trial and Error",
        "authors": [
            "Boshi Wang",
            "Hao Fang",
            "Jason Eisner",
            "Benjamin Van Durme",
            "Yu Su"
        ],
        "abstract": "Tools are essential for large language models (LLMs) to acquire up-to-date\ninformation and take consequential actions in external environments. Existing\nwork on tool-augmented LLMs primarily focuses on the broad coverage of tools\nand the flexibility of adding new tools. However, a critical aspect that has\nsurprisingly been understudied is simply how accurately an LLM uses tools for\nwhich it has been trained. We find that existing LLMs, including GPT-4 and\nopen-source LLMs specifically fine-tuned for tool use, only reach a correctness\nrate in the range of 30% to 60%, far from reliable use in practice. We propose\na biologically inspired method for tool-augmented LLMs, simulated trial and\nerror (STE), that orchestrates three key mechanisms for successful tool use\nbehaviors in the biological system: trial and error, imagination, and memory.\nSpecifically, STE leverages an LLM's 'imagination' to simulate plausible\nscenarios for using a tool, after which the LLM interacts with the tool to\nlearn from its execution feedback. Both short-term and long-term memory are\nemployed to improve the depth and breadth of the exploration, respectively.\nComprehensive experiments on ToolBench show that STE substantially improves\ntool learning for LLMs under both in-context learning and fine-tuning settings,\nbringing a boost of 46.7% to Mistral-Instruct-7B and enabling it to outperform\nGPT-4. We also show effective continual learning of tools via a simple\nexperience replay strategy.",
        "publication_date": "2024-03-07T18:50:51Z",
        "upvotes": 21
    },
    "2403.04732": {
        "url": "https://arxiv.org/abs/2403.04732",
        "title": "How Far Are We from Intelligent Visual Deductive Reasoning?",
        "authors": [
            "Yizhe Zhang",
            "He Bai",
            "Ruixiang Zhang",
            "Jiatao Gu",
            "Shuangfei Zhai",
            "Josh Susskind",
            "Navdeep Jaitly"
        ],
        "abstract": "Vision-Language Models (VLMs) such as GPT-4V have recently demonstrated\nincredible strides on diverse vision language tasks. We dig into vision-based\ndeductive reasoning, a more sophisticated but less explored realm, and find\npreviously unexposed blindspots in the current SOTA VLMs. Specifically, we\nleverage Raven's Progressive Matrices (RPMs), to assess VLMs' abilities to\nperform multi-hop relational and deductive reasoning relying solely on visual\nclues. We perform comprehensive evaluations of several popular VLMs employing\nstandard strategies such as in-context learning, self-consistency, and\nChain-of-thoughts (CoT) on three diverse datasets, including the Mensa IQ test,\nIntelligenceTest, and RAVEN. The results reveal that despite the impressive\ncapabilities of LLMs in text-based reasoning, we are still far from achieving\ncomparable proficiency in visual deductive reasoning. We found that certain\nstandard strategies that are effective when applied to LLMs do not seamlessly\ntranslate to the challenges presented by visual reasoning tasks. Moreover, a\ndetailed analysis reveals that VLMs struggle to solve these tasks mainly\nbecause they are unable to perceive and comprehend multiple, confounding\nabstract patterns in RPM examples.",
        "publication_date": "2024-03-07T18:35:54Z",
        "upvotes": 18
    },
    "2403.04706": {
        "url": "https://arxiv.org/abs/2403.04706",
        "title": "Common 7B Language Models Already Possess Strong Math Capabilities",
        "authors": [
            "Chen Li",
            "Weiqi Wang",
            "Jingcheng Hu",
            "Yixuan Wei",
            "Nanning Zheng",
            "Han Hu",
            "Zheng Zhang",
            "Houwen Peng"
        ],
        "abstract": "Mathematical capabilities were previously believed to emerge in common\nlanguage models only at a very large scale or require extensive math-related\npre-training. This paper shows that the LLaMA-2 7B model with common\npre-training already exhibits strong mathematical abilities, as evidenced by\nits impressive accuracy of 97.7% and 72.0% on the GSM8K and MATH benchmarks,\nrespectively, when selecting the best response from 256 random generations. The\nprimary issue with the current base model is the difficulty in consistently\neliciting its inherent mathematical capabilities. Notably, the accuracy for the\nfirst answer drops to 49.5% and 7.9% on the GSM8K and MATH benchmarks,\nrespectively. We find that simply scaling up the SFT data can significantly\nenhance the reliability of generating correct answers. However, the potential\nfor extensive scaling is constrained by the scarcity of publicly available math\nquestions. To overcome this limitation, we employ synthetic data, which proves\nto be nearly as effective as real data and shows no clear saturation when\nscaled up to approximately one million samples. This straightforward approach\nachieves an accuracy of 82.6% on GSM8K and 40.6% on MATH using LLaMA-2 7B\nmodels, surpassing previous models by 14.2% and 20.8%, respectively. We also\nprovide insights into scaling behaviors across different reasoning complexities\nand error types.",
        "publication_date": "2024-03-07T18:00:40Z",
        "upvotes": 15
    },
    "2403.04634": {
        "url": "https://arxiv.org/abs/2403.04634",
        "title": "Pix2Gif: Motion-Guided Diffusion for GIF Generation",
        "authors": [
            "Hitesh Kandala",
            "Jianfeng Gao",
            "Jianwei Yang"
        ],
        "abstract": "We present Pix2Gif, a motion-guided diffusion model for image-to-GIF (video)\ngeneration. We tackle this problem differently by formulating the task as an\nimage translation problem steered by text and motion magnitude prompts, as\nshown in teaser fig. To ensure that the model adheres to motion guidance, we\npropose a new motion-guided warping module to spatially transform the features\nof the source image conditioned on the two types of prompts. Furthermore, we\nintroduce a perceptual loss to ensure the transformed feature map remains\nwithin the same space as the target image, ensuring content consistency and\ncoherence. In preparation for the model training, we meticulously curated data\nby extracting coherent image frames from the TGIF video-caption dataset, which\nprovides rich information about the temporal changes of subjects. After\npretraining, we apply our model in a zero-shot manner to a number of video\ndatasets. Extensive qualitative and quantitative experiments demonstrate the\neffectiveness of our model -- it not only captures the semantic prompt from\ntext but also the spatial ones from motion guidance. We train all our models\nusing a single node of 16xV100 GPUs. Code, dataset and models are made public\nat: https://hiteshk03.github.io/Pix2Gif/.",
        "publication_date": "2024-03-07T16:18:28Z",
        "upvotes": 12
    },
    "2403.04116": {
        "url": "https://arxiv.org/abs/2403.04116",
        "title": "Radiative Gaussian Splatting for Efficient X-ray Novel View Synthesis",
        "authors": [
            "Yuanhao Cai",
            "Yixun Liang",
            "Jiahao Wang",
            "Angtian Wang",
            "Yulun Zhang",
            "Xiaokang Yang",
            "Zongwei Zhou",
            "Alan Yuille"
        ],
        "abstract": "X-ray is widely applied for transmission imaging due to its stronger\npenetration than natural light. When rendering novel view X-ray projections,\nexisting methods mainly based on NeRF suffer from long training time and slow\ninference speed. In this paper, we propose a 3D Gaussian splatting-based\nframework, namely X-Gaussian, for X-ray novel view synthesis. Firstly, we\nredesign a radiative Gaussian point cloud model inspired by the isotropic\nnature of X-ray imaging. Our model excludes the influence of view direction\nwhen learning to predict the radiation intensity of 3D points. Based on this\nmodel, we develop a Differentiable Radiative Rasterization (DRR) with CUDA\nimplementation. Secondly, we customize an Angle-pose Cuboid Uniform\nInitialization (ACUI) strategy that directly uses the parameters of the X-ray\nscanner to compute the camera information and then uniformly samples point\npositions within a cuboid enclosing the scanned object. Experiments show that\nour X-Gaussian outperforms state-of-the-art methods by 6.5 dB while enjoying\nless than 15% training time and over 73x inference speed. The application on\nsparse-view CT reconstruction also reveals the practical values of our method.\nCode and models will be publicly available at\nhttps://github.com/caiyuanhao1998/X-Gaussian . A video demo of the training\nprocess visualization is at https://www.youtube.com/watch?v=gDVf_Ngeghg .",
        "publication_date": "2024-03-07T00:12:08Z",
        "upvotes": 3
    },
    "2403.05530": {
        "url": "https://arxiv.org/abs/2403.05530",
        "title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens\n  of context",
        "authors": [
            "Machel Reid",
            "Nikolay Savinov",
            "Denis Teplyashin",
            "Dmitry Lepikhin",
            "Timothy Lillicrap",
            "Jean-baptiste Alayrac",
            "Radu Soricut",
            "Angeliki Lazaridou",
            "Orhan Firat",
            "Julian Schrittwieser",
            "Ioannis Antonoglou",
            "Rohan Anil",
            "Sebastian Borgeaud",
            "Andrew Dai",
            "Katie Millican",
            "Ethan Dyer",
            "Mia Glaese",
            "Thibault Sottiaux",
            "Benjamin Lee",
            "Fabio Viola",
            "Malcolm Reynolds",
            "Yuanzhong Xu",
            "James Molloy",
            "Jilin Chen",
            "Michael Isard",
            "Paul Barham",
            "Tom Hennigan",
            "Ross McIlroy",
            "Melvin Johnson",
            "Johan Schalkwyk",
            "Eli Collins",
            "Eliza Rutherford",
            "Erica Moreira",
            "Kareem Ayoub",
            "Megha Goel",
            "Clemens Meyer",
            "Gregory Thornton",
            "Zhen Yang",
            "Henryk Michalewski",
            "Zaheer Abbas",
            "Nathan Schucher",
            "Ankesh Anand",
            "Richard Ives",
            "James Keeling",
            "Karel Lenc",
            "Salem Haykal",
            "Siamak Shakeri",
            "Pranav Shyam",
            "Aakanksha Chowdhery",
            "Roman Ring",
            "Stephen Spencer",
            "Eren Sezener",
            "Luke Vilnis",
            "Oscar Chang",
            "Nobuyuki Morioka",
            "George Tucker",
            "Ce Zheng",
            "Oliver Woodman",
            "Nithya Attaluri",
            "Tomas Kocisky",
            "Evgenii Eltyshev",
            "Xi Chen",
            "Timothy Chung",
            "Vittorio Selo",
            "Siddhartha Brahma",
            "Petko Georgiev",
            "Ambrose Slone",
            "Zhenkai Zhu",
            "James Lottes",
            "Siyuan Qiao",
            "Ben Caine",
            "Sebastian Riedel",
            "Alex Tomala",
            "Martin Chadwick",
            "Juliette Love",
            "Peter Choy",
            "Sid Mittal",
            "Neil Houlsby",
            "Yunhao Tang",
            "Matthew Lamm",
            "Libin Bai",
            "Qiao Zhang",
            "Luheng He",
            "Yong Cheng",
            "Peter Humphreys",
            "Yujia Li",
            "Sergey Brin",
            "Albin Cassirer",
            "Yingjie Miao",
            "Lukas Zilka",
            "Taylor Tobin",
            "Kelvin Xu",
            "Lev Proleev",
            "Daniel Sohn",
            "Alberto Magni",
            "Lisa Anne Hendricks",
            "Isabel Gao",
            "Santiago Onta\u00f1\u00f3n",
            "Oskar Bunyan",
            "Nathan Byrd",
            "Abhanshu Sharma",
            "Biao Zhang",
            "Mario Pinto",
            "Rishika Sinha",
            "Harsh Mehta",
            "Dawei Jia",
            "Sergi Caelles",
            "Albert Webson",
            "Alex Morris",
            "Becca Roelofs",
            "Yifan Ding",
            "Robin Strudel",
            "Xuehan Xiong",
            "Marvin Ritter",
            "Mostafa Dehghani",
            "Rahma Chaabouni",
            "Abhijit Karmarkar",
            "Guangda Lai",
            "Fabian Mentzer",
            "Bibo Xu",
            "YaGuang Li",
            "Yujing Zhang",
            "Tom Le Paine",
            "Alex Goldin",
            "Behnam Neyshabur",
            "Kate Baumli",
            "Anselm Levskaya",
            "Michael Laskin",
            "Wenhao Jia",
            "Jack W. Rae",
            "Kefan Xiao",
            "Antoine He",
            "Skye Giordano",
            "Lakshman Yagati",
            "Jean-Baptiste Lespiau",
            "Paul Natsev",
            "Sanjay Ganapathy",
            "Fangyu Liu",
            "Danilo Martins",
            "Nanxin Chen",
            "Yunhan Xu",
            "Megan Barnes",
            "Rhys May",
            "Arpi Vezer",
            "Junhyuk Oh",
            "Ken Franko",
            "Sophie Bridgers",
            "Ruizhe Zhao",
            "Boxi Wu",
            "Basil Mustafa",
            "Sean Sechrist",
            "Emilio Parisotto",
            "Thanumalayan Sankaranarayana Pillai",
            "Chris Larkin",
            "Chenjie Gu",
            "Christina Sorokin",
            "Maxim Krikun",
            "Alexey Guseynov",
            "Jessica Landon",
            "Romina Datta",
            "Alexander Pritzel",
            "Phoebe Thacker",
            "Fan Yang",
            "Kevin Hui",
            "Anja Hauth",
            "Chih-Kuan Yeh",
            "David Barker",
            "Justin Mao-Jones",
            "Sophia Austin",
            "Hannah Sheahan",
            "Parker Schuh",
            "James Svensson",
            "Rohan Jain",
            "Vinay Ramasesh",
            "Anton Briukhov",
            "Da-Woon Chung",
            "Tamara von Glehn",
            "Christina Butterfield",
            "Priya Jhakra",
            "Matthew Wiethoff",
            "Justin Frye",
            "Jordan Grimstad",
            "Beer Changpinyo",
            "Charline Le Lan",
            "Anna Bortsova",
            "Yonghui Wu",
            "Paul Voigtlaender",
            "Tara Sainath",
            "Charlotte Smith",
            "Will Hawkins",
            "Kris Cao",
            "James Besley",
            "Srivatsan Srinivasan",
            "Mark Omernick",
            "Colin Gaffney",
            "Gabriela Surita",
            "Ryan Burnell",
            "Bogdan Damoc",
            "Junwhan Ahn",
            "Andrew Brock",
            "Mantas Pajarskas",
            "Anastasia Petrushkina",
            "Seb Noury",
            "Lorenzo Blanco",
            "Kevin Swersky",
            "Arun Ahuja",
            "Thi Avrahami",
            "Vedant Misra",
            "Raoul de Liedekerke",
            "Mariko Iinuma",
            "Alex Polozov",
            "Sarah York",
            "George van den Driessche",
            "Paul Michel",
            "Justin Chiu",
            "Rory Blevins",
            "Zach Gleicher",
            "Adri\u00e0 Recasens",
            "Alban Rrustemi",
            "Elena Gribovskaya",
            "Aurko Roy",
            "Wiktor Gworek",
            "S\u00e9b Arnold",
            "Lisa Lee",
            "James Lee-Thorp",
            "Marcello Maggioni",
            "Enrique Piqueras",
            "Kartikeya Badola",
            "Sharad Vikram",
            "Lucas Gonzalez",
            "Anirudh Baddepudi",
            "Evan Senter",
            "Jacob Devlin",
            "James Qin",
            "Michael Azzam",
            "Maja Trebacz",
            "Martin Polacek",
            "Kashyap Krishnakumar",
            "Shuo-yiin Chang",
            "Matthew Tung",
            "Ivo Penchev",
            "Rishabh Joshi",
            "Kate Olszewska",
            "Carrie Muir",
            "Mateo Wirth",
            "Ale Jakse Hartman",
            "Josh Newlan",
            "Sheleem Kashem",
            "Vijay Bolina",
            "Elahe Dabir",
            "Joost van Amersfoort",
            "Zafarali Ahmed",
            "James Cobon-Kerr",
            "Aishwarya Kamath",
            "Arnar Mar Hrafnkelsson",
            "Le Hou",
            "Ian Mackinnon",
            "Alexandre Frechette",
            "Eric Noland",
            "Xiance Si",
            "Emanuel Taropa",
            "Dong Li",
            "Phil Crone",
            "Anmol Gulati",
            "S\u00e9bastien Cevey",
            "Jonas Adler",
            "Ada Ma",
            "David Silver",
            "Simon Tokumine",
            "Richard Powell",
            "Stephan Lee",
            "Michael Chang",
            "Samer Hassan",
            "Diana Mincu",
            "Antoine Yang",
            "Nir Levine",
            "Jenny Brennan",
            "Mingqiu Wang",
            "Sarah Hodkinson",
            "Jeffrey Zhao",
            "Josh Lipschultz",
            "Aedan Pope",
            "Michael B. Chang",
            "Cheng Li",
            "Laurent El Shafey",
            "Michela Paganini",
            "Sholto Douglas",
            "Bernd Bohnet",
            "Fabio Pardo",
            "Seth Odoom",
            "Mihaela Rosca",
            "Cicero Nogueira dos Santos",
            "Kedar Soparkar",
            "Arthur Guez",
            "Tom Hudson",
            "Steven Hansen",
            "Chulayuth Asawaroengchai",
            "Ravi Addanki",
            "Tianhe Yu",
            "Wojciech Stokowiec",
            "Mina Khan",
            "Justin Gilmer",
            "Jaehoon Lee",
            "Carrie Grimes Bostock",
            "Keran Rong",
            "Jonathan Caton",
            "Pedram Pejman",
            "Filip Pavetic",
            "Geoff Brown",
            "Vivek Sharma",
            "Mario Lu\u010di\u0107",
            "Rajkumar Samuel",
            "Josip Djolonga",
            "Amol Mandhane",
            "Lars Lowe Sj\u00f6sund",
            "Elena Buchatskaya",
            "Elspeth White",
            "Natalie Clay",
            "Jiepu Jiang",
            "Hyeontaek Lim",
            "Ross Hemsley",
            "Jane Labanowski",
            "Nicola De Cao",
            "David Steiner",
            "Sayed Hadi Hashemi",
            "Jacob Austin",
            "Anita Gergely",
            "Tim Blyth",
            "Joe Stanton",
            "Kaushik Shivakumar",
            "Aditya Siddhant",
            "Anders Andreassen",
            "Carlos Araya",
            "Nikhil Sethi",
            "Rakesh Shivanna",
            "Steven Hand",
            "Ankur Bapna",
            "Ali Khodaei",
            "Antoine Miech",
            "Garrett Tanzer",
            "Andy Swing",
            "Shantanu Thakoor",
            "Zhufeng Pan",
            "Zachary Nado",
            "Stephanie Winkler",
            "Dian Yu",
            "Mohammad Saleh",
            "Loren Maggiore",
            "Iain Barr",
            "Minh Giang",
            "Thais Kagohara",
            "Ivo Danihelka",
            "Amit Marathe",
            "Vladimir Feinberg",
            "Mohamed Elhawaty",
            "Nimesh Ghelani",
            "Dan Horgan",
            "Helen Miller",
            "Lexi Walker",
            "Richard Tanburn",
            "Mukarram Tariq",
            "Disha Shrivastava",
            "Fei Xia",
            "Chung-Cheng Chiu",
            "Zoe Ashwood",
            "Khuslen Baatarsukh",
            "Sina Samangooei",
            "Fred Alcober",
            "Axel Stjerngren",
            "Paul Komarek",
            "Katerina Tsihlas",
            "Anudhyan Boral",
            "Ramona Comanescu",
            "Jeremy Chen",
            "Ruibo Liu",
            "Dawn Bloxwich",
            "Charlie Chen",
            "Yanhua Sun",
            "Fangxiaoyu Feng",
            "Matthew Mauger",
            "Xerxes Dotiwalla",
            "Vincent Hellendoorn",
            "Michael Sharman",
            "Ivy Zheng",
            "Krishna Haridasan",
            "Gabe Barth-Maron",
            "Craig Swanson",
            "Dominika Rogozi\u0144ska",
            "Alek Andreev",
            "Paul Kishan Rubenstein",
            "Ruoxin Sang",
            "Dan Hurt",
            "Gamaleldin Elsayed",
            "Renshen Wang",
            "Dave Lacey",
            "Anastasija Ili\u0107",
            "Yao Zhao",
            "Lora Aroyo",
            "Chimezie Iwuanyanwu",
            "Vitaly Nikolaev",
            "Balaji Lakshminarayanan",
            "Sadegh Jazayeri",
            "Rapha\u00ebl Lopez Kaufman",
            "Mani Varadarajan",
            "Chetan Tekur",
            "Doug Fritz",
            "Misha Khalman",
            "David Reitter",
            "Kingshuk Dasgupta",
            "Shourya Sarcar",
            "Tina Ornduff",
            "Javier Snaider",
            "Fantine Huot",
            "Johnson Jia",
            "Rupert Kemp",
            "Nejc Trdin",
            "Anitha Vijayakumar",
            "Lucy Kim",
            "Christof Angermueller",
            "Li Lao",
            "Tianqi Liu",
            "Haibin Zhang",
            "David Engel",
            "Somer Greene",
            "Ana\u00efs White",
            "Jessica Austin",
            "Lilly Taylor",
            "Shereen Ashraf",
            "Dangyi Liu",
            "Maria Georgaki",
            "Irene Cai",
            "Yana Kulizhskaya",
            "Sonam Goenka",
            "Brennan Saeta",
            "Kiran Vodrahalli",
            "Christian Frank",
            "Dario de Cesare",
            "Brona Robenek",
            "Harry Richardson",
            "Mahmoud Alnahlawi",
            "Christopher Yew",
            "Priya Ponnapalli",
            "Marco Tagliasacchi",
            "Alex Korchemniy",
            "Yelin Kim",
            "Dinghua Li",
            "Bill Rosgen",
            "Zoe Ashwood",
            "Kyle Levin",
            "Jeremy Wiesner",
            "Praseem Banzal",
            "Praveen Srinivasan",
            "Hongkun Yu",
            "\u00c7a\u011flar \u00dcnl\u00fc",
            "David Reid",
            "Zora Tung",
            "Daniel Finchelstein",
            "Ravin Kumar",
            "Andre Elisseeff",
            "Jin Huang",
            "Ming Zhang",
            "Rui Zhu",
            "Ricardo Aguilar",
            "Mai Gim\u00e9nez",
            "Jiawei Xia",
            "Olivier Dousse",
            "Willi Gierke",
            "Soheil Hassas Yeganeh",
            "Damion Yates",
            "Komal Jalan",
            "Lu Li",
            "Eri Latorre-Chimoto",
            "Duc Dung Nguyen",
            "Ken Durden",
            "Praveen Kallakuri",
            "Yaxin Liu",
            "Matthew Johnson",
            "Tomy Tsai",
            "Alice Talbert",
            "Jasmine Liu",
            "Alexander Neitz",
            "Chen Elkind",
            "Marco Selvi",
            "Mimi Jasarevic",
            "Livio Baldini Soares",
            "Albert Cui",
            "Pidong Wang",
            "Alek Wenjiao Wang",
            "Xinyu Ye",
            "Krystal Kallarackal",
            "Lucia Loher",
            "Hoi Lam",
            "Josef Broder",
            "Dan Holtmann-Rice",
            "Nina Martin",
            "Bramandia Ramadhana",
            "Daniel Toyama",
            "Mrinal Shukla",
            "Sujoy Basu",
            "Abhi Mohan",
            "Nick Fernando",
            "Noah Fiedel",
            "Kim Paterson",
            "Hui Li",
            "Ankush Garg",
            "Jane Park",
            "DongHyun Choi",
            "Diane Wu",
            "Sankalp Singh",
            "Zhishuai Zhang",
            "Amir Globerson",
            "Lily Yu",
            "John Carpenter",
            "F\u00e9lix de Chaumont Quitry",
            "Carey Radebaugh",
            "Chu-Cheng Lin",
            "Alex Tudor",
            "Prakash Shroff",
            "Drew Garmon",
            "Dayou Du",
            "Neera Vats",
            "Han Lu",
            "Shariq Iqbal",
            "Alex Yakubovich",
            "Nilesh Tripuraneni",
            "James Manyika",
            "Haroon Qureshi",
            "Nan Hua",
            "Christel Ngani",
            "Maria Abi Raad",
            "Hannah Forbes",
            "Anna Bulanova",
            "Jeff Stanway",
            "Mukund Sundararajan",
            "Victor Ungureanu",
            "Colton Bishop",
            "Yunjie Li",
            "Balaji Venkatraman",
            "Bo Li",
            "Chloe Thornton",
            "Salvatore Scellato",
            "Nishesh Gupta",
            "Yicheng Wang",
            "Ian Tenney",
            "Xihui Wu",
            "Ashish Shenoy",
            "Gabriel Carvajal",
            "Diana Gage Wright",
            "Ben Bariach",
            "Zhuyun Xiao",
            "Peter Hawkins",
            "Sid Dalmia",
            "Clement Farabet",
            "Pedro Valenzuela",
            "Quan Yuan",
            "Chris Welty",
            "Ananth Agarwal",
            "Mia Chen",
            "Wooyeol Kim",
            "Brice Hulse",
            "Nandita Dukkipati",
            "Adam Paszke",
            "Andrew Bolt",
            "Elnaz Davoodi",
            "Kiam Choo",
            "Jennifer Beattie",
            "Jennifer Prendki",
            "Harsha Vashisht",
            "Rebeca Santamaria-Fernandez",
            "Luis C. Cobo",
            "Jarek Wilkiewicz",
            "David Madras",
            "Ali Elqursh",
            "Grant Uy",
            "Kevin Ramirez",
            "Matt Harvey",
            "Tyler Liechty",
            "Heiga Zen",
            "Jeff Seibert",
            "Clara Huiyi Hu",
            "Mohamed Elhawaty",
            "Andrey Khorlin",
            "Maigo Le",
            "Asaf Aharoni",
            "Megan Li",
            "Lily Wang",
            "Sandeep Kumar",
            "Alejandro Lince",
            "Norman Casagrande",
            "Jay Hoover",
            "Dalia El Badawy",
            "David Soergel",
            "Denis Vnukov",
            "Matt Miecnikowski",
            "Jiri Simsa",
            "Anna Koop",
            "Praveen Kumar",
            "Thibault Sellam",
            "Daniel Vlasic",
            "Samira Daruki",
            "Nir Shabat",
            "John Zhang",
            "Guolong Su",
            "Jiageng Zhang",
            "Jeremiah Liu",
            "Yi Sun",
            "Evan Palmer",
            "Alireza Ghaffarkhah",
            "Xi Xiong",
            "Victor Cotruta",
            "Michael Fink",
            "Lucas Dixon",
            "Ashwin Sreevatsa",
            "Adrian Goedeckemeyer",
            "Alek Dimitriev",
            "Mohsen Jafari",
            "Remi Crocker",
            "Nicholas FitzGerald",
            "Aviral Kumar",
            "Sanjay Ghemawat",
            "Ivan Philips",
            "Frederick Liu",
            "Yannie Liang",
            "Rachel Sterneck",
            "Alena Repina",
            "Marcus Wu",
            "Laura Knight",
            "Marin Georgiev",
            "Hyo Lee",
            "Harry Askham",
            "Abhishek Chakladar",
            "Annie Louis",
            "Carl Crous",
            "Hardie Cate",
            "Dessie Petrova",
            "Michael Quinn",
            "Denese Owusu-Afriyie",
            "Achintya Singhal",
            "Nan Wei",
            "Solomon Kim",
            "Damien Vincent",
            "Milad Nasr",
            "Christopher A. Choquette-Choo",
            "Reiko Tojo",
            "Shawn Lu",
            "Diego de Las Casas",
            "Yuchung Cheng",
            "Tolga Bolukbasi",
            "Katherine Lee",
            "Saaber Fatehi",
            "Rajagopal Ananthanarayanan",
            "Miteyan Patel",
            "Charbel Kaed",
            "Jing Li",
            "Jakub Sygnowski",
            "Shreyas Rammohan Belle",
            "Zhe Chen",
            "Jaclyn Konzelmann",
            "Siim P\u00f5der",
            "Roopal Garg",
            "Vinod Koverkathu",
            "Adam Brown",
            "Chris Dyer",
            "Rosanne Liu",
            "Azade Nova",
            "Jun Xu",
            "Slav Petrov",
            "Demis Hassabis",
            "Koray Kavukcuoglu",
            "Jeffrey Dean",
            "Oriol Vinyals"
        ],
        "abstract": "In this report, we present the latest model of the Gemini family, Gemini 1.5\nPro, a highly compute-efficient multimodal mixture-of-experts model capable of\nrecalling and reasoning over fine-grained information from millions of tokens\nof context, including multiple long documents and hours of video and audio.\nGemini 1.5 Pro achieves near-perfect recall on long-context retrieval tasks\nacross modalities, improves the state-of-the-art in long-document QA,\nlong-video QA and long-context ASR, and matches or surpasses Gemini 1.0 Ultra's\nstate-of-the-art performance across a broad set of benchmarks. Studying the\nlimits of Gemini 1.5 Pro's long-context ability, we find continued improvement\nin next-token prediction and near-perfect retrieval (>99%) up to at least 10M\ntokens, a generational leap over existing models such as Claude 2.1 (200k) and\nGPT-4 Turbo (128k). Finally, we highlight surprising new capabilities of large\nlanguage models at the frontier; when given a grammar manual for Kalamang, a\nlanguage with fewer than 200 speakers worldwide, the model learns to translate\nEnglish to Kalamang at a similar level to a person who learned from the same\ncontent.",
        "publication_date": "2024-03-08T18:54:20Z",
        "upvotes": 48
    },
    "2403.05135": {
        "url": "https://arxiv.org/abs/2403.05135",
        "title": "ELLA: Equip Diffusion Models with LLM for Enhanced Semantic Alignment",
        "authors": [
            "Xiwei Hu",
            "Rui Wang",
            "Yixiao Fang",
            "Bin Fu",
            "Pei Cheng",
            "Gang Yu"
        ],
        "abstract": "Diffusion models have demonstrated remarkable performance in the domain of\ntext-to-image generation. However, most widely used models still employ CLIP as\ntheir text encoder, which constrains their ability to comprehend dense prompts,\nencompassing multiple objects, detailed attributes, complex relationships,\nlong-text alignment, etc. In this paper, we introduce an Efficient Large\nLanguage Model Adapter, termed ELLA, which equips text-to-image diffusion\nmodels with powerful Large Language Models (LLM) to enhance text alignment\nwithout training of either U-Net or LLM. To seamlessly bridge two pre-trained\nmodels, we investigate a range of semantic alignment connector designs and\npropose a novel module, the Timestep-Aware Semantic Connector (TSC), which\ndynamically extracts timestep-dependent conditions from LLM. Our approach\nadapts semantic features at different stages of the denoising process,\nassisting diffusion models in interpreting lengthy and intricate prompts over\nsampling timesteps. Additionally, ELLA can be readily incorporated with\ncommunity models and tools to improve their prompt-following capabilities. To\nassess text-to-image models in dense prompt following, we introduce Dense\nPrompt Graph Benchmark (DPG-Bench), a challenging benchmark consisting of 1K\ndense prompts. Extensive experiments demonstrate the superiority of ELLA in\ndense prompt following compared to state-of-the-art methods, particularly in\nmultiple object compositions involving diverse attributes and relationships.",
        "publication_date": "2024-03-08T08:08:10Z",
        "upvotes": 39
    },
    "2403.05525": {
        "url": "https://arxiv.org/abs/2403.05525",
        "title": "DeepSeek-VL: Towards Real-World Vision-Language Understanding",
        "authors": [
            "Haoyu Lu",
            "Wen Liu",
            "Bo Zhang",
            "Bingxuan Wang",
            "Kai Dong",
            "Bo Liu",
            "Jingxiang Sun",
            "Tongzheng Ren",
            "Zhuoshu Li",
            "Hao Yang",
            "Yaofeng Sun",
            "Chengqi Deng",
            "Hanwei Xu",
            "Zhenda Xie",
            "Chong Ruan"
        ],
        "abstract": "We present DeepSeek-VL, an open-source Vision-Language (VL) Model designed\nfor real-world vision and language understanding applications. Our approach is\nstructured around three key dimensions:\n  We strive to ensure our data is diverse, scalable, and extensively covers\nreal-world scenarios including web screenshots, PDFs, OCR, charts, and\nknowledge-based content, aiming for a comprehensive representation of practical\ncontexts. Further, we create a use case taxonomy from real user scenarios and\nconstruct an instruction tuning dataset accordingly. The fine-tuning with this\ndataset substantially improves the model's user experience in practical\napplications. Considering efficiency and the demands of most real-world\nscenarios, DeepSeek-VL incorporates a hybrid vision encoder that efficiently\nprocesses high-resolution images (1024 x 1024), while maintaining a relatively\nlow computational overhead. This design choice ensures the model's ability to\ncapture critical semantic and detailed information across various visual tasks.\nWe posit that a proficient Vision-Language Model should, foremost, possess\nstrong language abilities. To ensure the preservation of LLM capabilities\nduring pretraining, we investigate an effective VL pretraining strategy by\nintegrating LLM training from the beginning and carefully managing the\ncompetitive dynamics observed between vision and language modalities.\n  The DeepSeek-VL family (both 1.3B and 7B models) showcases superior user\nexperiences as a vision-language chatbot in real-world applications, achieving\nstate-of-the-art or competitive performance across a wide range of\nvisual-language benchmarks at the same model size while maintaining robust\nperformance on language-centric benchmarks. We have made both 1.3B and 7B\nmodels publicly accessible to foster innovations based on this foundation\nmodel.",
        "publication_date": "2024-03-08T18:46:00Z",
        "upvotes": 33
    },
    "2403.05185": {
        "url": "https://arxiv.org/abs/2403.05185",
        "title": "Personalized Audiobook Recommendations at Spotify Through Graph Neural\n  Networks",
        "authors": [
            "Marco De Nadai",
            "Francesco Fabbri",
            "Paul Gigioli",
            "Alice Wang",
            "Ang Li",
            "Fabrizio Silvestri",
            "Laura Kim",
            "Shawn Lin",
            "Vladan Radosavljevic",
            "Sandeep Ghael",
            "David Nyhan",
            "Hugues Bouchard",
            "Mounia Lalmas-Roelleke",
            "Andreas Damianou"
        ],
        "abstract": "In the ever-evolving digital audio landscape, Spotify, well-known for its\nmusic and talk content, has recently introduced audiobooks to its vast user\nbase. While promising, this move presents significant challenges for\npersonalized recommendations. Unlike music and podcasts, audiobooks, initially\navailable for a fee, cannot be easily skimmed before purchase, posing higher\nstakes for the relevance of recommendations. Furthermore, introducing a new\ncontent type into an existing platform confronts extreme data sparsity, as most\nusers are unfamiliar with this new content type. Lastly, recommending content\nto millions of users requires the model to react fast and be scalable. To\naddress these challenges, we leverage podcast and music user preferences and\nintroduce 2T-HGNN, a scalable recommendation system comprising Heterogeneous\nGraph Neural Networks (HGNNs) and a Two Tower (2T) model. This novel approach\nuncovers nuanced item relationships while ensuring low latency and complexity.\nWe decouple users from the HGNN graph and propose an innovative multi-link\nneighbor sampler. These choices, together with the 2T component, significantly\nreduce the complexity of the HGNN model. Empirical evaluations involving\nmillions of users show significant improvement in the quality of personalized\nrecommendations, resulting in a +46% increase in new audiobooks start rate and\na +23% boost in streaming rates. Intriguingly, our model's impact extends\nbeyond audiobooks, benefiting established products like podcasts.",
        "publication_date": "2024-03-08T09:53:07Z",
        "upvotes": 19
    },
    "2403.05034": {
        "url": "https://arxiv.org/abs/2403.05034",
        "title": "CRM: Single Image to 3D Textured Mesh with Convolutional Reconstruction\n  Model",
        "authors": [
            "Zhengyi Wang",
            "Yikai Wang",
            "Yifei Chen",
            "Chendong Xiang",
            "Shuo Chen",
            "Dajiang Yu",
            "Chongxuan Li",
            "Hang Su",
            "Jun Zhu"
        ],
        "abstract": "Feed-forward 3D generative models like the Large Reconstruction Model (LRM)\nhave demonstrated exceptional generation speed. However, the transformer-based\nmethods do not leverage the geometric priors of the triplane component in their\narchitecture, often leading to sub-optimal quality given the limited size of 3D\ndata and slow training. In this work, we present the Convolutional\nReconstruction Model (CRM), a high-fidelity feed-forward single image-to-3D\ngenerative model. Recognizing the limitations posed by sparse 3D data, we\nhighlight the necessity of integrating geometric priors into network design.\nCRM builds on the key observation that the visualization of triplane exhibits\nspatial correspondence of six orthographic images. First, it generates six\northographic view images from a single input image, then feeds these images\ninto a convolutional U-Net, leveraging its strong pixel-level alignment\ncapabilities and significant bandwidth to create a high-resolution triplane.\nCRM further employs Flexicubes as geometric representation, facilitating direct\nend-to-end optimization on textured meshes. Overall, our model delivers a\nhigh-fidelity textured mesh from an image in just 10 seconds, without any\ntest-time optimization.",
        "publication_date": "2024-03-08T04:25:29Z",
        "upvotes": 16
    },
    "2403.05121": {
        "url": "https://arxiv.org/abs/2403.05121",
        "title": "CogView3: Finer and Faster Text-to-Image Generation via Relay Diffusion",
        "authors": [
            "Wendi Zheng",
            "Jiayan Teng",
            "Zhuoyi Yang",
            "Weihan Wang",
            "Jidong Chen",
            "Xiaotao Gu",
            "Yuxiao Dong",
            "Ming Ding",
            "Jie Tang"
        ],
        "abstract": "Recent advancements in text-to-image generative systems have been largely\ndriven by diffusion models. However, single-stage text-to-image diffusion\nmodels still face challenges, in terms of computational efficiency and the\nrefinement of image details. To tackle the issue, we propose CogView3, an\ninnovative cascaded framework that enhances the performance of text-to-image\ndiffusion. CogView3 is the first model implementing relay diffusion in the\nrealm of text-to-image generation, executing the task by first creating\nlow-resolution images and subsequently applying relay-based super-resolution.\nThis methodology not only results in competitive text-to-image outputs but also\ngreatly reduces both training and inference costs. Our experimental results\ndemonstrate that CogView3 outperforms SDXL, the current state-of-the-art\nopen-source text-to-image diffusion model, by 77.0\\% in human evaluations, all\nwhile requiring only about 1/2 of the inference time. The distilled variant of\nCogView3 achieves comparable performance while only utilizing 1/10 of the\ninference time by SDXL.",
        "publication_date": "2024-03-08T07:32:50Z",
        "upvotes": 15
    },
    "2403.05438": {
        "url": "https://arxiv.org/abs/2403.05438",
        "title": "VideoElevator: Elevating Video Generation Quality with Versatile\n  Text-to-Image Diffusion Models",
        "authors": [
            "Yabo Zhang",
            "Yuxiang Wei",
            "Xianhui Lin",
            "Zheng Hui",
            "Peiran Ren",
            "Xuansong Xie",
            "Xiangyang Ji",
            "Wangmeng Zuo"
        ],
        "abstract": "Text-to-image diffusion models (T2I) have demonstrated unprecedented\ncapabilities in creating realistic and aesthetic images. On the contrary,\ntext-to-video diffusion models (T2V) still lag far behind in frame quality and\ntext alignment, owing to insufficient quality and quantity of training videos.\nIn this paper, we introduce VideoElevator, a training-free and plug-and-play\nmethod, which elevates the performance of T2V using superior capabilities of\nT2I. Different from conventional T2V sampling (i.e., temporal and spatial\nmodeling), VideoElevator explicitly decomposes each sampling step into temporal\nmotion refining and spatial quality elevating. Specifically, temporal motion\nrefining uses encapsulated T2V to enhance temporal consistency, followed by\ninverting to the noise distribution required by T2I. Then, spatial quality\nelevating harnesses inflated T2I to directly predict less noisy latent, adding\nmore photo-realistic details. We have conducted experiments in extensive\nprompts under the combination of various T2V and T2I. The results show that\nVideoElevator not only improves the performance of T2V baselines with\nfoundational T2I, but also facilitates stylistic video synthesis with\npersonalized T2I. Our code is available at\nhttps://github.com/YBYBZhang/VideoElevator.",
        "publication_date": "2024-03-08T16:44:54Z",
        "upvotes": 14
    },
    "2403.06634": {
        "url": "https://arxiv.org/abs/2403.06634",
        "title": "Stealing Part of a Production Language Model",
        "authors": [
            "Nicholas Carlini",
            "Daniel Paleka",
            "Krishnamurthy Dj Dvijotham",
            "Thomas Steinke",
            "Jonathan Hayase",
            "A. Feder Cooper",
            "Katherine Lee",
            "Matthew Jagielski",
            "Milad Nasr",
            "Arthur Conmy",
            "Eric Wallace",
            "David Rolnick",
            "Florian Tram\u00e8r"
        ],
        "abstract": "We introduce the first model-stealing attack that extracts precise,\nnontrivial information from black-box production language models like OpenAI's\nChatGPT or Google's PaLM-2. Specifically, our attack recovers the embedding\nprojection layer (up to symmetries) of a transformer model, given typical API\naccess. For under \\$20 USD, our attack extracts the entire projection matrix of\nOpenAI's Ada and Babbage language models. We thereby confirm, for the first\ntime, that these black-box models have a hidden dimension of 1024 and 2048,\nrespectively. We also recover the exact hidden dimension size of the\ngpt-3.5-turbo model, and estimate it would cost under \\$2,000 in queries to\nrecover the entire projection matrix. We conclude with potential defenses and\nmitigations, and discuss the implications of possible future work that could\nextend our attack.",
        "publication_date": "2024-03-11T11:46:12Z",
        "upvotes": 84
    },
    "2403.06504": {
        "url": "https://arxiv.org/abs/2403.06504",
        "title": "Adding NVMe SSDs to Enable and Accelerate 100B Model Fine-tuning on a\n  Single GPU",
        "authors": [
            "Changyue Liao",
            "Mo Sun",
            "Zihan Yang",
            "Kaiqi Chen",
            "Binhang Yuan",
            "Fei Wu",
            "Zeke Wang"
        ],
        "abstract": "Recent advances in large language models have brought immense value to the\nworld, with their superior capabilities stemming from the massive number of\nparameters they utilize. However, even the GPUs with the highest memory\ncapacities, currently peaking at 80GB, are far from sufficient to accommodate\nthese vast parameters and their associated optimizer states when conducting\nstochastic gradient descent-based optimization. One approach to hosting such\nhuge models is to aggregate device memory from many GPUs. However, this\napproach introduces prohibitive costs for most academic researchers, who always\nhave a limited budget for many high-end GPU servers. In this paper, we focus on\nhuge model fine-tuning on a single, even low-end, GPU in a commodity server,\nwhich is accessible to most AI researchers. In such a scenario, the\nstate-of-the-art work ZeRO-Infinity suffers from two severe issues when running\nin a commodity server: 1) low GPU utilization due to inefficient swapping, and\n2) limited trainable model size due to CPU memory capacity. The underlying\nreason is that ZeRO-Infinity is optimized for running on high-end GPU servers.\nTo this end, we present Fuyou, a low-cost training framework that enables\nefficient 100B huge model fine-tuning on a low-end server with a low-end GPU\nand limited CPU memory capacity. The key idea is to add the SSD-CPU\ncommunication as an optimization dimension and thus carefully co-optimize\ncomputation and data swapping from a systematic approach to maximize GPU\nutilization. The experimental results show that 1) Fuyou is able to fine-tune\n175B GPT-3 on a consumer GPU RTX 4090 with high GPU utilization, while\nZeRO-Infinity fails to fine-tune; and 2) when training a small GPT-3 13B model,\nFuyou achieves 156 TFLOPS on an RTX 4090 GPU while ZeRO-Infinity only achieves\n45 TFLOPS.",
        "publication_date": "2024-03-11T08:25:53Z",
        "upvotes": 52
    },
    "2403.06738": {
        "url": "https://arxiv.org/abs/2403.06738",
        "title": "V3D: Video Diffusion Models are Effective 3D Generators",
        "authors": [
            "Zilong Chen",
            "Yikai Wang",
            "Feng Wang",
            "Zhengyi Wang",
            "Huaping Liu"
        ],
        "abstract": "Automatic 3D generation has recently attracted widespread attention. Recent\nmethods have greatly accelerated the generation speed, but usually produce\nless-detailed objects due to limited model capacity or 3D data. Motivated by\nrecent advancements in video diffusion models, we introduce V3D, which\nleverages the world simulation capacity of pre-trained video diffusion models\nto facilitate 3D generation. To fully unleash the potential of video diffusion\nto perceive the 3D world, we further introduce geometrical consistency prior\nand extend the video diffusion model to a multi-view consistent 3D generator.\nBenefiting from this, the state-of-the-art video diffusion model could be\nfine-tuned to generate 360degree orbit frames surrounding an object given a\nsingle image. With our tailored reconstruction pipelines, we can generate\nhigh-quality meshes or 3D Gaussians within 3 minutes. Furthermore, our method\ncan be extended to scene-level novel view synthesis, achieving precise control\nover the camera path with sparse input views. Extensive experiments demonstrate\nthe superior performance of the proposed approach, especially in terms of\ngeneration quality and multi-view consistency. Our code is available at\nhttps://github.com/heheyas/V3D",
        "publication_date": "2024-03-11T14:03:36Z",
        "upvotes": 26
    },
    "2403.06764": {
        "url": "https://arxiv.org/abs/2403.06764",
        "title": "An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference\n  Acceleration for Large Vision-Language Models",
        "authors": [
            "Liang Chen",
            "Haozhe Zhao",
            "Tianyu Liu",
            "Shuai Bai",
            "Junyang Lin",
            "Chang Zhou",
            "Baobao Chang"
        ],
        "abstract": "In this study, we identify the inefficient attention phenomena in Large\nVision-Language Models (LVLMs), notably within prominent models like LLaVA-1.5,\nQwenVL-Chat and Video-LLaVA. We find out that the attention computation over\nvisual tokens is of extreme inefficiency in the deep layers of popular LVLMs,\nsuggesting a need for a sparser approach compared to textual data handling. To\nthis end, we introduce FastV, a versatile plug-and-play method designed to\noptimize computational efficiency by learning adaptive attention patterns in\nearly layers and pruning visual tokens in subsequent ones. Our evaluations\ndemonstrate FastV's ability to dramatically reduce computational costs (e.g., a\n45 reduction in FLOPs for LLaVA-1.5-13B) without sacrificing performance in a\nwide range of image and video understanding tasks. The computational efficiency\nand performance trade-off of FastV are highly customizable and\npareto-efficient. It can compress the FLOPs of a 13B-parameter model to achieve\na lower budget than that of a 7B-parameter model, while still maintaining\nsuperior performance. We believe FastV has practical values for deployment of\nLVLMs in edge devices and commercial models. Code is released at\nhttps://github.com/pkunlp-icler/FastV.",
        "publication_date": "2024-03-11T14:35:32Z",
        "upvotes": 24
    },
    "2403.06977": {
        "url": "https://arxiv.org/abs/2403.06977",
        "title": "VideoMamba: State Space Model for Efficient Video Understanding",
        "authors": [
            "Kunchang Li",
            "Xinhao Li",
            "Yi Wang",
            "Yinan He",
            "Yali Wang",
            "Limin Wang",
            "Yu Qiao"
        ],
        "abstract": "Addressing the dual challenges of local redundancy and global dependencies in\nvideo understanding, this work innovatively adapts the Mamba to the video\ndomain. The proposed VideoMamba overcomes the limitations of existing 3D\nconvolution neural networks and video transformers. Its linear-complexity\noperator enables efficient long-term modeling, which is crucial for\nhigh-resolution long video understanding. Extensive evaluations reveal\nVideoMamba's four core abilities: (1) Scalability in the visual domain without\nextensive dataset pretraining, thanks to a novel self-distillation technique;\n(2) Sensitivity for recognizing short-term actions even with fine-grained\nmotion differences; (3) Superiority in long-term video understanding,\nshowcasing significant advancements over traditional feature-based models; and\n(4) Compatibility with other modalities, demonstrating robustness in\nmulti-modal contexts. Through these distinct advantages, VideoMamba sets a new\nbenchmark for video understanding, offering a scalable and efficient solution\nfor comprehensive video understanding. All the code and models are available at\nhttps://github.com/OpenGVLab/VideoMamba.",
        "publication_date": "2024-03-11T17:59:34Z",
        "upvotes": 21
    },
    "2403.05812": {
        "url": "https://arxiv.org/abs/2403.05812",
        "title": "Algorithmic progress in language models",
        "authors": [
            "Anson Ho",
            "Tamay Besiroglu",
            "Ege Erdil",
            "David Owen",
            "Robi Rahman",
            "Zifan Carl Guo",
            "David Atkinson",
            "Neil Thompson",
            "Jaime Sevilla"
        ],
        "abstract": "We investigate the rate at which algorithms for pre-training language models\nhave improved since the advent of deep learning. Using a dataset of over 200\nlanguage model evaluations on Wikitext and Penn Treebank spanning 2012-2023, we\nfind that the compute required to reach a set performance threshold has halved\napproximately every 8 months, with a 95% confidence interval of around 5 to 14\nmonths, substantially faster than hardware gains per Moore's Law. We estimate\naugmented scaling laws, which enable us to quantify algorithmic progress and\ndetermine the relative contributions of scaling models versus innovations in\ntraining algorithms. Despite the rapid pace of algorithmic progress and the\ndevelopment of new architectures such as the transformer, our analysis reveals\nthat the increase in compute made an even larger contribution to overall\nperformance improvements over this time period. Though limited by noisy\nbenchmark data, our analysis quantifies the rapid progress in language\nmodeling, shedding light on the relative contributions from compute and\nalgorithms.",
        "publication_date": "2024-03-09T06:26:21Z",
        "upvotes": 16
    },
    "2403.06807": {
        "url": "https://arxiv.org/abs/2403.06807",
        "title": "Multistep Consistency Models",
        "authors": [
            "Jonathan Heek",
            "Emiel Hoogeboom",
            "Tim Salimans"
        ],
        "abstract": "Diffusion models are relatively easy to train but require many steps to\ngenerate samples. Consistency models are far more difficult to train, but\ngenerate samples in a single step.\n  In this paper we propose Multistep Consistency Models: A unification between\nConsistency Models (Song et al., 2023) and TRACT (Berthelot et al., 2023) that\ncan interpolate between a consistency model and a diffusion model: a trade-off\nbetween sampling speed and sampling quality. Specifically, a 1-step consistency\nmodel is a conventional consistency model whereas we show that a $\\infty$-step\nconsistency model is a diffusion model.\n  Multistep Consistency Models work really well in practice. By increasing the\nsample budget from a single step to 2-8 steps, we can train models more easily\nthat generate higher quality samples, while retaining much of the sampling\nspeed benefits. Notable results are 1.4 FID on Imagenet 64 in 8 step and 2.1\nFID on Imagenet128 in 8 steps with consistency distillation. We also show that\nour method scales to a text-to-image diffusion model, generating samples that\nare very close to the quality of the original model.",
        "publication_date": "2024-03-11T15:26:34Z",
        "upvotes": 13
    },
    "2403.06098": {
        "url": "https://arxiv.org/abs/2403.06098",
        "title": "VidProM: A Million-scale Real Prompt-Gallery Dataset for Text-to-Video\n  Diffusion Models",
        "authors": [
            "Wenhao Wang",
            "Yi Yang"
        ],
        "abstract": "The arrival of Sora marks a new era for text-to-video diffusion models,\nbringing significant advancements in video generation and potential\napplications. However, Sora, as well as other text-to-video diffusion models,\nhighly relies on the prompts, and there is no publicly available dataset\nfeaturing a study of text-to-video prompts. In this paper, we introduce\nVidProM, the first large-scale dataset comprising 1.67 million unique\ntext-to-video prompts from real users. Additionally, the dataset includes 6.69\nmillion videos generated by four state-of-the-art diffusion models and some\nrelated data. We initially demonstrate the curation of this large-scale\ndataset, which is a time-consuming and costly process. Subsequently, we show\nhow the proposed VidProM differs from DiffusionDB, a large-scale prompt-gallery\ndataset for image generation. Based on the analysis of these prompts, we\nidentify the necessity for a new prompt dataset specifically designed for\ntext-to-video generation and gain insights into the preferences of real users\nwhen creating videos. Our large-scale and diverse dataset also inspires many\nexciting new research areas. For instance, to develop better, more efficient,\nand safer text-to-video diffusion models, we suggest exploring text-to-video\nprompt engineering, efficient video generation, and video copy detection for\ndiffusion models. We make the collected dataset VidProM publicly available at\nGitHub and Hugging Face under the CC-BY- NC 4.0 License.",
        "publication_date": "2024-03-10T05:40:12Z",
        "upvotes": 10
    },
    "2403.06775": {
        "url": "https://arxiv.org/abs/2403.06775",
        "title": "FaceChain-SuDe: Building Derived Class to Inherit Category Attributes\n  for One-shot Subject-Driven Generation",
        "authors": [
            "Pengchong Qiao",
            "Lei Shang",
            "Chang Liu",
            "Baigui Sun",
            "Xiangyang Ji",
            "Jie Chen"
        ],
        "abstract": "Subject-driven generation has garnered significant interest recently due to\nits ability to personalize text-to-image generation. Typical works focus on\nlearning the new subject's private attributes. However, an important fact has\nnot been taken seriously that a subject is not an isolated new concept but\nshould be a specialization of a certain category in the pre-trained model. This\nresults in the subject failing to comprehensively inherit the attributes in its\ncategory, causing poor attribute-related generations. In this paper, motivated\nby object-oriented programming, we model the subject as a derived class whose\nbase class is its semantic category. This modeling enables the subject to\ninherit public attributes from its category while learning its private\nattributes from the user-provided example. Specifically, we propose a\nplug-and-play method, Subject-Derived regularization (SuDe). It constructs the\nbase-derived class modeling by constraining the subject-driven generated images\nto semantically belong to the subject's category. Extensive experiments under\nthree baselines and two backbones on various subjects show that our SuDe\nenables imaginative attribute-related generations while maintaining subject\nfidelity. Codes will be open sourced soon at FaceChain\n(https://github.com/modelscope/facechain).",
        "publication_date": "2024-03-11T14:43:40Z",
        "upvotes": 3
    },
    "2403.07508": {
        "url": "https://arxiv.org/abs/2403.07508",
        "title": "MoAI: Mixture of All Intelligence for Large Language and Vision Models",
        "authors": [
            "Byung-Kwan Lee",
            "Beomchan Park",
            "Chae Won Kim",
            "Yong Man Ro"
        ],
        "abstract": "The rise of large language models (LLMs) and instruction tuning has led to\nthe current trend of instruction-tuned large language and vision models\n(LLVMs). This trend involves either meticulously curating numerous instruction\ntuning datasets tailored to specific objectives or enlarging LLVMs to manage\nvast amounts of vision language (VL) data. However, current LLVMs have\ndisregarded the detailed and comprehensive real-world scene understanding\navailable from specialized computer vision (CV) models in visual perception\ntasks such as segmentation, detection, scene graph generation (SGG), and\noptical character recognition (OCR). Instead, the existing LLVMs rely mainly on\nthe large capacity and emergent capabilities of their LLM backbones. Therefore,\nwe present a new LLVM, Mixture of All Intelligence (MoAI), which leverages\nauxiliary visual information obtained from the outputs of external\nsegmentation, detection, SGG, and OCR models. MoAI operates through two newly\nintroduced modules: MoAI-Compressor and MoAI-Mixer. After verbalizing the\noutputs of the external CV models, the MoAI-Compressor aligns and condenses\nthem to efficiently use relevant auxiliary visual information for VL tasks.\nMoAI-Mixer then blends three types of intelligence (1) visual features, (2)\nauxiliary features from the external CV models, and (3) language features by\nutilizing the concept of Mixture of Experts. Through this integration, MoAI\nsignificantly outperforms both open-source and closed-source LLVMs in numerous\nzero-shot VL tasks, particularly those related to real-world scene\nunderstanding such as object existence, positions, relations, and OCR without\nenlarging the model size or curating extra visual instruction tuning datasets.",
        "publication_date": "2024-03-12T10:44:13Z",
        "upvotes": 66
    },
    "2403.07815": {
        "url": "https://arxiv.org/abs/2403.07815",
        "title": "Chronos: Learning the Language of Time Series",
        "authors": [
            "Abdul Fatir Ansari",
            "Lorenzo Stella",
            "Caner Turkmen",
            "Xiyuan Zhang",
            "Pedro Mercado",
            "Huibin Shen",
            "Oleksandr Shchur",
            "Syama Sundar Rangapuram",
            "Sebastian Pineda Arango",
            "Shubham Kapoor",
            "Jasper Zschiegner",
            "Danielle C. Maddix",
            "Michael W. Mahoney",
            "Kari Torkkola",
            "Andrew Gordon Wilson",
            "Michael Bohlke-Schneider",
            "Yuyang Wang"
        ],
        "abstract": "We introduce Chronos, a simple yet effective framework for pretrained\nprobabilistic time series models. Chronos tokenizes time series values using\nscaling and quantization into a fixed vocabulary and trains existing\ntransformer-based language model architectures on these tokenized time series\nvia the cross-entropy loss. We pretrained Chronos models based on the T5 family\n(ranging from 20M to 710M parameters) on a large collection of publicly\navailable datasets, complemented by a synthetic dataset that we generated via\nGaussian processes to improve generalization. In a comprehensive benchmark\nconsisting of 42 datasets, and comprising both classical local models and deep\nlearning methods, we show that Chronos models: (a) significantly outperform\nother methods on datasets that were part of the training corpus; and (b) have\ncomparable and occasionally superior zero-shot performance on new datasets,\nrelative to methods that were trained specifically on them. Our results\ndemonstrate that Chronos models can leverage time series data from diverse\ndomains to improve zero-shot accuracy on unseen forecasting tasks, positioning\npretrained models as a viable tool to greatly simplify forecasting pipelines.",
        "publication_date": "2024-03-12T16:53:54Z",
        "upvotes": 38
    },
    "2403.07816": {
        "url": "https://arxiv.org/abs/2403.07816",
        "title": "Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM",
        "authors": [
            "Sainbayar Sukhbaatar",
            "Olga Golovneva",
            "Vasu Sharma",
            "Hu Xu",
            "Xi Victoria Lin",
            "Baptiste Rozi\u00e8re",
            "Jacob Kahn",
            "Daniel Li",
            "Wen-tau Yih",
            "Jason Weston",
            "Xian Li"
        ],
        "abstract": "We investigate efficient methods for training Large Language Models (LLMs) to\npossess capabilities in multiple specialized domains, such as coding, math\nreasoning and world knowledge. Our method, named Branch-Train-MiX (BTX), starts\nfrom a seed model, which is branched to train experts in embarrassingly\nparallel fashion with high throughput and reduced communication cost. After\nindividual experts are asynchronously trained, BTX brings together their\nfeedforward parameters as experts in Mixture-of-Expert (MoE) layers and\naverages the remaining parameters, followed by an MoE-finetuning stage to learn\ntoken-level routing. BTX generalizes two special cases, the Branch-Train-Merge\nmethod, which does not have the MoE finetuning stage to learn routing, and\nsparse upcycling, which omits the stage of training experts asynchronously.\nCompared to alternative approaches, BTX achieves the best accuracy-efficiency\ntradeoff.",
        "publication_date": "2024-03-12T16:54:58Z",
        "upvotes": 36
    },
    "2403.07750": {
        "url": "https://arxiv.org/abs/2403.07750",
        "title": "Synth$^2$: Boosting Visual-Language Models with Synthetic Captions and\n  Image Embeddings",
        "authors": [
            "Sahand Sharifzadeh",
            "Christos Kaplanis",
            "Shreya Pathak",
            "Dharshan Kumaran",
            "Anastasija Ilic",
            "Jovana Mitrovic",
            "Charles Blundell",
            "Andrea Banino"
        ],
        "abstract": "The creation of high-quality human-labeled image-caption datasets presents a\nsignificant bottleneck in the development of Visual-Language Models (VLMs). We\npropose a novel approach that leverages the strengths of Large Language Models\n(LLMs) and image generation models to create synthetic image-text pairs for\nefficient and effective VLM training. Our method employs pretraining a\ntext-to-image model to synthesize image embeddings starting from captions\ngenerated by an LLM. These synthetic pairs are then used to train a VLM.\nExtensive experiments demonstrate that the VLM trained with synthetic data\nexhibits comparable performance on image captioning, while requiring a fraction\nof the data used by models trained solely on human-annotated data. In\nparticular, we outperform the baseline by 17% through augmentation with a\nsynthetic dataset. Furthermore, we show that synthesizing in the image\nembedding space is 25% faster than in the pixel space. This research introduces\na promising technique for generating large-scale, customizable image datasets,\nleading to enhanced VLM performance and wider applicability across various\ndomains, all with improved data efficiency and resource utilization.",
        "publication_date": "2024-03-12T15:36:42Z",
        "upvotes": 19
    },
    "2403.07487": {
        "url": "https://arxiv.org/abs/2403.07487",
        "title": "Motion Mamba: Efficient and Long Sequence Motion Generation with\n  Hierarchical and Bidirectional Selective SSM",
        "authors": [
            "Zeyu Zhang",
            "Akide Liu",
            "Ian Reid",
            "Richard Hartley",
            "Bohan Zhuang",
            "Hao Tang"
        ],
        "abstract": "Human motion generation stands as a significant pursuit in generative\ncomputer vision, while achieving long-sequence and efficient motion generation\nremains challenging. Recent advancements in state space models (SSMs), notably\nMamba, have showcased considerable promise in long sequence modeling with an\nefficient hardware-aware design, which appears to be a promising direction to\nbuild motion generation model upon it. Nevertheless, adapting SSMs to motion\ngeneration faces hurdles since the lack of a specialized design architecture to\nmodel motion sequence. To address these challenges, we propose Motion Mamba, a\nsimple and efficient approach that presents the pioneering motion generation\nmodel utilized SSMs. Specifically, we design a Hierarchical Temporal Mamba\n(HTM) block to process temporal data by ensemble varying numbers of isolated\nSSM modules across a symmetric U-Net architecture aimed at preserving motion\nconsistency between frames. We also design a Bidirectional Spatial Mamba (BSM)\nblock to bidirectionally process latent poses, to enhance accurate motion\ngeneration within a temporal frame. Our proposed method achieves up to 50% FID\nimprovement and up to 4 times faster on the HumanML3D and KIT-ML datasets\ncompared to the previous best diffusion-based method, which demonstrates strong\ncapabilities of high-quality long sequence motion modeling and real-time human\nmotion generation. See project website\nhttps://steve-zeyu-zhang.github.io/MotionMamba/",
        "publication_date": "2024-03-12T10:25:29Z",
        "upvotes": 11
    },
    "2403.07128": {
        "url": "https://arxiv.org/abs/2403.07128",
        "title": "FAX: Scalable and Differentiable Federated Primitives in JAX",
        "authors": [
            "Keith Rush",
            "Zachary Charles",
            "Zachary Garrett"
        ],
        "abstract": "We present FAX, a JAX-based library designed to support large-scale\ndistributed and federated computations in both data center and cross-device\napplications. FAX leverages JAX's sharding mechanisms to enable native\ntargeting of TPUs and state-of-the-art JAX runtimes, including Pathways. FAX\nembeds building blocks for federated computations as primitives in JAX. This\nenables three key benefits. First, FAX computations can be translated to XLA\nHLO. Second, FAX provides a full implementation of federated automatic\ndifferentiation, greatly simplifying the expression of federated computations.\nLast, FAX computations can be interpreted out to existing production\ncross-device federated compute systems. We show that FAX provides an easily\nprogrammable, performant, and scalable framework for federated computations in\nthe data center. FAX is available at\nhttps://github.com/google-research/google-research/tree/master/fax .",
        "publication_date": "2024-03-11T19:51:01Z",
        "upvotes": 11
    },
    "2403.07420": {
        "url": "https://arxiv.org/abs/2403.07420",
        "title": "DragAnything: Motion Control for Anything using Entity Representation",
        "authors": [
            "Weijia Wu",
            "Zhuang Li",
            "Yuchao Gu",
            "Rui Zhao",
            "Yefei He",
            "David Junhao Zhang",
            "Mike Zheng Shou",
            "Yan Li",
            "Tingting Gao",
            "Di Zhang"
        ],
        "abstract": "We introduce DragAnything, which utilizes a entity representation to achieve\nmotion control for any object in controllable video generation. Comparison to\nexisting motion control methods, DragAnything offers several advantages.\nFirstly, trajectory-based is more userfriendly for interaction, when acquiring\nother guidance signals (e.g., masks, depth maps) is labor-intensive. Users only\nneed to draw a line (trajectory) during interaction. Secondly, our entity\nrepresentation serves as an open-domain embedding capable of representing any\nobject, enabling the control of motion for diverse entities, including\nbackground. Lastly, our entity representation allows simultaneous and distinct\nmotion control for multiple objects. Extensive experiments demonstrate that our\nDragAnything achieves state-of-the-art performance for FVD, FID, and User\nStudy, particularly in terms of object motion control, where our method\nsurpasses the previous methods (e.g., DragNUWA) by 26% in human voting.",
        "publication_date": "2024-03-12T08:57:29Z",
        "upvotes": 11
    },
    "2403.07563": {
        "url": "https://arxiv.org/abs/2403.07563",
        "title": "Learning Generalizable Feature Fields for Mobile Manipulation",
        "authors": [
            "Ri-Zhao Qiu",
            "Yafei Hu",
            "Ge Yang",
            "Yuchen Song",
            "Yang Fu",
            "Jianglong Ye",
            "Jiteng Mu",
            "Ruihan Yang",
            "Nikolay Atanasov",
            "Sebastian Scherer",
            "Xiaolong Wang"
        ],
        "abstract": "An open problem in mobile manipulation is how to represent objects and scenes\nin a unified manner, so that robots can use it both for navigating in the\nenvironment and manipulating objects. The latter requires capturing intricate\ngeometry while understanding fine-grained semantics, whereas the former\ninvolves capturing the complexity inherit to an expansive physical scale. In\nthis work, we present GeFF (Generalizable Feature Fields), a scene-level\ngeneralizable neural feature field that acts as a unified representation for\nboth navigation and manipulation that performs in real-time. To do so, we treat\ngenerative novel view synthesis as a pre-training task, and then align the\nresulting rich scene priors with natural language via CLIP feature\ndistillation. We demonstrate the effectiveness of this approach by deploying\nGeFF on a quadrupedal robot equipped with a manipulator. We evaluate GeFF's\nability to generalize to open-set objects as well as running time, when\nperforming open-vocabulary mobile manipulation in dynamic scenes.",
        "publication_date": "2024-03-12T11:51:55Z",
        "upvotes": 6
    },
    "2403.08763": {
        "url": "https://arxiv.org/abs/2403.08763",
        "title": "Simple and Scalable Strategies to Continually Pre-train Large Language\n  Models",
        "authors": [
            "Adam Ibrahim",
            "Benjamin Th\u00e9rien",
            "Kshitij Gupta",
            "Mats L. Richter",
            "Quentin Anthony",
            "Timoth\u00e9e Lesort",
            "Eugene Belilovsky",
            "Irina Rish"
        ],
        "abstract": "Large language models (LLMs) are routinely pre-trained on billions of tokens,\nonly to start the process over again once new data becomes available. A much\nmore efficient solution is to continually pre-train these models, saving\nsignificant compute compared to re-training. However, the distribution shift\ninduced by new data typically results in degraded performance on previous data\nor poor adaptation to the new data. In this work, we show that a simple and\nscalable combination of learning rate (LR) re-warming, LR re-decaying, and\nreplay of previous data is sufficient to match the performance of fully\nre-training from scratch on all available data, as measured by the final loss\nand the average score on several language model (LM) evaluation benchmarks.\nSpecifically, we show this for a weak but realistic distribution shift between\ntwo commonly used LLM pre-training datasets (English$\\rightarrow$English) and a\nstronger distribution shift (English$\\rightarrow$German) at the $405$M\nparameter model scale with large dataset sizes (hundreds of billions of\ntokens). Selecting the weak but realistic shift for larger-scale experiments,\nwe also find that our continual learning strategies match the re-training\nbaseline for a 10B parameter LLM. Our results demonstrate that LLMs can be\nsuccessfully updated via simple and scalable continual learning strategies,\nmatching the re-training baseline using only a fraction of the compute.\nFinally, inspired by previous work, we propose alternatives to the cosine\nlearning rate schedule that help circumvent forgetting induced by LR re-warming\nand that are not bound to a fixed token budget.",
        "publication_date": "2024-03-13T17:58:57Z",
        "upvotes": 46
    },
    "2403.08295": {
        "url": "https://arxiv.org/abs/2403.08295",
        "title": "Gemma: Open Models Based on Gemini Research and Technology",
        "authors": [
            "Gemma Team",
            "Thomas Mesnard",
            "Cassidy Hardin",
            "Robert Dadashi",
            "Surya Bhupatiraju",
            "Shreya Pathak",
            "Laurent Sifre",
            "Morgane Rivi\u00e8re",
            "Mihir Sanjay Kale",
            "Juliette Love",
            "Pouya Tafti",
            "L\u00e9onard Hussenot",
            "Aakanksha Chowdhery",
            "Adam Roberts",
            "Aditya Barua",
            "Alex Botev",
            "Alex Castro-Ros",
            "Ambrose Slone",
            "Am\u00e9lie H\u00e9liou",
            "Andrea Tacchetti",
            "Anna Bulanova",
            "Antonia Paterson",
            "Beth Tsai",
            "Bobak Shahriari",
            "Charline Le Lan",
            "Christopher A. Choquette-Choo",
            "Cl\u00e9ment Crepy",
            "Daniel Cer",
            "Daphne Ippolito",
            "David Reid",
            "Elena Buchatskaya",
            "Eric Ni",
            "Eric Noland",
            "Geng Yan",
            "George Tucker",
            "George-Christian Muraru",
            "Grigory Rozhdestvenskiy",
            "Henryk Michalewski",
            "Ian Tenney",
            "Ivan Grishchenko",
            "Jacob Austin",
            "James Keeling",
            "Jane Labanowski",
            "Jean-Baptiste Lespiau",
            "Jeff Stanway",
            "Jenny Brennan",
            "Jeremy Chen",
            "Johan Ferret",
            "Justin Chiu",
            "Justin Mao-Jones",
            "Katherine Lee",
            "Kathy Yu",
            "Katie Millican",
            "Lars Lowe Sjoesund",
            "Lisa Lee",
            "Lucas Dixon",
            "Machel Reid",
            "Maciej Miku\u0142a",
            "Mateo Wirth",
            "Michael Sharman",
            "Nikolai Chinaev",
            "Nithum Thain",
            "Olivier Bachem",
            "Oscar Chang",
            "Oscar Wahltinez",
            "Paige Bailey",
            "Paul Michel",
            "Petko Yotov",
            "Pier Giuseppe Sessa",
            "Rahma Chaabouni",
            "Ramona Comanescu",
            "Reena Jana",
            "Rohan Anil",
            "Ross McIlroy",
            "Ruibo Liu",
            "Ryan Mullins",
            "Samuel L Smith",
            "Sebastian Borgeaud",
            "Sertan Girgin",
            "Sholto Douglas",
            "Shree Pandya",
            "Siamak Shakeri",
            "Soham De",
            "Ted Klimenko",
            "Tom Hennigan",
            "Vlad Feinberg",
            "Wojciech Stokowiec",
            "Yu-hui Chen",
            "Zafarali Ahmed",
            "Zhitao Gong",
            "Tris Warkentin",
            "Ludovic Peran",
            "Minh Giang",
            "Cl\u00e9ment Farabet",
            "Oriol Vinyals",
            "Jeff Dean",
            "Koray Kavukcuoglu",
            "Demis Hassabis",
            "Zoubin Ghahramani",
            "Douglas Eck",
            "Joelle Barral",
            "Fernando Pereira",
            "Eli Collins",
            "Armand Joulin",
            "Noah Fiedel",
            "Evan Senter",
            "Alek Andreev",
            "Kathleen Kenealy"
        ],
        "abstract": "This work introduces Gemma, a family of lightweight, state-of-the art open\nmodels built from the research and technology used to create Gemini models.\nGemma models demonstrate strong performance across academic benchmarks for\nlanguage understanding, reasoning, and safety. We release two sizes of models\n(2 billion and 7 billion parameters), and provide both pretrained and\nfine-tuned checkpoints. Gemma outperforms similarly sized open models on 11 out\nof 18 text-based tasks, and we present comprehensive evaluations of safety and\nresponsibility aspects of the models, alongside a detailed description of model\ndevelopment. We believe the responsible release of LLMs is critical for\nimproving the safety of frontier models, and for enabling the next wave of LLM\ninnovations.",
        "publication_date": "2024-03-13T06:59:16Z",
        "upvotes": 41
    },
    "2403.08764": {
        "url": "https://arxiv.org/abs/2403.08764",
        "title": "VLOGGER: Multimodal Diffusion for Embodied Avatar Synthesis",
        "authors": [
            "Enric Corona",
            "Andrei Zanfir",
            "Eduard Gabriel Bazavan",
            "Nikos Kolotouros",
            "Thiemo Alldieck",
            "Cristian Sminchisescu"
        ],
        "abstract": "We propose VLOGGER, a method for audio-driven human video generation from a\nsingle input image of a person, which builds on the success of recent\ngenerative diffusion models. Our method consists of 1) a stochastic\nhuman-to-3d-motion diffusion model, and 2) a novel diffusion-based architecture\nthat augments text-to-image models with both spatial and temporal controls.\nThis supports the generation of high quality video of variable length, easily\ncontrollable through high-level representations of human faces and bodies. In\ncontrast to previous work, our method does not require training for each\nperson, does not rely on face detection and cropping, generates the complete\nimage (not just the face or the lips), and considers a broad spectrum of\nscenarios (e.g. visible torso or diverse subject identities) that are critical\nto correctly synthesize humans who communicate. We also curate MENTOR, a new\nand diverse dataset with 3d pose and expression annotations, one order of\nmagnitude larger than previous ones (800,000 identities) and with dynamic\ngestures, on which we train and ablate our main technical contributions.\n  VLOGGER outperforms state-of-the-art methods in three public benchmarks,\nconsidering image quality, identity preservation and temporal consistency while\nalso generating upper-body gestures. We analyze the performance of VLOGGER with\nrespect to multiple diversity metrics, showing that our architectural choices\nand the use of MENTOR benefit training a fair and unbiased model at scale.\nFinally we show applications in video editing and personalization.",
        "publication_date": "2024-03-13T17:59:02Z",
        "upvotes": 33
    },
    "2403.08715": {
        "url": "https://arxiv.org/abs/2403.08715",
        "title": "SOTOPIA-$\u03c0$: Interactive Learning of Socially Intelligent Language\n  Agents",
        "authors": [
            "Ruiyi Wang",
            "Haofei Yu",
            "Wenxin Zhang",
            "Zhengyang Qi",
            "Maarten Sap",
            "Graham Neubig",
            "Yonatan Bisk",
            "Hao Zhu"
        ],
        "abstract": "Humans learn social skills through both imitation and social interaction.\nThis social learning process is largely understudied by existing research on\nbuilding language agents. Motivated by this gap, we propose an interactive\nlearning method, SOTOPIA-$\\pi$, improving the social intelligence of language\nagents. This method leverages behavior cloning and self-reinforcement training\non filtered social interaction data according to large language model (LLM)\nratings. We show that our training method allows a 7B LLM to reach the social\ngoal completion ability of an expert model (GPT-4-based agent), while improving\nthe safety of language agents and maintaining general QA ability on the MMLU\nbenchmark. We also find that this training paradigm uncovers some difficulties\nin LLM-based evaluation of social intelligence: LLM-based evaluators\noverestimate the abilities of the language agents trained specifically for\nsocial interaction.",
        "publication_date": "2024-03-13T17:17:48Z",
        "upvotes": 19
    },
    "2403.08268": {
        "url": "https://arxiv.org/abs/2403.08268",
        "title": "Follow-Your-Click: Open-domain Regional Image Animation via Short\n  Prompts",
        "authors": [
            "Yue Ma",
            "Yingqing He",
            "Hongfa Wang",
            "Andong Wang",
            "Chenyang Qi",
            "Chengfei Cai",
            "Xiu Li",
            "Zhifeng Li",
            "Heung-Yeung Shum",
            "Wei Liu",
            "Qifeng Chen"
        ],
        "abstract": "Despite recent advances in image-to-video generation, better controllability\nand local animation are less explored. Most existing image-to-video methods are\nnot locally aware and tend to move the entire scene. However, human artists may\nneed to control the movement of different objects or regions. Additionally,\ncurrent I2V methods require users not only to describe the target motion but\nalso to provide redundant detailed descriptions of frame contents. These two\nissues hinder the practical utilization of current I2V tools. In this paper, we\npropose a practical framework, named Follow-Your-Click, to achieve image\nanimation with a simple user click (for specifying what to move) and a short\nmotion prompt (for specifying how to move). Technically, we propose the\nfirst-frame masking strategy, which significantly improves the video generation\nquality, and a motion-augmented module equipped with a short motion prompt\ndataset to improve the short prompt following abilities of our model. To\nfurther control the motion speed, we propose flow-based motion magnitude\ncontrol to control the speed of target movement more precisely. Our framework\nhas simpler yet precise user control and better generation performance than\nprevious methods. Extensive experiments compared with 7 baselines, including\nboth commercial tools and research methods on 8 metrics, suggest the\nsuperiority of our approach. Project Page: https://follow-your-click.github.io/",
        "publication_date": "2024-03-13T05:44:37Z",
        "upvotes": 15
    },
    "2403.07918": {
        "url": "https://arxiv.org/abs/2403.07918",
        "title": "On the Societal Impact of Open Foundation Models",
        "authors": [
            "Sayash Kapoor",
            "Rishi Bommasani",
            "Kevin Klyman",
            "Shayne Longpre",
            "Ashwin Ramaswami",
            "Peter Cihon",
            "Aspen Hopkins",
            "Kevin Bankston",
            "Stella Biderman",
            "Miranda Bogen",
            "Rumman Chowdhury",
            "Alex Engler",
            "Peter Henderson",
            "Yacine Jernite",
            "Seth Lazar",
            "Stefano Maffulli",
            "Alondra Nelson",
            "Joelle Pineau",
            "Aviya Skowron",
            "Dawn Song",
            "Victor Storchan",
            "Daniel Zhang",
            "Daniel E. Ho",
            "Percy Liang",
            "Arvind Narayanan"
        ],
        "abstract": "Foundation models are powerful technologies: how they are released publicly\ndirectly shapes their societal impact. In this position paper, we focus on open\nfoundation models, defined here as those with broadly available model weights\n(e.g. Llama 2, Stable Diffusion XL). We identify five distinctive properties\n(e.g. greater customizability, poor monitoring) of open foundation models that\nlead to both their benefits and risks. Open foundation models present\nsignificant benefits, with some caveats, that span innovation, competition, the\ndistribution of decision-making power, and transparency. To understand their\nrisks of misuse, we design a risk assessment framework for analyzing their\nmarginal risk. Across several misuse vectors (e.g. cyberattacks, bioweapons),\nwe find that current research is insufficient to effectively characterize the\nmarginal risk of open foundation models relative to pre-existing technologies.\nThe framework helps explain why the marginal risk is low in some cases,\nclarifies disagreements about misuse risks by revealing that past work has\nfocused on different subsets of the framework with different assumptions, and\narticulates a way forward for more constructive debate. Overall, our work helps\nsupport a more grounded assessment of the societal impact of open foundation\nmodels by outlining what research is needed to empirically validate their\ntheoretical benefits and risks.",
        "publication_date": "2024-02-27T16:49:53Z",
        "upvotes": 14
    },
    "2403.08629": {
        "url": "https://arxiv.org/abs/2403.08629",
        "title": "Scaling Up Dynamic Human-Scene Interaction Modeling",
        "authors": [
            "Nan Jiang",
            "Zhiyuan Zhang",
            "Hongjie Li",
            "Xiaoxuan Ma",
            "Zan Wang",
            "Yixin Chen",
            "Tengyu Liu",
            "Yixin Zhu",
            "Siyuan Huang"
        ],
        "abstract": "Confronting the challenges of data scarcity and advanced motion synthesis in\nhuman-scene interaction modeling, we introduce the TRUMANS dataset alongside a\nnovel HSI motion synthesis method. TRUMANS stands as the most comprehensive\nmotion-captured HSI dataset currently available, encompassing over 15 hours of\nhuman interactions across 100 indoor scenes. It intricately captures whole-body\nhuman motions and part-level object dynamics, focusing on the realism of\ncontact. This dataset is further scaled up by transforming physical\nenvironments into exact virtual models and applying extensive augmentations to\nappearance and motion for both humans and objects while maintaining interaction\nfidelity. Utilizing TRUMANS, we devise a diffusion-based autoregressive model\nthat efficiently generates HSI sequences of any length, taking into account\nboth scene context and intended actions. In experiments, our approach shows\nremarkable zero-shot generalizability on a range of 3D scene datasets (e.g.,\nPROX, Replica, ScanNet, ScanNet++), producing motions that closely mimic\noriginal motion-captured sequences, as confirmed by quantitative experiments\nand human studies.",
        "publication_date": "2024-03-13T15:45:04Z",
        "upvotes": 13
    },
    "2403.08540": {
        "url": "https://arxiv.org/abs/2403.08540",
        "title": "Language models scale reliably with over-training and on downstream\n  tasks",
        "authors": [
            "Samir Yitzhak Gadre",
            "Georgios Smyrnis",
            "Vaishaal Shankar",
            "Suchin Gururangan",
            "Mitchell Wortsman",
            "Rulin Shao",
            "Jean Mercat",
            "Alex Fang",
            "Jeffrey Li",
            "Sedrick Keh",
            "Rui Xin",
            "Marianna Nezhurina",
            "Igor Vasiljevic",
            "Jenia Jitsev",
            "Alexandros G. Dimakis",
            "Gabriel Ilharco",
            "Shuran Song",
            "Thomas Kollar",
            "Yair Carmon",
            "Achal Dave",
            "Reinhard Heckel",
            "Niklas Muennighoff",
            "Ludwig Schmidt"
        ],
        "abstract": "Scaling laws are useful guides for developing language models, but there are\nstill gaps between current scaling studies and how language models are\nultimately trained and evaluated. For instance, scaling is usually studied in\nthe compute-optimal training regime (i.e., \"Chinchilla optimal\" regime);\nhowever, in practice, models are often over-trained to reduce inference costs.\nMoreover, scaling laws mostly predict loss on next-token prediction, but\nultimately models are compared based on downstream task performance. In this\npaper, we address both shortcomings. To do so, we create a testbed of 104\nmodels with 0.011B to 6.9B parameters trained with various numbers of tokens on\nthree data distributions. First, we investigate scaling in the over-trained\nregime. We fit scaling laws that extrapolate in both the number of model\nparameters and the ratio of training tokens to parameters. This enables us to\npredict the validation loss of a 1.4B parameter, 900B token run (i.e.,\n32$\\times$ over-trained) and a 6.9B parameter, 138B token\nrun$\\unicode{x2014}$each from experiments that take 300$\\times$ less compute.\nSecond, we relate the perplexity of a language model to its downstream task\nperformance via a power law. We use this law to predict top-1 error averaged\nover downstream tasks for the two aforementioned models using experiments that\ntake 20$\\times$ less compute. Our experiments are available at\nhttps://github.com/mlfoundations/scaling.",
        "publication_date": "2024-03-13T13:54:00Z",
        "upvotes": 13
    },
    "2403.08551": {
        "url": "https://arxiv.org/abs/2403.08551",
        "title": "GaussianImage: 1000 FPS Image Representation and Compression by 2D\n  Gaussian Splatting",
        "authors": [
            "Xinjie Zhang",
            "Xingtong Ge",
            "Tongda Xu",
            "Dailan He",
            "Yan Wang",
            "Hongwei Qin",
            "Guo Lu",
            "Jing Geng",
            "Jun Zhang"
        ],
        "abstract": "Implicit neural representations (INRs) recently achieved great success in\nimage representation and compression, offering high visual quality and fast\nrendering speeds with 10-1000 FPS, assuming sufficient GPU resources are\navailable. However, this requirement often hinders their use on low-end devices\nwith limited memory. In response, we propose a groundbreaking paradigm of image\nrepresentation and compression by 2D Gaussian Splatting, named GaussianImage.\nWe first introduce 2D Gaussian to represent the image, where each Gaussian has\n8 parameters including position, covariance and color. Subsequently, we unveil\na novel rendering algorithm based on accumulated summation. Remarkably, our\nmethod with a minimum of 3$\\times$ lower GPU memory usage and 5$\\times$ faster\nfitting time not only rivals INRs (e.g., WIRE, I-NGP) in representation\nperformance, but also delivers a faster rendering speed of 1500-2000 FPS\nregardless of parameter size. Furthermore, we integrate existing vector\nquantization technique to build an image codec. Experimental results\ndemonstrate that our codec attains rate-distortion performance comparable to\ncompression-based INRs such as COIN and COIN++, while facilitating decoding\nspeeds of approximately 1000 FPS. Additionally, preliminary proof of concept\nshows that our codec surpasses COIN and COIN++ in performance when using\npartial bits-back coding.",
        "publication_date": "2024-03-13T14:02:54Z",
        "upvotes": 8
    },
    "2403.09611": {
        "url": "https://arxiv.org/abs/2403.09611",
        "title": "MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training",
        "authors": [
            "Brandon McKinzie",
            "Zhe Gan",
            "Jean-Philippe Fauconnier",
            "Sam Dodge",
            "Bowen Zhang",
            "Philipp Dufter",
            "Dhruti Shah",
            "Xianzhi Du",
            "Futang Peng",
            "Floris Weers",
            "Anton Belyi",
            "Haotian Zhang",
            "Karanjeet Singh",
            "Doug Kang",
            "Ankur Jain",
            "Hongyu H\u00e8",
            "Max Schwarzer",
            "Tom Gunter",
            "Xiang Kong",
            "Aonan Zhang",
            "Jianyu Wang",
            "Chong Wang",
            "Nan Du",
            "Tao Lei",
            "Sam Wiseman",
            "Guoli Yin",
            "Mark Lee",
            "Zirui Wang",
            "Ruoming Pang",
            "Peter Grasch",
            "Alexander Toshev",
            "Yinfei Yang"
        ],
        "abstract": "In this work, we discuss building performant Multimodal Large Language Models\n(MLLMs). In particular, we study the importance of various architecture\ncomponents and data choices. Through careful and comprehensive ablations of the\nimage encoder, the vision language connector, and various pre-training data\nchoices, we identified several crucial design lessons. For example, we\ndemonstrate that for large-scale multimodal pre-training using a careful mix of\nimage-caption, interleaved image-text, and text-only data is crucial for\nachieving state-of-the-art (SOTA) few-shot results across multiple benchmarks,\ncompared to other published pre-training results. Further, we show that the\nimage encoder together with image resolution and the image token count has\nsubstantial impact, while the vision-language connector design is of\ncomparatively negligible importance. By scaling up the presented recipe, we\nbuild MM1, a family of multimodal models up to 30B parameters, including both\ndense models and mixture-of-experts (MoE) variants, that are SOTA in\npre-training metrics and achieve competitive performance after supervised\nfine-tuning on a range of established multimodal benchmarks. Thanks to\nlarge-scale pre-training, MM1 enjoys appealing properties such as enhanced\nin-context learning, and multi-image reasoning, enabling few-shot\nchain-of-thought prompting.",
        "publication_date": "2024-03-14T17:51:32Z",
        "upvotes": 113
    },
    "2403.09629": {
        "url": "https://arxiv.org/abs/2403.09629",
        "title": "Quiet-STaR: Language Models Can Teach Themselves to Think Before\n  Speaking",
        "authors": [
            "Eric Zelikman",
            "Georges Harik",
            "Yijia Shao",
            "Varuna Jayasiri",
            "Nick Haber",
            "Noah D. Goodman"
        ],
        "abstract": "When writing and talking, people sometimes pause to think. Although\nreasoning-focused works have often framed reasoning as a method of answering\nquestions or completing agentic tasks, reasoning is implicit in almost all\nwritten text. For example, this applies to the steps not stated between the\nlines of a proof or to the theory of mind underlying a conversation. In the\nSelf-Taught Reasoner (STaR, Zelikman et al. 2022), useful thinking is learned\nby inferring rationales from few-shot examples in question-answering and\nlearning from those that lead to a correct answer. This is a highly constrained\nsetting -- ideally, a language model could instead learn to infer unstated\nrationales in arbitrary text. We present Quiet-STaR, a generalization of STaR\nin which LMs learn to generate rationales at each token to explain future text,\nimproving their predictions. We address key challenges, including 1) the\ncomputational cost of generating continuations, 2) the fact that the LM does\nnot initially know how to generate or use internal thoughts, and 3) the need to\npredict beyond individual next tokens. To resolve these, we propose a tokenwise\nparallel sampling algorithm, using learnable tokens indicating a thought's\nstart and end, and an extended teacher-forcing technique. Encouragingly,\ngenerated rationales disproportionately help model difficult-to-predict tokens\nand improve the LM's ability to directly answer difficult questions. In\nparticular, after continued pretraining of an LM on a corpus of internet text\nwith Quiet-STaR, we find zero-shot improvements on GSM8K\n(5.9%$\\rightarrow$10.9%) and CommonsenseQA (36.3%$\\rightarrow$47.2%) and\nobserve a perplexity improvement of difficult tokens in natural text.\nCrucially, these improvements require no fine-tuning on these tasks. Quiet-STaR\nmarks a step towards LMs that can learn to reason in a more general and\nscalable way.",
        "publication_date": "2024-03-14T17:58:16Z",
        "upvotes": 52
    },
    "2403.09029": {
        "url": "https://arxiv.org/abs/2403.09029",
        "title": "Unlocking the conversion of Web Screenshots into HTML Code with the\n  WebSight Dataset",
        "authors": [
            "Hugo Lauren\u00e7on",
            "L\u00e9o Tronchon",
            "Victor Sanh"
        ],
        "abstract": "Using vision-language models (VLMs) in web development presents a promising\nstrategy to increase efficiency and unblock no-code solutions: by providing a\nscreenshot or a sketch of a UI, a VLM could generate the code to reproduce it,\nfor instance in a language like HTML. Despite the advancements in VLMs for\nvarious tasks, the specific challenge of converting a screenshot into a\ncorresponding HTML has been minimally explored. We posit that this is mainly\ndue to the absence of a suitable, high-quality dataset. This work introduces\nWebSight, a synthetic dataset consisting of 2 million pairs of HTML codes and\ntheir corresponding screenshots. We fine-tune a foundational VLM on our dataset\nand show proficiency in converting webpage screenshots to functional HTML code.\nTo accelerate the research in this area, we open-source WebSight.",
        "publication_date": "2024-03-14T01:40:40Z",
        "upvotes": 50
    },
    "2403.09394": {
        "url": "https://arxiv.org/abs/2403.09394",
        "title": "GiT: Towards Generalist Vision Transformer through Universal Language\n  Interface",
        "authors": [
            "Haiyang Wang",
            "Hao Tang",
            "Li Jiang",
            "Shaoshuai Shi",
            "Muhammad Ferjad Naeem",
            "Hongsheng Li",
            "Bernt Schiele",
            "Liwei Wang"
        ],
        "abstract": "This paper proposes a simple, yet effective framework, called GiT,\nsimultaneously applicable for various vision tasks only with a vanilla ViT.\nMotivated by the universality of the Multi-layer Transformer architecture (e.g,\nGPT) widely used in large language models (LLMs), we seek to broaden its scope\nto serve as a powerful vision foundation model (VFM). However, unlike language\nmodeling, visual tasks typically require specific modules, such as bounding box\nheads for detection and pixel decoders for segmentation, greatly hindering the\napplication of powerful multi-layer transformers in the vision domain. To solve\nthis, we design a universal language interface that empowers the successful\nauto-regressive decoding to adeptly unify various visual tasks, from\nimage-level understanding (e.g., captioning), over sparse perception (e.g.,\ndetection), to dense prediction (e.g., segmentation). Based on the above\ndesigns, the entire model is composed solely of a ViT, without any specific\nadditions, offering a remarkable architectural simplification. GiT is a\nmulti-task visual model, jointly trained across five representative benchmarks\nwithout task-specific fine-tuning. Interestingly, our GiT builds a new\nbenchmark in generalist performance, and fosters mutual enhancement across\ntasks, leading to significant improvements compared to isolated training. This\nreflects a similar impact observed in LLMs. Further enriching training with 27\ndatasets, GiT achieves strong zero-shot results over various tasks. Due to its\nsimple design, this paradigm holds promise for narrowing the architectural gap\nbetween vision and language. Code and models will be available at\n\\url{https://github.com/Haiyang-W/GiT}.",
        "publication_date": "2024-03-14T13:47:41Z",
        "upvotes": 24
    },
    "2403.09055": {
        "url": "https://arxiv.org/abs/2403.09055",
        "title": "StreamMultiDiffusion: Real-Time Interactive Generation with Region-Based\n  Semantic Control",
        "authors": [
            "Jaerin Lee",
            "Daniel Sungho Jung",
            "Kanggeon Lee",
            "Kyoung Mu Lee"
        ],
        "abstract": "The enormous success of diffusion models in text-to-image synthesis has made\nthem promising candidates for the next generation of end-user applications for\nimage generation and editing. Previous works have focused on improving the\nusability of diffusion models by reducing the inference time or increasing user\ninteractivity by allowing new, fine-grained controls such as region-based text\nprompts. However, we empirically find that integrating both branches of works\nis nontrivial, limiting the potential of diffusion models. To solve this\nincompatibility, we present StreamMultiDiffusion, the first real-time\nregion-based text-to-image generation framework. By stabilizing fast inference\ntechniques and restructuring the model into a newly proposed multi-prompt\nstream batch architecture, we achieve $\\times 10$ faster panorama generation\nthan existing solutions, and the generation speed of 1.57 FPS in region-based\ntext-to-image synthesis on a single RTX 2080 Ti GPU. Our solution opens up a\nnew paradigm for interactive image generation named semantic palette, where\nhigh-quality images are generated in real-time from given multiple hand-drawn\nregions, encoding prescribed semantic meanings (e.g., eagle, girl). Our code\nand demo application are available at\nhttps://github.com/ironjr/StreamMultiDiffusion.",
        "publication_date": "2024-03-14T02:51:01Z",
        "upvotes": 23
    },
    "2403.09334": {
        "url": "https://arxiv.org/abs/2403.09334",
        "title": "Video Editing via Factorized Diffusion Distillation",
        "authors": [
            "Uriel Singer",
            "Amit Zohar",
            "Yuval Kirstain",
            "Shelly Sheynin",
            "Adam Polyak",
            "Devi Parikh",
            "Yaniv Taigman"
        ],
        "abstract": "We introduce Emu Video Edit (EVE), a model that establishes a new\nstate-of-the art in video editing without relying on any supervised video\nediting data. To develop EVE we separately train an image editing adapter and a\nvideo generation adapter, and attach both to the same text-to-image model.\nThen, to align the adapters towards video editing we introduce a new\nunsupervised distillation procedure, Factorized Diffusion Distillation. This\nprocedure distills knowledge from one or more teachers simultaneously, without\nany supervised data. We utilize this procedure to teach EVE to edit videos by\njointly distilling knowledge to (i) precisely edit each individual frame from\nthe image editing adapter, and (ii) ensure temporal consistency among the\nedited frames using the video generation adapter. Finally, to demonstrate the\npotential of our approach in unlocking other capabilities, we align additional\ncombinations of adapters",
        "publication_date": "2024-03-14T12:22:54Z",
        "upvotes": 21
    },
    "2403.09347": {
        "url": "https://arxiv.org/abs/2403.09347",
        "title": "BurstAttention: An Efficient Distributed Attention Framework for\n  Extremely Long Sequences",
        "authors": [
            "Sun Ao",
            "Weilin Zhao",
            "Xu Han",
            "Cheng Yang",
            "Zhiyuan Liu",
            "Chuan Shi",
            "Maosong Sun",
            "Shengnan Wang",
            "Teng Su"
        ],
        "abstract": "Effective attention modules have played a crucial role in the success of\nTransformer-based large language models (LLMs), but the quadratic time and\nmemory complexities of these attention modules also pose a challenge when\nprocessing long sequences. One potential solution for the long sequence problem\nis to utilize distributed clusters to parallelize the computation of attention\nmodules across multiple devices (e.g., GPUs). However, adopting a distributed\napproach inevitably introduces extra memory overheads to store local attention\nresults and incurs additional communication costs to aggregate local results\ninto global ones. In this paper, we propose a distributed attention framework\nnamed ``BurstAttention'' to optimize memory access and communication operations\nat both the global cluster and local device levels. In our experiments, we\ncompare BurstAttention with other competitive distributed attention solutions\nfor long sequence processing. The experimental results under different length\nsettings demonstrate that BurstAttention offers significant advantages for\nprocessing long sequences compared with these competitive baselines, reducing\n40% communication overheads and achieving 2 X speedup during training 32K\nsequence length on 8 X A100.",
        "publication_date": "2024-03-14T12:51:58Z",
        "upvotes": 20
    },
    "2403.09333": {
        "url": "https://arxiv.org/abs/2403.09333",
        "title": "Griffon v2: Advancing Multimodal Perception with High-Resolution Scaling\n  and Visual-Language Co-Referring",
        "authors": [
            "Yufei Zhan",
            "Yousong Zhu",
            "Hongyin Zhao",
            "Fan Yang",
            "Ming Tang",
            "Jinqiao Wang"
        ],
        "abstract": "Large Vision Language Models have achieved fine-grained object perception,\nbut the limitation of image resolution remains a significant obstacle to\nsurpass the performance of task-specific experts in complex and dense\nscenarios. Such limitation further restricts the model's potential to achieve\nnuanced visual and language referring in domains such as GUI Agents, Counting\nand \\etc. To address this issue, we introduce a unified high-resolution\ngeneralist model, Griffon v2, enabling flexible object referring with visual\nand textual prompts. To efficiently scaling up image resolution, we design a\nsimple and lightweight down-sampling projector to overcome the input tokens\nconstraint in Large Language Models. This design inherently preserves the\ncomplete contexts and fine details, and significantly improves multimodal\nperception ability especially for small objects. Building upon this, we further\nequip the model with visual-language co-referring capabilities through a\nplug-and-play visual tokenizer. It enables user-friendly interaction with\nflexible target images, free-form texts and even coordinates. Experiments\ndemonstrate that Griffon v2 can localize any objects of interest with visual\nand textual referring, achieve state-of-the-art performance on REC, phrase\ngrounding, and REG tasks, and outperform expert models in object detection and\nobject counting. Data, codes and models will be released at\nhttps://github.com/jefferyZhan/Griffon.",
        "publication_date": "2024-03-14T12:21:37Z",
        "upvotes": 13
    },
    "2403.09626": {
        "url": "https://arxiv.org/abs/2403.09626",
        "title": "Video Mamba Suite: State Space Model as a Versatile Alternative for\n  Video Understanding",
        "authors": [
            "Guo Chen",
            "Yifei Huang",
            "Jilan Xu",
            "Baoqi Pei",
            "Zhe Chen",
            "Zhiqi Li",
            "Jiahao Wang",
            "Kunchang Li",
            "Tong Lu",
            "Limin Wang"
        ],
        "abstract": "Understanding videos is one of the fundamental directions in computer vision\nresearch, with extensive efforts dedicated to exploring various architectures\nsuch as RNN, 3D CNN, and Transformers. The newly proposed architecture of state\nspace model, e.g., Mamba, shows promising traits to extend its success in long\nsequence modeling to video modeling. To assess whether Mamba can be a viable\nalternative to Transformers in the video understanding domain, in this work, we\nconduct a comprehensive set of studies, probing different roles Mamba can play\nin modeling videos, while investigating diverse tasks where Mamba could exhibit\nsuperiority. We categorize Mamba into four roles for modeling videos, deriving\na Video Mamba Suite composed of 14 models/modules, and evaluating them on 12\nvideo understanding tasks. Our extensive experiments reveal the strong\npotential of Mamba on both video-only and video-language tasks while showing\npromising efficiency-performance trade-offs. We hope this work could provide\nvaluable data points and insights for future research on video understanding.\nCode is public: https://github.com/OpenGVLab/video-mamba-suite.",
        "publication_date": "2024-03-14T17:57:07Z",
        "upvotes": 11
    },
    "2403.09622": {
        "url": "https://arxiv.org/abs/2403.09622",
        "title": "Glyph-ByT5: A Customized Text Encoder for Accurate Visual Text Rendering",
        "authors": [
            "Zeyu Liu",
            "Weicong Liang",
            "Zhanhao Liang",
            "Chong Luo",
            "Ji Li",
            "Gao Huang",
            "Yuhui Yuan"
        ],
        "abstract": "Visual text rendering poses a fundamental challenge for contemporary\ntext-to-image generation models, with the core problem lying in text encoder\ndeficiencies. To achieve accurate text rendering, we identify two crucial\nrequirements for text encoders: character awareness and alignment with glyphs.\nOur solution involves crafting a series of customized text encoder, Glyph-ByT5,\nby fine-tuning the character-aware ByT5 encoder using a meticulously curated\npaired glyph-text dataset. We present an effective method for integrating\nGlyph-ByT5 with SDXL, resulting in the creation of the Glyph-SDXL model for\ndesign image generation. This significantly enhances text rendering accuracy,\nimproving it from less than $20\\%$ to nearly $90\\%$ on our design image\nbenchmark. Noteworthy is Glyph-SDXL's newfound ability for text paragraph\nrendering, achieving high spelling accuracy for tens to hundreds of characters\nwith automated multi-line layouts. Finally, through fine-tuning Glyph-SDXL with\na small set of high-quality, photorealistic images featuring visual text, we\nshowcase a substantial improvement in scene text rendering capabilities in\nopen-domain real images. These compelling outcomes aim to encourage further\nexploration in designing customized text encoders for diverse and challenging\ntasks.",
        "publication_date": "2024-03-14T17:55:33Z",
        "upvotes": 9
    },
    "2403.09530": {
        "url": "https://arxiv.org/abs/2403.09530",
        "title": "VisionGPT-3D: A Generalized Multimodal Agent for Enhanced 3D Vision\n  Understanding",
        "authors": [
            "Chris Kelly",
            "Luhui Hu",
            "Jiayin Hu",
            "Yu Tian",
            "Deshun Yang",
            "Bang Yang",
            "Cindy Yang",
            "Zihao Li",
            "Zaoshan Huang",
            "Yuexian Zou"
        ],
        "abstract": "The evolution of text to visual components facilitates people's daily lives,\nsuch as generating image, videos from text and identifying the desired elements\nwithin the images. Computer vision models involving the multimodal abilities in\nthe previous days are focused on image detection, classification based on\nwell-defined objects. Large language models (LLMs) introduces the\ntransformation from nature language to visual objects, which present the visual\nlayout for text contexts. OpenAI GPT-4 has emerged as the pinnacle in LLMs,\nwhile the computer vision (CV) domain boasts a plethora of state-of-the-art\n(SOTA) models and algorithms to convert 2D images to their 3D representations.\nHowever, the mismatching between the algorithms with the problem could lead to\nundesired results. In response to this challenge, we propose an unified\nVisionGPT-3D framework to consolidate the state-of-the-art vision models,\nthereby facilitating the development of vision-oriented AI. VisionGPT-3D\nprovides a versatile multimodal framework building upon the strengths of\nmultimodal foundation models. It seamlessly integrates various SOTA vision\nmodels and brings the automation in the selection of SOTA vision models,\nidentifies the suitable 3D mesh creation algorithms corresponding to 2D depth\nmaps analysis, generates optimal results based on diverse multimodal inputs\nsuch as text prompts.\n  Keywords: VisionGPT-3D, 3D vision understanding, Multimodal agent",
        "publication_date": "2024-03-14T16:13:00Z",
        "upvotes": 8
    },
    "2403.08773": {
        "url": "https://arxiv.org/abs/2403.08773",
        "title": "Veagle: Advancements in Multimodal Representation Learning",
        "authors": [
            "Rajat Chawla",
            "Arkajit Datta",
            "Tushar Verma",
            "Adarsh Jha",
            "Anmol Gautam",
            "Ayush Vatsal",
            "Sukrit Chaterjee",
            "Mukunda NS",
            "Ishaan Bhola"
        ],
        "abstract": "Lately, researchers in artificial intelligence have been really interested in\nhow language and vision come together, giving rise to the development of\nmultimodal models that aim to seamlessly integrate textual and visual\ninformation. Multimodal models, an extension of Large Language Models (LLMs),\nhave exhibited remarkable capabilities in addressing a diverse array of tasks,\nranging from image captioning and visual question answering (VQA) to visual\ngrounding. While these models have showcased significant advancements,\nchallenges persist in accurately interpreting images and answering the\nquestion, a common occurrence in real-world scenarios. This paper introduces a\nnovel approach to enhance the multimodal capabilities of existing models. In\nresponse to the limitations observed in current Vision Language Models (VLMs)\nand Multimodal Large Language Models (MLLMs), our proposed model Veagle,\nincorporates a unique mechanism inspired by the successes and insights of\nprevious works. Veagle leverages a dynamic mechanism to project encoded visual\ninformation directly into the language model. This dynamic approach allows for\na more nuanced understanding of intricate details present in visual contexts.\nTo validate the effectiveness of Veagle, we conduct comprehensive experiments\non benchmark datasets, emphasizing tasks such as visual question answering and\nimage understanding. Our results indicate a improvement of 5-6 \\% in\nperformance, with Veagle outperforming existing models by a notable margin. The\noutcomes underscore the model's versatility and applicability beyond\ntraditional benchmarks.",
        "publication_date": "2024-01-18T12:45:25Z",
        "upvotes": 7
    },
    "2403.09338": {
        "url": "https://arxiv.org/abs/2403.09338",
        "title": "LocalMamba: Visual State Space Model with Windowed Selective Scan",
        "authors": [
            "Tao Huang",
            "Xiaohuan Pei",
            "Shan You",
            "Fei Wang",
            "Chen Qian",
            "Chang Xu"
        ],
        "abstract": "Recent advancements in state space models, notably Mamba, have demonstrated\nsignificant progress in modeling long sequences for tasks like language\nunderstanding. Yet, their application in vision tasks has not markedly\nsurpassed the performance of traditional Convolutional Neural Networks (CNNs)\nand Vision Transformers (ViTs). This paper posits that the key to enhancing\nVision Mamba (ViM) lies in optimizing scan directions for sequence modeling.\nTraditional ViM approaches, which flatten spatial tokens, overlook the\npreservation of local 2D dependencies, thereby elongating the distance between\nadjacent tokens. We introduce a novel local scanning strategy that divides\nimages into distinct windows, effectively capturing local dependencies while\nmaintaining a global perspective. Additionally, acknowledging the varying\npreferences for scan patterns across different network layers, we propose a\ndynamic method to independently search for the optimal scan choices for each\nlayer, substantially improving performance. Extensive experiments across both\nplain and hierarchical models underscore our approach's superiority in\neffectively capturing image representations. For example, our model\nsignificantly outperforms Vim-Ti by 3.1% on ImageNet with the same 1.5G FLOPs.\nCode is available at: https://github.com/hunto/LocalMamba.",
        "publication_date": "2024-03-14T12:32:40Z",
        "upvotes": 7
    },
    "2403.09631": {
        "url": "https://arxiv.org/abs/2403.09631",
        "title": "3D-VLA: A 3D Vision-Language-Action Generative World Model",
        "authors": [
            "Haoyu Zhen",
            "Xiaowen Qiu",
            "Peihao Chen",
            "Jincheng Yang",
            "Xin Yan",
            "Yilun Du",
            "Yining Hong",
            "Chuang Gan"
        ],
        "abstract": "Recent vision-language-action (VLA) models rely on 2D inputs, lacking\nintegration with the broader realm of the 3D physical world. Furthermore, they\nperform action prediction by learning a direct mapping from perception to\naction, neglecting the vast dynamics of the world and the relations between\nactions and dynamics. In contrast, human beings are endowed with world models\nthat depict imagination about future scenarios to plan actions accordingly. To\nthis end, we propose 3D-VLA by introducing a new family of embodied foundation\nmodels that seamlessly link 3D perception, reasoning, and action through a\ngenerative world model. Specifically, 3D-VLA is built on top of a 3D-based\nlarge language model (LLM), and a set of interaction tokens is introduced to\nengage with the embodied environment. Furthermore, to inject generation\nabilities into the model, we train a series of embodied diffusion models and\nalign them into the LLM for predicting the goal images and point clouds. To\ntrain our 3D-VLA, we curate a large-scale 3D embodied instruction dataset by\nextracting vast 3D-related information from existing robotics datasets. Our\nexperiments on held-in datasets demonstrate that 3D-VLA significantly improves\nthe reasoning, multimodal generation, and planning capabilities in embodied\nenvironments, showcasing its potential in real-world applications.",
        "publication_date": "2024-03-14T17:58:41Z",
        "upvotes": 6
    },
    "2403.10131": {
        "url": "https://arxiv.org/abs/2403.10131",
        "title": "RAFT: Adapting Language Model to Domain Specific RAG",
        "authors": [
            "Tianjun Zhang",
            "Shishir G. Patil",
            "Naman Jain",
            "Sheng Shen",
            "Matei Zaharia",
            "Ion Stoica",
            "Joseph E. Gonzalez"
        ],
        "abstract": "Pretraining Large Language Models (LLMs) on large corpora of textual data is\nnow a standard paradigm. When using these LLMs for many downstream\napplications, it is common to additionally bake in new knowledge (e.g.,\ntime-critical news, or private domain knowledge) into the pretrained model\neither through RAG-based-prompting, or fine-tuning. However, the optimal\nmethodology for the model to gain such new knowledge remains an open question.\nIn this paper, we present Retrieval Augmented FineTuning (RAFT), a training\nrecipe that improves the model's ability to answer questions in a \"open-book\"\nin-domain settings. In RAFT, given a question, and a set of retrieved\ndocuments, we train the model to ignore those documents that don't help in\nanswering the question, which we call, distractor documents. RAFT accomplishes\nthis by citing verbatim the right sequence from the relevant document that\nwould help answer the question. This coupled with RAFT's chain-of-thought-style\nresponse helps improve the model's ability to reason. In domain-specific RAG,\nRAFT consistently improves the model's performance across PubMed, HotpotQA, and\nGorilla datasets, presenting a post-training recipe to improve pre-trained LLMs\nto in-domain RAG. RAFT's code and demo are open-sourced at\ngithub.com/ShishirPatil/gorilla.",
        "publication_date": "2024-03-15T09:26:02Z",
        "upvotes": 55
    },
    "2403.10301": {
        "url": "https://arxiv.org/abs/2403.10301",
        "title": "Uni-SMART: Universal Science Multimodal Analysis and Research\n  Transformer",
        "authors": [
            "Hengxing Cai",
            "Xiaochen Cai",
            "Shuwen Yang",
            "Jiankun Wang",
            "Lin Yao",
            "Zhifeng Gao",
            "Junhan Chang",
            "Sihang Li",
            "Mingjun Xu",
            "Changxin Wang",
            "Hongshuai Wang",
            "Yongge Li",
            "Mujie Lin",
            "Yaqi Li",
            "Yuqi Yin",
            "Linfeng Zhang",
            "Guolin Ke"
        ],
        "abstract": "In scientific research and its application, scientific literature analysis is\ncrucial as it allows researchers to build on the work of others. However, the\nfast growth of scientific knowledge has led to a massive increase in scholarly\narticles, making in-depth literature analysis increasingly challenging and\ntime-consuming. The emergence of Large Language Models (LLMs) has offered a new\nway to address this challenge. Known for their strong abilities in summarizing\ntexts, LLMs are seen as a potential tool to improve the analysis of scientific\nliterature. However, existing LLMs have their own limits. Scientific literature\noften includes a wide range of multimodal elements, such as molecular\nstructure, tables, and charts, which are hard for text-focused LLMs to\nunderstand and analyze. This issue points to the urgent need for new solutions\nthat can fully understand and analyze multimodal content in scientific\nliterature. To answer this demand, we present Uni-SMART (Universal Science\nMultimodal Analysis and Research Transformer), an innovative model designed for\nin-depth understanding of multimodal scientific literature. Through rigorous\nquantitative evaluation across several domains, Uni-SMART demonstrates superior\nperformance over leading text-focused LLMs. Furthermore, our exploration\nextends to practical applications, including patent infringement detection and\nnuanced analysis of charts. These applications not only highlight Uni-SMART's\nadaptability but also its potential to revolutionize how we interact with\nscientific literature.",
        "publication_date": "2024-03-15T13:43:47Z",
        "upvotes": 49
    },
    "2403.10517": {
        "url": "https://arxiv.org/abs/2403.10517",
        "title": "VideoAgent: Long-form Video Understanding with Large Language Model as\n  Agent",
        "authors": [
            "Xiaohan Wang",
            "Yuhui Zhang",
            "Orr Zohar",
            "Serena Yeung-Levy"
        ],
        "abstract": "Long-form video understanding represents a significant challenge within\ncomputer vision, demanding a model capable of reasoning over long multi-modal\nsequences. Motivated by the human cognitive process for long-form video\nunderstanding, we emphasize interactive reasoning and planning over the ability\nto process lengthy visual inputs. We introduce a novel agent-based system,\nVideoAgent, that employs a large language model as a central agent to\niteratively identify and compile crucial information to answer a question, with\nvision-language foundation models serving as tools to translate and retrieve\nvisual information. Evaluated on the challenging EgoSchema and NExT-QA\nbenchmarks, VideoAgent achieves 54.1% and 71.3% zero-shot accuracy with only\n8.4 and 8.2 frames used on average. These results demonstrate superior\neffectiveness and efficiency of our method over the current state-of-the-art\nmethods, highlighting the potential of agent-based approaches in advancing\nlong-form video understanding.",
        "publication_date": "2024-03-15T17:57:52Z",
        "upvotes": 28
    },
    "2403.09704": {
        "url": "https://arxiv.org/abs/2403.09704",
        "title": "Alignment Studio: Aligning Large Language Models to Particular\n  Contextual Regulations",
        "authors": [
            "Swapnaja Achintalwar",
            "Ioana Baldini",
            "Djallel Bouneffouf",
            "Joan Byamugisha",
            "Maria Chang",
            "Pierre Dognin",
            "Eitan Farchi",
            "Ndivhuwo Makondo",
            "Aleksandra Mojsilovic",
            "Manish Nagireddy",
            "Karthikeyan Natesan Ramamurthy",
            "Inkit Padhi",
            "Orna Raz",
            "Jesus Rios",
            "Prasanna Sattigeri",
            "Moninder Singh",
            "Siphiwe Thwala",
            "Rosario A. Uceda-Sosa",
            "Kush R. Varshney"
        ],
        "abstract": "The alignment of large language models is usually done by model providers to\nadd or control behaviors that are common or universally understood across use\ncases and contexts. In contrast, in this article, we present an approach and\narchitecture that empowers application developers to tune a model to their\nparticular values, social norms, laws and other regulations, and orchestrate\nbetween potentially conflicting requirements in context. We lay out three main\ncomponents of such an Alignment Studio architecture: Framers, Instructors, and\nAuditors that work in concert to control the behavior of a language model. We\nillustrate this approach with a running example of aligning a company's\ninternal-facing enterprise chatbot to its business conduct guidelines.",
        "publication_date": "2024-03-08T21:26:49Z",
        "upvotes": 27
    },
    "2403.09919": {
        "url": "https://arxiv.org/abs/2403.09919",
        "title": "Recurrent Drafter for Fast Speculative Decoding in Large Language Models",
        "authors": [
            "Aonan Zhang",
            "Chong Wang",
            "Yi Wang",
            "Xuanyu Zhang",
            "Yunfei Cheng"
        ],
        "abstract": "In this paper, we introduce an improved approach of speculative decoding\naimed at enhancing the efficiency of serving large language models. Our method\ncapitalizes on the strengths of two established techniques: the classic\ntwo-model speculative decoding approach, and the more recent single-model\napproach, Medusa. Drawing inspiration from Medusa, our approach adopts a\nsingle-model strategy for speculative decoding. However, our method\ndistinguishes itself by employing a single, lightweight draft head with a\nrecurrent dependency design, akin in essence to the small, draft model uses in\nclassic speculative decoding, but without the complexities of the full\ntransformer architecture. And because of the recurrent dependency, we can use\nbeam search to swiftly filter out undesired candidates with the draft head. The\noutcome is a method that combines the simplicity of single-model design and\navoids the need to create a data-dependent tree attention structure only for\ninference in Medusa. We empirically demonstrate the effectiveness of the\nproposed method on several popular open source language models, along with a\ncomprehensive analysis of the trade-offs involved in adopting this approach.",
        "publication_date": "2024-03-14T23:40:56Z",
        "upvotes": 18
    },
    "2403.10493": {
        "url": "https://arxiv.org/abs/2403.10493",
        "title": "MusicHiFi: Fast High-Fidelity Stereo Vocoding",
        "authors": [
            "Ge Zhu",
            "Juan-Pablo Caceres",
            "Zhiyao Duan",
            "Nicholas J. Bryan"
        ],
        "abstract": "Diffusion-based audio and music generation models commonly generate music by\nconstructing an image representation of audio (e.g., a mel-spectrogram) and\nthen converting it to audio using a phase reconstruction model or vocoder.\nTypical vocoders, however, produce monophonic audio at lower resolutions (e.g.,\n16-24 kHz), which limits their effectiveness. We propose MusicHiFi -- an\nefficient high-fidelity stereophonic vocoder. Our method employs a cascade of\nthree generative adversarial networks (GANs) that convert low-resolution\nmel-spectrograms to audio, upsamples to high-resolution audio via bandwidth\nexpansion, and upmixes to stereophonic audio. Compared to previous work, we\npropose 1) a unified GAN-based generator and discriminator architecture and\ntraining procedure for each stage of our cascade, 2) a new fast, near\ndownsampling-compatible bandwidth extension module, and 3) a new fast\ndownmix-compatible mono-to-stereo upmixer that ensures the preservation of\nmonophonic content in the output. We evaluate our approach using both objective\nand subjective listening tests and find our approach yields comparable or\nbetter audio quality, better spatialization control, and significantly faster\ninference speed compared to past work. Sound examples are at\nhttps://MusicHiFi.github.io/web/.",
        "publication_date": "2024-03-15T17:27:42Z",
        "upvotes": 15
    },
    "2403.10242": {
        "url": "https://arxiv.org/abs/2403.10242",
        "title": "FDGaussian: Fast Gaussian Splatting from Single Image via\n  Geometric-aware Diffusion Model",
        "authors": [
            "Qijun Feng",
            "Zhen Xing",
            "Zuxuan Wu",
            "Yu-Gang Jiang"
        ],
        "abstract": "Reconstructing detailed 3D objects from single-view images remains a\nchallenging task due to the limited information available. In this paper, we\nintroduce FDGaussian, a novel two-stage framework for single-image 3D\nreconstruction. Recent methods typically utilize pre-trained 2D diffusion\nmodels to generate plausible novel views from the input image, yet they\nencounter issues with either multi-view inconsistency or lack of geometric\nfidelity. To overcome these challenges, we propose an orthogonal plane\ndecomposition mechanism to extract 3D geometric features from the 2D input,\nenabling the generation of consistent multi-view images. Moreover, we further\naccelerate the state-of-the-art Gaussian Splatting incorporating epipolar\nattention to fuse images from different viewpoints. We demonstrate that\nFDGaussian generates images with high consistency across different views and\nreconstructs high-quality 3D objects, both qualitatively and quantitatively.\nMore examples can be found at our website https://qjfeng.net/FDGaussian/.",
        "publication_date": "2024-03-15T12:24:36Z",
        "upvotes": 9
    },
    "2403.09977": {
        "url": "https://arxiv.org/abs/2403.09977",
        "title": "EfficientVMamba: Atrous Selective Scan for Light Weight Visual Mamba",
        "authors": [
            "Xiaohuan Pei",
            "Tao Huang",
            "Chang Xu"
        ],
        "abstract": "Prior efforts in light-weight model development mainly centered on CNN and\nTransformer-based designs yet faced persistent challenges. CNNs adept at local\nfeature extraction compromise resolution while Transformers offer global reach\nbut escalate computational demands $\\mathcal{O}(N^2)$. This ongoing trade-off\nbetween accuracy and efficiency remains a significant hurdle. Recently, state\nspace models (SSMs), such as Mamba, have shown outstanding performance and\ncompetitiveness in various tasks such as language modeling and computer vision,\nwhile reducing the time complexity of global information extraction to\n$\\mathcal{O}(N)$. Inspired by this, this work proposes to explore the potential\nof visual state space models in light-weight model design and introduce a novel\nefficient model variant dubbed EfficientVMamba. Concretely, our EfficientVMamba\nintegrates a atrous-based selective scan approach by efficient skip sampling,\nconstituting building blocks designed to harness both global and local\nrepresentational features. Additionally, we investigate the integration between\nSSM blocks and convolutions, and introduce an efficient visual state space\nblock combined with an additional convolution branch, which further elevate the\nmodel performance. Experimental results show that, EfficientVMamba scales down\nthe computational complexity while yields competitive results across a variety\nof vision tasks. For example, our EfficientVMamba-S with $1.3$G FLOPs improves\nVim-Ti with $1.5$G FLOPs by a large margin of $5.6\\%$ accuracy on ImageNet.\nCode is available at: \\url{https://github.com/TerryPei/EfficientVMamba}.",
        "publication_date": "2024-03-15T02:48:47Z",
        "upvotes": 7
    },
    "2403.10395": {
        "url": "https://arxiv.org/abs/2403.10395",
        "title": "Isotropic3D: Image-to-3D Generation Based on a Single CLIP Embedding",
        "authors": [
            "Pengkun Liu",
            "Yikai Wang",
            "Fuchun Sun",
            "Jiafang Li",
            "Hang Xiao",
            "Hongxiang Xue",
            "Xinzhou Wang"
        ],
        "abstract": "Encouraged by the growing availability of pre-trained 2D diffusion models,\nimage-to-3D generation by leveraging Score Distillation Sampling (SDS) is\nmaking remarkable progress. Most existing methods combine novel-view lifting\nfrom 2D diffusion models which usually take the reference image as a condition\nwhile applying hard L2 image supervision at the reference view. Yet heavily\nadhering to the image is prone to corrupting the inductive knowledge of the 2D\ndiffusion model leading to flat or distorted 3D generation frequently. In this\nwork, we reexamine image-to-3D in a novel perspective and present Isotropic3D,\nan image-to-3D generation pipeline that takes only an image CLIP embedding as\ninput. Isotropic3D allows the optimization to be isotropic w.r.t. the azimuth\nangle by solely resting on the SDS loss. The core of our framework lies in a\ntwo-stage diffusion model fine-tuning. Firstly, we fine-tune a text-to-3D\ndiffusion model by substituting its text encoder with an image encoder, by\nwhich the model preliminarily acquires image-to-image capabilities. Secondly,\nwe perform fine-tuning using our Explicit Multi-view Attention (EMA) which\ncombines noisy multi-view images with the noise-free reference image as an\nexplicit condition. CLIP embedding is sent to the diffusion model throughout\nthe whole process while reference images are discarded once after fine-tuning.\nAs a result, with a single image CLIP embedding, Isotropic3D is capable of\ngenerating multi-view mutually consistent images and also a 3D model with more\nsymmetrical and neat content, well-proportioned geometry, rich colored texture,\nand less distortion compared with existing image-to-3D methods while still\npreserving the similarity to the reference image to a large extent. The project\npage is available at https://isotropic3d.github.io/. The code and models are\navailable at https://github.com/pkunliu/Isotropic3D.",
        "publication_date": "2024-03-15T15:27:58Z",
        "upvotes": 6
    },
    "2403.09981": {
        "url": "https://arxiv.org/abs/2403.09981",
        "title": "Controllable Text-to-3D Generation via Surface-Aligned Gaussian\n  Splatting",
        "authors": [
            "Zhiqi Li",
            "Yiming Chen",
            "Lingzhe Zhao",
            "Peidong Liu"
        ],
        "abstract": "While text-to-3D and image-to-3D generation tasks have received considerable\nattention, one important but under-explored field between them is controllable\ntext-to-3D generation, which we mainly focus on in this work. To address this\ntask, 1) we introduce Multi-view ControlNet (MVControl), a novel neural network\narchitecture designed to enhance existing pre-trained multi-view diffusion\nmodels by integrating additional input conditions, such as edge, depth, normal,\nand scribble maps. Our innovation lies in the introduction of a conditioning\nmodule that controls the base diffusion model using both local and global\nembeddings, which are computed from the input condition images and camera\nposes. Once trained, MVControl is able to offer 3D diffusion guidance for\noptimization-based 3D generation. And, 2) we propose an efficient multi-stage\n3D generation pipeline that leverages the benefits of recent large\nreconstruction models and score distillation algorithm. Building upon our\nMVControl architecture, we employ a unique hybrid diffusion guidance method to\ndirect the optimization process. In pursuit of efficiency, we adopt 3D\nGaussians as our representation instead of the commonly used implicit\nrepresentations. We also pioneer the use of SuGaR, a hybrid representation that\nbinds Gaussians to mesh triangle faces. This approach alleviates the issue of\npoor geometry in 3D Gaussians and enables the direct sculpting of fine-grained\ngeometry on the mesh. Extensive experiments demonstrate that our method\nachieves robust generalization and enables the controllable generation of\nhigh-quality 3D content.",
        "publication_date": "2024-03-15T02:57:20Z",
        "upvotes": 6
    },
    "2403.10425": {
        "url": "https://arxiv.org/abs/2403.10425",
        "title": "NeuFlow: Real-time, High-accuracy Optical Flow Estimation on Robots\n  Using Edge Devices",
        "authors": [
            "Zhiyong Zhang",
            "Huaizu Jiang",
            "Hanumant Singh"
        ],
        "abstract": "Real-time high-accuracy optical flow estimation is a crucial component in\nvarious applications, including localization and mapping in robotics, object\ntracking, and activity recognition in computer vision. While recent\nlearning-based optical flow methods have achieved high accuracy, they often\ncome with heavy computation costs. In this paper, we propose a highly efficient\noptical flow architecture, called NeuFlow, that addresses both high accuracy\nand computational cost concerns. The architecture follows a global-to-local\nscheme. Given the features of the input images extracted at different spatial\nresolutions, global matching is employed to estimate an initial optical flow on\nthe 1/16 resolution, capturing large displacement, which is then refined on the\n1/8 resolution with lightweight CNN layers for better accuracy. We evaluate our\napproach on Jetson Orin Nano and RTX 2080 to demonstrate efficiency\nimprovements across different computing platforms. We achieve a notable 10x-80x\nspeedup compared to several state-of-the-art methods, while maintaining\ncomparable accuracy. Our approach achieves around 30 FPS on edge computing\nplatforms, which represents a significant breakthrough in deploying complex\ncomputer vision tasks such as SLAM on small robots like drones. The full\ntraining and evaluation code is available at\nhttps://github.com/neufieldrobotics/NeuFlow.",
        "publication_date": "2024-03-15T15:58:51Z",
        "upvotes": 2
    },
    "2403.12015": {
        "url": "https://arxiv.org/abs/2403.12015",
        "title": "Fast High-Resolution Image Synthesis with Latent Adversarial Diffusion\n  Distillation",
        "authors": [
            "Axel Sauer",
            "Frederic Boesel",
            "Tim Dockhorn",
            "Andreas Blattmann",
            "Patrick Esser",
            "Robin Rombach"
        ],
        "abstract": "Diffusion models are the main driver of progress in image and video\nsynthesis, but suffer from slow inference speed. Distillation methods, like the\nrecently introduced adversarial diffusion distillation (ADD) aim to shift the\nmodel from many-shot to single-step inference, albeit at the cost of expensive\nand difficult optimization due to its reliance on a fixed pretrained DINOv2\ndiscriminator. We introduce Latent Adversarial Diffusion Distillation (LADD), a\nnovel distillation approach overcoming the limitations of ADD. In contrast to\npixel-based ADD, LADD utilizes generative features from pretrained latent\ndiffusion models. This approach simplifies training and enhances performance,\nenabling high-resolution multi-aspect ratio image synthesis. We apply LADD to\nStable Diffusion 3 (8B) to obtain SD3-Turbo, a fast model that matches the\nperformance of state-of-the-art text-to-image generators using only four\nunguided sampling steps. Moreover, we systematically investigate its scaling\nbehavior and demonstrate LADD's effectiveness in various applications such as\nimage editing and inpainting.",
        "publication_date": "2024-03-18T17:51:43Z",
        "upvotes": 59
    },
    "2403.10704": {
        "url": "https://arxiv.org/abs/2403.10704",
        "title": "PERL: Parameter Efficient Reinforcement Learning from Human Feedback",
        "authors": [
            "Hakim Sidahmed",
            "Samrat Phatale",
            "Alex Hutcheson",
            "Zhuonan Lin",
            "Zhang Chen",
            "Zac Yu",
            "Jarvis Jin",
            "Roman Komarytsia",
            "Christiane Ahlheim",
            "Yonghao Zhu",
            "Simral Chaudhary",
            "Bowen Li",
            "Saravanan Ganesh",
            "Bill Byrne",
            "Jessica Hoffmann",
            "Hassan Mansoor",
            "Wei Li",
            "Abhinav Rastogi",
            "Lucas Dixon"
        ],
        "abstract": "Reinforcement Learning from Human Feedback (RLHF) has proven to be a strong\nmethod to align Pretrained Large Language Models (LLMs) with human preferences.\nBut training models with RLHF is computationally expensive, and an overall\ncomplex process. In this work, we study RLHF where the underlying models are\ntrained using the parameter efficient method of Low-Rank Adaptation (LoRA)\nintroduced by Hu et al. [2021]. We investigate the setup of \"Parameter\nEfficient Reinforcement Learning\" (PERL), in which we perform reward model\ntraining and reinforcement learning using LoRA. We compare PERL to conventional\nfine-tuning (full-tuning) across various configurations for 7 benchmarks,\nincluding 2 novel datasets, of reward modeling and reinforcement learning. We\nfind that PERL performs on par with the conventional RLHF setting, while\ntraining faster, and with less memory. This enables the high performance of\nRLHF, while reducing the computational burden that limits its adoption as an\nalignment technique for Large Language Models. We also release 2 novel thumbs\nup/down preference datasets: \"Taskmaster Coffee\", and \"Taskmaster Ticketing\" to\npromote research around RLHF.",
        "publication_date": "2024-03-15T21:43:46Z",
        "upvotes": 54
    },
    "2403.11901": {
        "url": "https://arxiv.org/abs/2403.11901",
        "title": "Larimar: Large Language Models with Episodic Memory Control",
        "authors": [
            "Payel Das",
            "Subhajit Chaudhury",
            "Elliot Nelson",
            "Igor Melnyk",
            "Sarath Swaminathan",
            "Sihui Dai",
            "Aur\u00e9lie Lozano",
            "Georgios Kollias",
            "Vijil Chenthamarakshan",
            "Ji\u0159\u00ed",
            "Navr\u00e1til",
            "Soham Dan",
            "Pin-Yu Chen"
        ],
        "abstract": "Efficient and accurate updating of knowledge stored in Large Language Models\n(LLMs) is one of the most pressing research challenges today. This paper\npresents Larimar - a novel, brain-inspired architecture for enhancing LLMs with\na distributed episodic memory. Larimar's memory allows for dynamic, one-shot\nupdates of knowledge without the need for computationally expensive re-training\nor fine-tuning. Experimental results on multiple fact editing benchmarks\ndemonstrate that Larimar attains accuracy comparable to most competitive\nbaselines, even in the challenging sequential editing setup, but also excels in\nspeed - yielding speed-ups of 4-10x depending on the base LLM - as well as\nflexibility due to the proposed architecture being simple, LLM-agnostic, and\nhence general. We further provide mechanisms for selective fact forgetting and\ninput context length generalization with Larimar and show their effectiveness.",
        "publication_date": "2024-03-18T16:01:42Z",
        "upvotes": 30
    },
    "2403.12008": {
        "url": "https://arxiv.org/abs/2403.12008",
        "title": "SV3D: Novel Multi-view Synthesis and 3D Generation from a Single Image\n  using Latent Video Diffusion",
        "authors": [
            "Vikram Voleti",
            "Chun-Han Yao",
            "Mark Boss",
            "Adam Letts",
            "David Pankratz",
            "Dmitry Tochilkin",
            "Christian Laforte",
            "Robin Rombach",
            "Varun Jampani"
        ],
        "abstract": "We present Stable Video 3D (SV3D) -- a latent video diffusion model for\nhigh-resolution, image-to-multi-view generation of orbital videos around a 3D\nobject. Recent work on 3D generation propose techniques to adapt 2D generative\nmodels for novel view synthesis (NVS) and 3D optimization. However, these\nmethods have several disadvantages due to either limited views or inconsistent\nNVS, thereby affecting the performance of 3D object generation. In this work,\nwe propose SV3D that adapts image-to-video diffusion model for novel multi-view\nsynthesis and 3D generation, thereby leveraging the generalization and\nmulti-view consistency of the video models, while further adding explicit\ncamera control for NVS. We also propose improved 3D optimization techniques to\nuse SV3D and its NVS outputs for image-to-3D generation. Extensive experimental\nresults on multiple datasets with 2D and 3D metrics as well as user study\ndemonstrate SV3D's state-of-the-art performance on NVS as well as 3D\nreconstruction compared to prior works.",
        "publication_date": "2024-03-18T17:46:06Z",
        "upvotes": 18
    },
    "2403.11781": {
        "url": "https://arxiv.org/abs/2403.11781",
        "title": "Infinite-ID: Identity-preserved Personalization via ID-semantics\n  Decoupling Paradigm",
        "authors": [
            "Yi Wu",
            "Ziqiang Li",
            "Heliang Zheng",
            "Chaoyue Wang",
            "Bin Li"
        ],
        "abstract": "Drawing on recent advancements in diffusion models for text-to-image\ngeneration, identity-preserved personalization has made significant progress in\naccurately capturing specific identities with just a single reference image.\nHowever, existing methods primarily integrate reference images within the text\nembedding space, leading to a complex entanglement of image and text\ninformation, which poses challenges for preserving both identity fidelity and\nsemantic consistency. To tackle this challenge, we propose Infinite-ID, an\nID-semantics decoupling paradigm for identity-preserved personalization.\nSpecifically, we introduce identity-enhanced training, incorporating an\nadditional image cross-attention module to capture sufficient ID information\nwhile deactivating the original text cross-attention module of the diffusion\nmodel. This ensures that the image stream faithfully represents the identity\nprovided by the reference image while mitigating interference from textual\ninput. Additionally, we introduce a feature interaction mechanism that combines\na mixed attention module with an AdaIN-mean operation to seamlessly merge the\ntwo streams. This mechanism not only enhances the fidelity of identity and\nsemantic consistency but also enables convenient control over the styles of the\ngenerated images. Extensive experimental results on both raw photo generation\nand style image generation demonstrate the superior performance of our proposed\nmethod.",
        "publication_date": "2024-03-18T13:39:53Z",
        "upvotes": 16
    },
    "2403.12032": {
        "url": "https://arxiv.org/abs/2403.12032",
        "title": "Generic 3D Diffusion Adapter Using Controlled Multi-View Editing",
        "authors": [
            "Hansheng Chen",
            "Ruoxi Shi",
            "Yulin Liu",
            "Bokui Shen",
            "Jiayuan Gu",
            "Gordon Wetzstein",
            "Hao Su",
            "Leonidas Guibas"
        ],
        "abstract": "Open-domain 3D object synthesis has been lagging behind image synthesis due\nto limited data and higher computational complexity. To bridge this gap, recent\nworks have investigated multi-view diffusion but often fall short in either 3D\nconsistency, visual quality, or efficiency. This paper proposes MVEdit, which\nfunctions as a 3D counterpart of SDEdit, employing ancestral sampling to\njointly denoise multi-view images and output high-quality textured meshes.\nBuilt on off-the-shelf 2D diffusion models, MVEdit achieves 3D consistency\nthrough a training-free 3D Adapter, which lifts the 2D views of the last\ntimestep into a coherent 3D representation, then conditions the 2D views of the\nnext timestep using rendered views, without uncompromising visual quality. With\nan inference time of only 2-5 minutes, this framework achieves better trade-off\nbetween quality and speed than score distillation. MVEdit is highly versatile\nand extendable, with a wide range of applications including text/image-to-3D\ngeneration, 3D-to-3D editing, and high-quality texture synthesis. In\nparticular, evaluations demonstrate state-of-the-art performance in both\nimage-to-3D and text-guided texture generation tasks. Additionally, we\nintroduce a method for fine-tuning 2D latent diffusion models on small 3D\ndatasets with limited resources, enabling fast low-resolution text-to-3D\ninitialization.",
        "publication_date": "2024-03-18T17:59:09Z",
        "upvotes": 14
    },
    "2403.11703": {
        "url": "https://arxiv.org/abs/2403.11703",
        "title": "LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images",
        "authors": [
            "Ruyi Xu",
            "Yuan Yao",
            "Zonghao Guo",
            "Junbo Cui",
            "Zanlin Ni",
            "Chunjiang Ge",
            "Tat-Seng Chua",
            "Zhiyuan Liu",
            "Maosong Sun",
            "Gao Huang"
        ],
        "abstract": "Visual encoding constitutes the basis of large multimodal models (LMMs) in\nunderstanding the visual world. Conventional LMMs process images in fixed sizes\nand limited resolutions, while recent explorations in this direction are\nlimited in adaptivity, efficiency, and even correctness. In this work, we first\ntake GPT-4V and LLaVA-1.5 as representative examples and expose systematic\nflaws rooted in their visual encoding strategy. To address the challenges, we\npresent LLaVA-UHD, a large multimodal model that can efficiently perceive\nimages in any aspect ratio and high resolution. LLaVA-UHD includes three key\ncomponents: (1) An image modularization strategy that divides native-resolution\nimages into smaller variable-sized slices for efficient and extensible\nencoding, (2) a compression module that further condenses image tokens from\nvisual encoders, and (3) a spatial schema to organize slice tokens for LLMs.\nComprehensive experiments show that LLaVA-UHD outperforms established LMMs\ntrained with 2-3 orders of magnitude more data on 9 benchmarks. Notably, our\nmodel built on LLaVA-1.5 336x336 supports 6 times larger (i.e., 672x1088)\nresolution images using only 94% inference computation, and achieves 6.4\naccuracy improvement on TextVQA. Moreover, the model can be efficiently trained\nin academic settings, within 23 hours on 8 A100 GPUs (vs. 26 hours of\nLLaVA-1.5). We make the data and code publicly available at\nhttps://github.com/thunlp/LLaVA-UHD.",
        "publication_date": "2024-03-18T12:04:11Z",
        "upvotes": 13
    },
    "2403.11207": {
        "url": "https://arxiv.org/abs/2403.11207",
        "title": "MindEye2: Shared-Subject Models Enable fMRI-To-Image With 1 Hour of Data",
        "authors": [
            "Paul S. Scotti",
            "Mihir Tripathy",
            "Cesar Kadir Torrico Villanueva",
            "Reese Kneeland",
            "Tong Chen",
            "Ashutosh Narang",
            "Charan Santhirasegaran",
            "Jonathan Xu",
            "Thomas Naselaris",
            "Kenneth A. Norman",
            "Tanishq Mathew Abraham"
        ],
        "abstract": "Reconstructions of visual perception from brain activity have improved\ntremendously, but the practical utility of such methods has been limited. This\nis because such models are trained independently per subject where each subject\nrequires dozens of hours of expensive fMRI training data to attain high-quality\nresults. The present work showcases high-quality reconstructions using only 1\nhour of fMRI training data. We pretrain our model across 7 subjects and then\nfine-tune on minimal data from a new subject. Our novel functional alignment\nprocedure linearly maps all brain data to a shared-subject latent space,\nfollowed by a shared non-linear mapping to CLIP image space. We then map from\nCLIP space to pixel space by fine-tuning Stable Diffusion XL to accept CLIP\nlatents as inputs instead of text. This approach improves out-of-subject\ngeneralization with limited training data and also attains state-of-the-art\nimage retrieval and reconstruction metrics compared to single-subject\napproaches. MindEye2 demonstrates how accurate reconstructions of perception\nare possible from a single visit to the MRI facility. All code is available on\nGitHub.",
        "publication_date": "2024-03-17T13:15:22Z",
        "upvotes": 13
    },
    "2403.10615": {
        "url": "https://arxiv.org/abs/2403.10615",
        "title": "LightIt: Illumination Modeling and Control for Diffusion Models",
        "authors": [
            "Peter Kocsis",
            "Julien Philip",
            "Kalyan Sunkavalli",
            "Matthias Nie\u00dfner",
            "Yannick Hold-Geoffroy"
        ],
        "abstract": "We introduce LightIt, a method for explicit illumination control for image\ngeneration. Recent generative methods lack lighting control, which is crucial\nto numerous artistic aspects of image generation such as setting the overall\nmood or cinematic appearance. To overcome these limitations, we propose to\ncondition the generation on shading and normal maps. We model the lighting with\nsingle bounce shading, which includes cast shadows. We first train a shading\nestimation module to generate a dataset of real-world images and shading pairs.\nThen, we train a control network using the estimated shading and normals as\ninput. Our method demonstrates high-quality image generation and lighting\ncontrol in numerous scenes. Additionally, we use our generated dataset to train\nan identity-preserving relighting model, conditioned on an image and a target\nshading. Our method is the first that enables the generation of images with\ncontrollable, consistent lighting and performs on par with specialized\nrelighting state-of-the-art methods.",
        "publication_date": "2024-03-15T18:26:33Z",
        "upvotes": 13
    },
    "2403.10616": {
        "url": "https://arxiv.org/abs/2403.10616",
        "title": "DiPaCo: Distributed Path Composition",
        "authors": [
            "Arthur Douillard",
            "Qixuan Feng",
            "Andrei A. Rusu",
            "Adhiguna Kuncoro",
            "Yani Donchev",
            "Rachita Chhaparia",
            "Ionel Gog",
            "Marc'Aurelio Ranzato",
            "Jiajun Shen",
            "Arthur Szlam"
        ],
        "abstract": "Progress in machine learning (ML) has been fueled by scaling neural network\nmodels. This scaling has been enabled by ever more heroic feats of engineering,\nnecessary for accommodating ML approaches that require high bandwidth\ncommunication between devices working in parallel. In this work, we propose a\nco-designed modular architecture and training approach for ML models, dubbed\nDIstributed PAth COmposition (DiPaCo). During training, DiPaCo distributes\ncomputation by paths through a set of shared modules. Together with a Local-SGD\ninspired optimization (DiLoCo) that keeps modules in sync with drastically\nreduced communication, Our approach facilitates training across poorly\nconnected and heterogeneous workers, with a design that ensures robustness to\nworker failures and preemptions. At inference time, only a single path needs to\nbe executed for each input, without the need for any model compression. We\nconsider this approach as a first prototype towards a new paradigm of\nlarge-scale learning, one that is less synchronous and more modular. Our\nexperiments on the widely used C4 benchmark show that, for the same amount of\ntraining steps but less wall-clock time, DiPaCo exceeds the performance of a 1\nbillion-parameter dense transformer language model by choosing one of 256\npossible paths, each with a size of 150 million parameters.",
        "publication_date": "2024-03-15T18:26:51Z",
        "upvotes": 10
    },
    "2403.11481": {
        "url": "https://arxiv.org/abs/2403.11481",
        "title": "VideoAgent: A Memory-augmented Multimodal Agent for Video Understanding",
        "authors": [
            "Yue Fan",
            "Xiaojian Ma",
            "Rujie Wu",
            "Yuntao Du",
            "Jiaqi Li",
            "Zhi Gao",
            "Qing Li"
        ],
        "abstract": "We explore how reconciling several foundation models (large language models\nand vision-language models) with a novel unified memory mechanism could tackle\nthe challenging video understanding problem, especially capturing the long-term\ntemporal relations in lengthy videos. In particular, the proposed multimodal\nagent VideoAgent: 1) constructs a structured memory to store both the generic\ntemporal event descriptions and object-centric tracking states of the video; 2)\ngiven an input task query, it employs tools including video segment\nlocalization and object memory querying along with other visual foundation\nmodels to interactively solve the task, utilizing the zero-shot tool-use\nability of LLMs. VideoAgent demonstrates impressive performances on several\nlong-horizon video understanding benchmarks, an average increase of 6.6% on\nNExT-QA and 26.0% on EgoSchema over baselines, closing the gap between\nopen-sourced models and private counterparts including Gemini 1.5 Pro.",
        "publication_date": "2024-03-18T05:07:59Z",
        "upvotes": 10
    },
    "2403.12019": {
        "url": "https://arxiv.org/abs/2403.12019",
        "title": "LN3Diff: Scalable Latent Neural Fields Diffusion for Speedy 3D\n  Generation",
        "authors": [
            "Yushi Lan",
            "Fangzhou Hong",
            "Shuai Yang",
            "Shangchen Zhou",
            "Xuyi Meng",
            "Bo Dai",
            "Xingang Pan",
            "Chen Change Loy"
        ],
        "abstract": "The field of neural rendering has witnessed significant progress with\nadvancements in generative models and differentiable rendering techniques.\nThough 2D diffusion has achieved success, a unified 3D diffusion pipeline\nremains unsettled. This paper introduces a novel framework called LN3Diff to\naddress this gap and enable fast, high-quality, and generic conditional 3D\ngeneration. Our approach harnesses a 3D-aware architecture and variational\nautoencoder (VAE) to encode the input image into a structured, compact, and 3D\nlatent space. The latent is decoded by a transformer-based decoder into a\nhigh-capacity 3D neural field. Through training a diffusion model on this\n3D-aware latent space, our method achieves state-of-the-art performance on\nShapeNet for 3D generation and demonstrates superior performance in monocular\n3D reconstruction and conditional 3D generation across various datasets.\nMoreover, it surpasses existing 3D diffusion methods in terms of inference\nspeed, requiring no per-instance optimization. Our proposed LN3Diff presents a\nsignificant advancement in 3D generative modeling and holds promise for various\napplications in 3D vision and graphics tasks.",
        "publication_date": "2024-03-18T17:54:34Z",
        "upvotes": 8
    },
    "2403.12034": {
        "url": "https://arxiv.org/abs/2403.12034",
        "title": "VFusion3D: Learning Scalable 3D Generative Models from Video Diffusion\n  Models",
        "authors": [
            "Junlin Han",
            "Filippos Kokkinos",
            "Philip Torr"
        ],
        "abstract": "This paper presents a novel paradigm for building scalable 3D generative\nmodels utilizing pre-trained video diffusion models. The primary obstacle in\ndeveloping foundation 3D generative models is the limited availability of 3D\ndata. Unlike images, texts, or videos, 3D data are not readily accessible and\nare difficult to acquire. This results in a significant disparity in scale\ncompared to the vast quantities of other types of data. To address this issue,\nwe propose using a video diffusion model, trained with extensive volumes of\ntext, images, and videos, as a knowledge source for 3D data. By unlocking its\nmulti-view generative capabilities through fine-tuning, we generate a\nlarge-scale synthetic multi-view dataset to train a feed-forward 3D generative\nmodel. The proposed model, VFusion3D, trained on nearly 3M synthetic multi-view\ndata, can generate a 3D asset from a single image in seconds and achieves\nsuperior performance when compared to current SOTA feed-forward 3D generative\nmodels, with users preferring our results over 70% of the time.",
        "publication_date": "2024-03-18T17:59:12Z",
        "upvotes": 5
    },
    "2403.12895": {
        "url": "https://arxiv.org/abs/2403.12895",
        "title": "mPLUG-DocOwl 1.5: Unified Structure Learning for OCR-free Document\n  Understanding",
        "authors": [
            "Anwen Hu",
            "Haiyang Xu",
            "Jiabo Ye",
            "Ming Yan",
            "Liang Zhang",
            "Bo Zhang",
            "Chen Li",
            "Ji Zhang",
            "Qin Jin",
            "Fei Huang",
            "Jingren Zhou"
        ],
        "abstract": "Structure information is critical for understanding the semantics of\ntext-rich images, such as documents, tables, and charts. Existing Multimodal\nLarge Language Models (MLLMs) for Visual Document Understanding are equipped\nwith text recognition ability but lack general structure understanding\nabilities for text-rich document images. In this work, we emphasize the\nimportance of structure information in Visual Document Understanding and\npropose the Unified Structure Learning to boost the performance of MLLMs. Our\nUnified Structure Learning comprises structure-aware parsing tasks and\nmulti-grained text localization tasks across 5 domains: document, webpage,\ntable, chart, and natural image. To better encode structure information, we\ndesign a simple and effective vision-to-text module H-Reducer, which can not\nonly maintain the layout information but also reduce the length of visual\nfeatures by merging horizontal adjacent patches through convolution, enabling\nthe LLM to understand high-resolution images more efficiently. Furthermore, by\nconstructing structure-aware text sequences and multi-grained pairs of texts\nand bounding boxes for publicly available text-rich images, we build a\ncomprehensive training set DocStruct4M to support structure learning. Finally,\nwe construct a small but high-quality reasoning tuning dataset DocReason25K to\ntrigger the detailed explanation ability in the document domain. Our model\nDocOwl 1.5 achieves state-of-the-art performance on 10 visual document\nunderstanding benchmarks, improving the SOTA performance of MLLMs with a 7B LLM\nby more than 10 points in 5/10 benchmarks. Our codes, models, and datasets are\npublicly available at\nhttps://github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl1.5.",
        "publication_date": "2024-03-19T16:48:40Z",
        "upvotes": 26
    },
    "2403.12968": {
        "url": "https://arxiv.org/abs/2403.12968",
        "title": "LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic\n  Prompt Compression",
        "authors": [
            "Zhuoshi Pan",
            "Qianhui Wu",
            "Huiqiang Jiang",
            "Menglin Xia",
            "Xufang Luo",
            "Jue Zhang",
            "Qingwei Lin",
            "Victor R\u00fchle",
            "Yuqing Yang",
            "Chin-Yew Lin",
            "H. Vicky Zhao",
            "Lili Qiu",
            "Dongmei Zhang"
        ],
        "abstract": "This paper focuses on task-agnostic prompt compression for better\ngeneralizability and efficiency. Considering the redundancy in natural\nlanguage, existing approaches compress prompts by removing tokens or lexical\nunits according to their information entropy obtained from a causal language\nmodel such as LLaMa-7B. The challenge is that information entropy may be a\nsuboptimal compression metric: (i) it only leverages unidirectional context and\nmay fail to capture all essential information needed for prompt compression;\n(ii) it is not aligned with the prompt compression objective.\n  To address these issues, we propose a data distillation procedure to derive\nknowledge from an LLM to compress prompts without losing crucial information,\nand meantime, introduce an extractive text compression dataset. We formulate\nprompt compression as a token classification problem to guarantee the\nfaithfulness of the compressed prompt to the original one, and use a\nTransformer encoder as the base architecture to capture all essential\ninformation for prompt compression from the full bidirectional context. Our\napproach leads to lower latency by explicitly learning the compression\nobjective with smaller models such as XLM-RoBERTa-large and mBERT.\n  We evaluate our method on both in-domain and out-of-domain datasets,\nincluding MeetingBank, LongBench, ZeroScrolls, GSM8K, and BBH. Despite its\nsmall size, our model shows significant performance gains over strong baselines\nand demonstrates robust generalization ability across different LLMs.\nAdditionally, our model is 3x-6x faster than existing prompt compression\nmethods, while accelerating the end-to-end latency by 1.6x-2.9x with\ncompression ratios of 2x-5x.",
        "publication_date": "2024-03-19T17:59:56Z",
        "upvotes": 20
    },
    "2403.12706": {
        "url": "https://arxiv.org/abs/2403.12706",
        "title": "AnimateDiff-Lightning: Cross-Model Diffusion Distillation",
        "authors": [
            "Shanchuan Lin",
            "Xiao Yang"
        ],
        "abstract": "We present AnimateDiff-Lightning for lightning-fast video generation. Our\nmodel uses progressive adversarial diffusion distillation to achieve new\nstate-of-the-art in few-step video generation. We discuss our modifications to\nadapt it for the video modality. Furthermore, we propose to simultaneously\ndistill the probability flow of multiple base diffusion models, resulting in a\nsingle distilled motion module with broader style compatibility. We are pleased\nto release our distilled AnimateDiff-Lightning model for the community's use.",
        "publication_date": "2024-03-19T13:08:54Z",
        "upvotes": 17
    },
    "2403.12173": {
        "url": "https://arxiv.org/abs/2403.12173",
        "title": "TnT-LLM: Text Mining at Scale with Large Language Models",
        "authors": [
            "Mengting Wan",
            "Tara Safavi",
            "Sujay Kumar Jauhar",
            "Yujin Kim",
            "Scott Counts",
            "Jennifer Neville",
            "Siddharth Suri",
            "Chirag Shah",
            "Ryen W White",
            "Longqi Yang",
            "Reid Andersen",
            "Georg Buscher",
            "Dhruv Joshi",
            "Nagu Rangan"
        ],
        "abstract": "Transforming unstructured text into structured and meaningful forms,\norganized by useful category labels, is a fundamental step in text mining for\ndownstream analysis and application. However, most existing methods for\nproducing label taxonomies and building text-based label classifiers still rely\nheavily on domain expertise and manual curation, making the process expensive\nand time-consuming. This is particularly challenging when the label space is\nunder-specified and large-scale data annotations are unavailable. In this\npaper, we address these challenges with Large Language Models (LLMs), whose\nprompt-based interface facilitates the induction and use of large-scale pseudo\nlabels. We propose TnT-LLM, a two-phase framework that employs LLMs to automate\nthe process of end-to-end label generation and assignment with minimal human\neffort for any given use-case. In the first phase, we introduce a zero-shot,\nmulti-stage reasoning approach which enables LLMs to produce and refine a label\ntaxonomy iteratively. In the second phase, LLMs are used as data labelers that\nyield training samples so that lightweight supervised classifiers can be\nreliably built, deployed, and served at scale. We apply TnT-LLM to the analysis\nof user intent and conversational domain for Bing Copilot (formerly Bing Chat),\nan open-domain chat-based search engine. Extensive experiments using both human\nand automatic evaluation metrics demonstrate that TnT-LLM generates more\naccurate and relevant label taxonomies when compared against state-of-the-art\nbaselines, and achieves a favorable balance between accuracy and efficiency for\nclassification at scale. We also share our practical experiences and insights\non the challenges and opportunities of using LLMs for large-scale text mining\nin real-world applications.",
        "publication_date": "2024-03-18T18:45:28Z",
        "upvotes": 15
    },
    "2403.12881": {
        "url": "https://arxiv.org/abs/2403.12881",
        "title": "Agent-FLAN: Designing Data and Methods of Effective Agent Tuning for\n  Large Language Models",
        "authors": [
            "Zehui Chen",
            "Kuikun Liu",
            "Qiuchen Wang",
            "Wenwei Zhang",
            "Jiangning Liu",
            "Dahua Lin",
            "Kai Chen",
            "Feng Zhao"
        ],
        "abstract": "Open-sourced Large Language Models (LLMs) have achieved great success in\nvarious NLP tasks, however, they are still far inferior to API-based models\nwhen acting as agents. How to integrate agent ability into general LLMs becomes\na crucial and urgent problem. This paper first delivers three key observations:\n(1) the current agent training corpus is entangled with both formats following\nand agent reasoning, which significantly shifts from the distribution of its\npre-training data; (2) LLMs exhibit different learning speeds on the\ncapabilities required by agent tasks; and (3) current approaches have\nside-effects when improving agent abilities by introducing hallucinations.\nBased on the above findings, we propose Agent-FLAN to effectively Fine-tune\nLANguage models for Agents. Through careful decomposition and redesign of the\ntraining corpus, Agent-FLAN enables Llama2-7B to outperform prior best works by\n3.5\\% across various agent evaluation datasets. With comprehensively\nconstructed negative samples, Agent-FLAN greatly alleviates the hallucination\nissues based on our established evaluation benchmark. Besides, it consistently\nimproves the agent capability of LLMs when scaling model sizes while slightly\nenhancing the general capability of LLMs. The code will be available at\nhttps://github.com/InternLM/Agent-FLAN.",
        "publication_date": "2024-03-19T16:26:10Z",
        "upvotes": 13
    },
    "2403.12943": {
        "url": "https://arxiv.org/abs/2403.12943",
        "title": "Vid2Robot: End-to-end Video-conditioned Policy Learning with\n  Cross-Attention Transformers",
        "authors": [
            "Vidhi Jain",
            "Maria Attarian",
            "Nikhil J Joshi",
            "Ayzaan Wahid",
            "Danny Driess",
            "Quan Vuong",
            "Pannag R Sanketi",
            "Pierre Sermanet",
            "Stefan Welker",
            "Christine Chan",
            "Igor Gilitschenski",
            "Yonatan Bisk",
            "Debidatta Dwibedi"
        ],
        "abstract": "While large-scale robotic systems typically rely on textual instructions for\ntasks, this work explores a different approach: can robots infer the task\ndirectly from observing humans? This shift necessitates the robot's ability to\ndecode human intent and translate it into executable actions within its\nphysical constraints and environment. We introduce Vid2Robot, a novel\nend-to-end video-based learning framework for robots. Given a video\ndemonstration of a manipulation task and current visual observations, Vid2Robot\ndirectly produces robot actions. This is achieved through a unified\nrepresentation model trained on a large dataset of human video and robot\ntrajectory. The model leverages cross-attention mechanisms to fuse prompt video\nfeatures to the robot's current state and generate appropriate actions that\nmimic the observed task. To further improve policy performance, we propose\nauxiliary contrastive losses that enhance the alignment between human and robot\nvideo representations. We evaluate Vid2Robot on real-world robots,\ndemonstrating a 20% improvement in performance compared to other\nvideo-conditioned policies when using human demonstration videos. Additionally,\nour model exhibits emergent capabilities, such as successfully transferring\nobserved motions from one object to another, and long-horizon composition, thus\nshowcasing its potential for real-world applications. Project website:\nvid2robot.github.io",
        "publication_date": "2024-03-19T17:47:37Z",
        "upvotes": 13
    },
    "2403.12365": {
        "url": "https://arxiv.org/abs/2403.12365",
        "title": "GaussianFlow: Splatting Gaussian Dynamics for 4D Content Creation",
        "authors": [
            "Quankai Gao",
            "Qiangeng Xu",
            "Zhe Cao",
            "Ben Mildenhall",
            "Wenchao Ma",
            "Le Chen",
            "Danhang Tang",
            "Ulrich Neumann"
        ],
        "abstract": "Creating 4D fields of Gaussian Splatting from images or videos is a\nchallenging task due to its under-constrained nature. While the optimization\ncan draw photometric reference from the input videos or be regulated by\ngenerative models, directly supervising Gaussian motions remains underexplored.\nIn this paper, we introduce a novel concept, Gaussian flow, which connects the\ndynamics of 3D Gaussians and pixel velocities between consecutive frames. The\nGaussian flow can be efficiently obtained by splatting Gaussian dynamics into\nthe image space. This differentiable process enables direct dynamic supervision\nfrom optical flow. Our method significantly benefits 4D dynamic content\ngeneration and 4D novel view synthesis with Gaussian Splatting, especially for\ncontents with rich motions that are hard to be handled by existing methods. The\ncommon color drifting issue that happens in 4D generation is also resolved with\nimproved Guassian dynamics. Superior visual quality on extensive experiments\ndemonstrates our method's effectiveness. Quantitative and qualitative\nevaluations show that our method achieves state-of-the-art results on both\ntasks of 4D generation and 4D novel view synthesis. Project page:\nhttps://zerg-overmind.github.io/GaussianFlow.github.io/",
        "publication_date": "2024-03-19T02:22:21Z",
        "upvotes": 9
    },
    "2403.12596": {
        "url": "https://arxiv.org/abs/2403.12596",
        "title": "Chart-based Reasoning: Transferring Capabilities from LLMs to VLMs",
        "authors": [
            "Victor Carbune",
            "Hassan Mansoor",
            "Fangyu Liu",
            "Rahul Aralikatte",
            "Gilles Baechler",
            "Jindong Chen",
            "Abhanshu Sharma"
        ],
        "abstract": "Vision-language models (VLMs) are achieving increasingly strong performance\non multimodal tasks. However, reasoning capabilities remain limited\nparticularly for smaller VLMs, while those of large-language models (LLMs) have\nseen numerous improvements. We propose a technique to transfer capabilities\nfrom LLMs to VLMs. On the recently introduced ChartQA, our method obtains\nstate-of-the-art performance when applied on the PaLI3-5B VLM by\n\\citet{chen2023pali3}, while also enabling much better performance on PlotQA\nand FigureQA.\n  We first improve the chart representation by continuing the pre-training\nstage using an improved version of the chart-to-table translation task by\n\\citet{liu2023deplot}. We then propose constructing a 20x larger dataset than\nthe original training set. To improve general reasoning capabilities and\nimprove numerical operations, we synthesize reasoning traces using the table\nrepresentation of charts. Lastly, our model is fine-tuned using the multitask\nloss introduced by \\citet{hsieh2023distilling}.\n  Our variant ChartPaLI-5B outperforms even 10x larger models such as PaLIX-55B\nwithout using an upstream OCR system, while keeping inference time constant\ncompared to the PaLI3-5B baseline. When rationales are further refined with a\nsimple program-of-thought prompt \\cite{chen2023program}, our model outperforms\nthe recently introduced Gemini Ultra and GPT-4V.",
        "publication_date": "2024-03-19T10:03:07Z",
        "upvotes": 9
    },
    "2403.12409": {
        "url": "https://arxiv.org/abs/2403.12409",
        "title": "ComboVerse: Compositional 3D Assets Creation Using Spatially-Aware\n  Diffusion Guidance",
        "authors": [
            "Yongwei Chen",
            "Tengfei Wang",
            "Tong Wu",
            "Xingang Pan",
            "Kui Jia",
            "Ziwei Liu"
        ],
        "abstract": "Generating high-quality 3D assets from a given image is highly desirable in\nvarious applications such as AR/VR. Recent advances in single-image 3D\ngeneration explore feed-forward models that learn to infer the 3D model of an\nobject without optimization. Though promising results have been achieved in\nsingle object generation, these methods often struggle to model complex 3D\nassets that inherently contain multiple objects. In this work, we present\nComboVerse, a 3D generation framework that produces high-quality 3D assets with\ncomplex compositions by learning to combine multiple models. 1) We first\nperform an in-depth analysis of this ``multi-object gap'' from both model and\ndata perspectives. 2) Next, with reconstructed 3D models of different objects,\nwe seek to adjust their sizes, rotation angles, and locations to create a 3D\nasset that matches the given image. 3) To automate this process, we apply\nspatially-aware score distillation sampling (SSDS) from pretrained diffusion\nmodels to guide the positioning of objects. Our proposed framework emphasizes\nspatial alignment of objects, compared with standard score distillation\nsampling, and thus achieves more accurate results. Extensive experiments\nvalidate ComboVerse achieves clear improvements over existing methods in\ngenerating compositional 3D assets.",
        "publication_date": "2024-03-19T03:39:43Z",
        "upvotes": 8
    },
    "2403.12962": {
        "url": "https://arxiv.org/abs/2403.12962",
        "title": "FRESCO: Spatial-Temporal Correspondence for Zero-Shot Video Translation",
        "authors": [
            "Shuai Yang",
            "Yifan Zhou",
            "Ziwei Liu",
            "Chen Change Loy"
        ],
        "abstract": "The remarkable efficacy of text-to-image diffusion models has motivated\nextensive exploration of their potential application in video domains.\nZero-shot methods seek to extend image diffusion models to videos without\nnecessitating model training. Recent methods mainly focus on incorporating\ninter-frame correspondence into attention mechanisms. However, the soft\nconstraint imposed on determining where to attend to valid features can\nsometimes be insufficient, resulting in temporal inconsistency. In this paper,\nwe introduce FRESCO, intra-frame correspondence alongside inter-frame\ncorrespondence to establish a more robust spatial-temporal constraint. This\nenhancement ensures a more consistent transformation of semantically similar\ncontent across frames. Beyond mere attention guidance, our approach involves an\nexplicit update of features to achieve high spatial-temporal consistency with\nthe input video, significantly improving the visual coherence of the resulting\ntranslated videos. Extensive experiments demonstrate the effectiveness of our\nproposed framework in producing high-quality, coherent videos, marking a\nnotable improvement over existing zero-shot methods.",
        "publication_date": "2024-03-19T17:59:18Z",
        "upvotes": 7
    },
    "2403.12963": {
        "url": "https://arxiv.org/abs/2403.12963",
        "title": "FouriScale: A Frequency Perspective on Training-Free High-Resolution\n  Image Synthesis",
        "authors": [
            "Linjiang Huang",
            "Rongyao Fang",
            "Aiping Zhang",
            "Guanglu Song",
            "Si Liu",
            "Yu Liu",
            "Hongsheng Li"
        ],
        "abstract": "In this study, we delve into the generation of high-resolution images from\npre-trained diffusion models, addressing persistent challenges, such as\nrepetitive patterns and structural distortions, that emerge when models are\napplied beyond their trained resolutions. To address this issue, we introduce\nan innovative, training-free approach FouriScale from the perspective of\nfrequency domain analysis. We replace the original convolutional layers in\npre-trained diffusion models by incorporating a dilation technique along with a\nlow-pass operation, intending to achieve structural consistency and scale\nconsistency across resolutions, respectively. Further enhanced by a\npadding-then-crop strategy, our method can flexibly handle text-to-image\ngeneration of various aspect ratios. By using the FouriScale as guidance, our\nmethod successfully balances the structural integrity and fidelity of generated\nimages, achieving an astonishing capacity of arbitrary-size, high-resolution,\nand high-quality generation. With its simplicity and compatibility, our method\ncan provide valuable insights for future explorations into the synthesis of\nultra-high-resolution images. The code will be released at\nhttps://github.com/LeonHLJ/FouriScale.",
        "publication_date": "2024-03-19T17:59:33Z",
        "upvotes": 6
    },
    "2403.12906": {
        "url": "https://arxiv.org/abs/2403.12906",
        "title": "TexDreamer: Towards Zero-Shot High-Fidelity 3D Human Texture Generation",
        "authors": [
            "Yufei Liu",
            "Junwei Zhu",
            "Junshu Tang",
            "Shijie Zhang",
            "Jiangning Zhang",
            "Weijian Cao",
            "Chengjie Wang",
            "Yunsheng Wu",
            "Dongjin Huang"
        ],
        "abstract": "Texturing 3D humans with semantic UV maps remains a challenge due to the\ndifficulty of acquiring reasonably unfolded UV. Despite recent text-to-3D\nadvancements in supervising multi-view renderings using large text-to-image\n(T2I) models, issues persist with generation speed, text consistency, and\ntexture quality, resulting in data scarcity among existing datasets. We present\nTexDreamer, the first zero-shot multimodal high-fidelity 3D human texture\ngeneration model. Utilizing an efficient texture adaptation finetuning\nstrategy, we adapt large T2I model to a semantic UV structure while preserving\nits original generalization capability. Leveraging a novel feature translator\nmodule, the trained model is capable of generating high-fidelity 3D human\ntextures from either text or image within seconds. Furthermore, we introduce\nArTicuLated humAn textureS (ATLAS), the largest high-resolution (1024 X 1024)\n3D human texture dataset which contains 50k high-fidelity textures with text\ndescriptions.",
        "publication_date": "2024-03-19T17:02:07Z",
        "upvotes": 4
    },
    "2403.12957": {
        "url": "https://arxiv.org/abs/2403.12957",
        "title": "GVGEN: Text-to-3D Generation with Volumetric Representation",
        "authors": [
            "Xianglong He",
            "Junyi Chen",
            "Sida Peng",
            "Di Huang",
            "Yangguang Li",
            "Xiaoshui Huang",
            "Chun Yuan",
            "Wanli Ouyang",
            "Tong He"
        ],
        "abstract": "In recent years, 3D Gaussian splatting has emerged as a powerful technique\nfor 3D reconstruction and generation, known for its fast and high-quality\nrendering capabilities. To address these shortcomings, this paper introduces a\nnovel diffusion-based framework, GVGEN, designed to efficiently generate 3D\nGaussian representations from text input. We propose two innovative\ntechniques:(1) Structured Volumetric Representation. We first arrange\ndisorganized 3D Gaussian points as a structured form GaussianVolume. This\ntransformation allows the capture of intricate texture details within a volume\ncomposed of a fixed number of Gaussians. To better optimize the representation\nof these details, we propose a unique pruning and densifying method named the\nCandidate Pool Strategy, enhancing detail fidelity through selective\noptimization. (2) Coarse-to-fine Generation Pipeline. To simplify the\ngeneration of GaussianVolume and empower the model to generate instances with\ndetailed 3D geometry, we propose a coarse-to-fine pipeline. It initially\nconstructs a basic geometric structure, followed by the prediction of complete\nGaussian attributes. Our framework, GVGEN, demonstrates superior performance in\nqualitative and quantitative assessments compared to existing 3D generation\nmethods. Simultaneously, it maintains a fast generation speed ($\\sim$7\nseconds), effectively striking a balance between quality and efficiency.",
        "publication_date": "2024-03-19T17:57:52Z",
        "upvotes": 4
    },
    "2403.13248": {
        "url": "https://arxiv.org/abs/2403.13248",
        "title": "Mora: Enabling Generalist Video Generation via A Multi-Agent Framework",
        "authors": [
            "Zhengqing Yuan",
            "Ruoxi Chen",
            "Zhaoxu Li",
            "Haolong Jia",
            "Lifang He",
            "Chi Wang",
            "Lichao Sun"
        ],
        "abstract": "Sora is the first large-scale generalist video generation model that garnered\nsignificant attention across society. Since its launch by OpenAI in February\n2024, no other video generation models have paralleled {Sora}'s performance or\nits capacity to support a broad spectrum of video generation tasks.\nAdditionally, there are only a few fully published video generation models,\nwith the majority being closed-source. To address this gap, this paper proposes\na new multi-agent framework Mora, which incorporates several advanced visual AI\nagents to replicate generalist video generation demonstrated by Sora. In\nparticular, Mora can utilize multiple visual agents and successfully mimic\nSora's video generation capabilities in various tasks, such as (1)\ntext-to-video generation, (2) text-conditional image-to-video generation, (3)\nextend generated videos, (4) video-to-video editing, (5) connect videos and (6)\nsimulate digital worlds. Our extensive experimental results show that Mora\nachieves performance that is proximate to that of Sora in various tasks.\nHowever, there exists an obvious performance gap between our work and Sora when\nassessed holistically. In summary, we hope this project can guide the future\ntrajectory of video generation through collaborative AI agents.",
        "publication_date": "2024-03-20T02:19:21Z",
        "upvotes": 70
    },
    "2403.13372": {
        "url": "https://arxiv.org/abs/2403.13372",
        "title": "LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models",
        "authors": [
            "Yaowei Zheng",
            "Richong Zhang",
            "Junhao Zhang",
            "Yanhan Ye",
            "Zheyan Luo",
            "Yongqiang Ma"
        ],
        "abstract": "Efficient fine-tuning is vital for adapting large language models (LLMs) to\ndownstream tasks. However, it requires non-trivial efforts to implement these\nmethods on different models. We present LlamaFactory, a unified framework that\nintegrates a suite of cutting-edge efficient training methods. It allows users\nto flexibly customize the fine-tuning of 100+ LLMs without the need for coding\nthrough the built-in web UI LlamaBoard. We empirically validate the efficiency\nand effectiveness of our framework on language modeling and text generation\ntasks. It has been released at https://github.com/hiyouga/LLaMA-Factory and\nalready received over 13,000 stars and 1,600 forks.",
        "publication_date": "2024-03-20T08:08:54Z",
        "upvotes": 48
    },
    "2403.13187": {
        "url": "https://arxiv.org/abs/2403.13187",
        "title": "Evolutionary Optimization of Model Merging Recipes",
        "authors": [
            "Takuya Akiba",
            "Makoto Shing",
            "Yujin Tang",
            "Qi Sun",
            "David Ha"
        ],
        "abstract": "We present a novel application of evolutionary algorithms to automate the\ncreation of powerful foundation models. While model merging has emerged as a\npromising approach for LLM development due to its cost-effectiveness, it\ncurrently relies on human intuition and domain knowledge, limiting its\npotential. Here, we propose an evolutionary approach that overcomes this\nlimitation by automatically discovering effective combinations of diverse\nopen-source models, harnessing their collective intelligence without requiring\nextensive additional training data or compute. Our approach operates in both\nparameter space and data flow space, allowing for optimization beyond just the\nweights of the individual models. This approach even facilitates cross-domain\nmerging, generating models like a Japanese LLM with Math reasoning\ncapabilities. Surprisingly, our Japanese Math LLM achieved state-of-the-art\nperformance on a variety of established Japanese LLM benchmarks, even\nsurpassing models with significantly more parameters, despite not being\nexplicitly trained for such tasks. Furthermore, a culturally-aware Japanese VLM\ngenerated through our approach demonstrates its effectiveness in describing\nJapanese culture-specific content, outperforming previous Japanese VLMs. This\nwork not only contributes new state-of-the-art models back to the open-source\ncommunity, but also introduces a new paradigm for automated model composition,\npaving the way for exploring alternative, efficient approaches to foundation\nmodel development.",
        "publication_date": "2024-03-19T22:56:53Z",
        "upvotes": 44
    },
    "2403.13064": {
        "url": "https://arxiv.org/abs/2403.13064",
        "title": "SceneScript: Reconstructing Scenes With An Autoregressive Structured\n  Language Model",
        "authors": [
            "Armen Avetisyan",
            "Christopher Xie",
            "Henry Howard-Jenkins",
            "Tsun-Yi Yang",
            "Samir Aroudj",
            "Suvam Patra",
            "Fuyang Zhang",
            "Duncan Frost",
            "Luke Holland",
            "Campbell Orme",
            "Jakob Engel",
            "Edward Miller",
            "Richard Newcombe",
            "Vasileios Balntas"
        ],
        "abstract": "We introduce SceneScript, a method that directly produces full scene models\nas a sequence of structured language commands using an autoregressive,\ntoken-based approach. Our proposed scene representation is inspired by recent\nsuccesses in transformers & LLMs, and departs from more traditional methods\nwhich commonly describe scenes as meshes, voxel grids, point clouds or radiance\nfields. Our method infers the set of structured language commands directly from\nencoded visual data using a scene language encoder-decoder architecture. To\ntrain SceneScript, we generate and release a large-scale synthetic dataset\ncalled Aria Synthetic Environments consisting of 100k high-quality in-door\nscenes, with photorealistic and ground-truth annotated renders of egocentric\nscene walkthroughs. Our method gives state-of-the art results in architectural\nlayout estimation, and competitive results in 3D object detection. Lastly, we\nexplore an advantage for SceneScript, which is the ability to readily adapt to\nnew commands via simple additions to the structured language, which we\nillustrate for tasks such as coarse 3D object part reconstruction.",
        "publication_date": "2024-03-19T18:01:29Z",
        "upvotes": 30
    },
    "2403.13043": {
        "url": "https://arxiv.org/abs/2403.13043",
        "title": "When Do We Not Need Larger Vision Models?",
        "authors": [
            "Baifeng Shi",
            "Ziyang Wu",
            "Maolin Mao",
            "Xin Wang",
            "Trevor Darrell"
        ],
        "abstract": "Scaling up the size of vision models has been the de facto standard to obtain\nmore powerful visual representations. In this work, we discuss the point beyond\nwhich larger vision models are not necessary. First, we demonstrate the power\nof Scaling on Scales (S$^2$), whereby a pre-trained and frozen smaller vision\nmodel (e.g., ViT-B or ViT-L), run over multiple image scales, can outperform\nlarger models (e.g., ViT-H or ViT-G) on classification, segmentation, depth\nestimation, Multimodal LLM (MLLM) benchmarks, and robotic manipulation.\nNotably, S$^2$ achieves state-of-the-art performance in detailed understanding\nof MLLM on the V* benchmark, surpassing models such as GPT-4V. We examine the\nconditions under which S$^2$ is a preferred scaling approach compared to\nscaling on model size. While larger models have the advantage of better\ngeneralization on hard examples, we show that features of larger vision models\ncan be well approximated by those of multi-scale smaller models. This suggests\nmost, if not all, of the representations learned by current large pre-trained\nmodels can also be obtained from multi-scale smaller models. Our results show\nthat a multi-scale smaller model has comparable learning capacity to a larger\nmodel, and pre-training smaller models with S$^2$ can match or even exceed the\nadvantage of larger models. We release a Python package that can apply S$^2$ on\nany vision model with one line of code:\nhttps://github.com/bfshi/scaling_on_scales.",
        "publication_date": "2024-03-19T17:58:39Z",
        "upvotes": 23
    },
    "2403.13535": {
        "url": "https://arxiv.org/abs/2403.13535",
        "title": "IDAdapter: Learning Mixed Features for Tuning-Free Personalization of\n  Text-to-Image Models",
        "authors": [
            "Siying Cui",
            "Jia Guo",
            "Xiang An",
            "Jiankang Deng",
            "Yongle Zhao",
            "Xinyu Wei",
            "Ziyong Feng"
        ],
        "abstract": "Leveraging Stable Diffusion for the generation of personalized portraits has\nemerged as a powerful and noteworthy tool, enabling users to create\nhigh-fidelity, custom character avatars based on their specific prompts.\nHowever, existing personalization methods face challenges, including test-time\nfine-tuning, the requirement of multiple input images, low preservation of\nidentity, and limited diversity in generated outcomes. To overcome these\nchallenges, we introduce IDAdapter, a tuning-free approach that enhances the\ndiversity and identity preservation in personalized image generation from a\nsingle face image. IDAdapter integrates a personalized concept into the\ngeneration process through a combination of textual and visual injections and a\nface identity loss. During the training phase, we incorporate mixed features\nfrom multiple reference images of a specific identity to enrich\nidentity-related content details, guiding the model to generate images with\nmore diverse styles, expressions, and angles compared to previous works.\nExtensive evaluations demonstrate the effectiveness of our method, achieving\nboth diversity and identity fidelity in generated images.",
        "publication_date": "2024-03-20T12:13:04Z",
        "upvotes": 20
    },
    "2403.13806": {
        "url": "https://arxiv.org/abs/2403.13806",
        "title": "RadSplat: Radiance Field-Informed Gaussian Splatting for Robust\n  Real-Time Rendering with 900+ FPS",
        "authors": [
            "Michael Niemeyer",
            "Fabian Manhardt",
            "Marie-Julie Rakotosaona",
            "Michael Oechsle",
            "Daniel Duckworth",
            "Rama Gosula",
            "Keisuke Tateno",
            "John Bates",
            "Dominik Kaeser",
            "Federico Tombari"
        ],
        "abstract": "Recent advances in view synthesis and real-time rendering have achieved\nphotorealistic quality at impressive rendering speeds. While Radiance\nField-based methods achieve state-of-the-art quality in challenging scenarios\nsuch as in-the-wild captures and large-scale scenes, they often suffer from\nexcessively high compute requirements linked to volumetric rendering. Gaussian\nSplatting-based methods, on the other hand, rely on rasterization and naturally\nachieve real-time rendering but suffer from brittle optimization heuristics\nthat underperform on more challenging scenes. In this work, we present\nRadSplat, a lightweight method for robust real-time rendering of complex\nscenes. Our main contributions are threefold. First, we use radiance fields as\na prior and supervision signal for optimizing point-based scene\nrepresentations, leading to improved quality and more robust optimization.\nNext, we develop a novel pruning technique reducing the overall point count\nwhile maintaining high quality, leading to smaller and more compact scene\nrepresentations with faster inference speeds. Finally, we propose a novel\ntest-time filtering approach that further accelerates rendering and allows to\nscale to larger, house-sized scenes. We find that our method enables\nstate-of-the-art synthesis of complex captures at 900+ FPS.",
        "publication_date": "2024-03-20T17:59:55Z",
        "upvotes": 18
    },
    "2403.13787": {
        "url": "https://arxiv.org/abs/2403.13787",
        "title": "RewardBench: Evaluating Reward Models for Language Modeling",
        "authors": [
            "Nathan Lambert",
            "Valentina Pyatkin",
            "Jacob Morrison",
            "LJ Miranda",
            "Bill Yuchen Lin",
            "Khyathi Chandu",
            "Nouha Dziri",
            "Sachin Kumar",
            "Tom Zick",
            "Yejin Choi",
            "Noah A. Smith",
            "Hannaneh Hajishirzi"
        ],
        "abstract": "Reward models (RMs) are at the crux of successful RLHF to align pretrained\nmodels to human preferences, yet there has been relatively little study that\nfocuses on evaluation of those reward models. Evaluating reward models presents\nan opportunity to understand the opaque technologies used for alignment of\nlanguage models and which values are embedded in them. To date, very few\ndescriptors of capabilities, training methods, or open-source reward models\nexist. In this paper, we present RewardBench, a benchmark dataset and code-base\nfor evaluation, to enhance scientific understanding of reward models. The\nRewardBench dataset is a collection of prompt-win-lose trios spanning chat,\nreasoning, and safety, to benchmark how reward models perform on challenging,\nstructured and out-of-distribution queries. We created specific comparison\ndatasets for RMs that have subtle, but verifiable reasons (e.g. bugs, incorrect\nfacts) why one answer should be preferred to another. On the RewardBench\nleaderboard, we evaluate reward models trained with a variety of methods, such\nas the direct MLE training of classifiers and the implicit reward modeling of\nDirect Preference Optimization (DPO), and on a spectrum of datasets. We present\nmany findings on propensity for refusals, reasoning limitations, and\ninstruction following shortcomings of various reward models towards a better\nunderstanding of the RLHF process.",
        "publication_date": "2024-03-20T17:49:54Z",
        "upvotes": 17
    },
    "2403.13447": {
        "url": "https://arxiv.org/abs/2403.13447",
        "title": "HyperLLaVA: Dynamic Visual and Language Expert Tuning for Multimodal\n  Large Language Models",
        "authors": [
            "Wenqiao Zhang",
            "Tianwei Lin",
            "Jiang Liu",
            "Fangxun Shu",
            "Haoyuan Li",
            "Lei Zhang",
            "He Wanggui",
            "Hao Zhou",
            "Zheqi Lv",
            "Hao Jiang",
            "Juncheng Li",
            "Siliang Tang",
            "Yueting Zhuang"
        ],
        "abstract": "Recent advancements indicate that scaling up Multimodal Large Language Models\n(MLLMs) effectively enhances performance on downstream multimodal tasks. The\nprevailing MLLM paradigm, \\emph{e.g.}, LLaVA, transforms visual features into\ntext-like tokens using a \\emph{static} vision-language mapper, thereby enabling\n\\emph{static} LLMs to develop the capability to comprehend visual information\nthrough visual instruction tuning. Although promising, the \\emph{static} tuning\nstrategy~\\footnote{The static tuning refers to the trained model with static\nparameters.} that shares the same parameters may constrain performance across\ndifferent downstream multimodal tasks. In light of this, we introduce\nHyperLLaVA, which involves adaptive tuning of the projector and LLM parameters,\nin conjunction with a dynamic visual expert and language expert, respectively.\nThese experts are derived from HyperNetworks, which generates adaptive\nparameter shifts through visual and language guidance, enabling dynamic\nprojector and LLM modeling in two-stage training.\n  Our experiments demonstrate that our solution significantly surpasses LLaVA\non existing MLLM benchmarks, including MME, MMBench, SEED-Bench, and\nLLaVA-Bench. ~\\footnote{Our project is available on the link\nhttps://github.com/DCDmllm/HyperLLaVA}.",
        "publication_date": "2024-03-20T09:42:43Z",
        "upvotes": 16
    },
    "2403.13802": {
        "url": "https://arxiv.org/abs/2403.13802",
        "title": "ZigMa: A DiT-style Zigzag Mamba Diffusion Model",
        "authors": [
            "Vincent Tao Hu",
            "Stefan Andreas Baumann",
            "Ming Gui",
            "Olga Grebenkova",
            "Pingchuan Ma",
            "Johannes Fischer",
            "Bj\u00f6rn Ommer"
        ],
        "abstract": "The diffusion model has long been plagued by scalability and quadratic\ncomplexity issues, especially within transformer-based structures. In this\nstudy, we aim to leverage the long sequence modeling capability of a\nState-Space Model called Mamba to extend its applicability to visual data\ngeneration. Firstly, we identify a critical oversight in most current\nMamba-based vision methods, namely the lack of consideration for spatial\ncontinuity in the scan scheme of Mamba. Secondly, building upon this insight,\nwe introduce a simple, plug-and-play, zero-parameter method named Zigzag Mamba,\nwhich outperforms Mamba-based baselines and demonstrates improved speed and\nmemory utilization compared to transformer-based baselines. Lastly, we\nintegrate Zigzag Mamba with the Stochastic Interpolant framework to investigate\nthe scalability of the model on large-resolution visual datasets, such as\nFacesHQ $1024\\times 1024$ and UCF101, MultiModal-CelebA-HQ, and MS COCO\n$256\\times 256$ . Code will be released at https://taohu.me/zigma/",
        "publication_date": "2024-03-20T17:59:14Z",
        "upvotes": 14
    },
    "2403.13788": {
        "url": "https://arxiv.org/abs/2403.13788",
        "title": "DepthFM: Fast Monocular Depth Estimation with Flow Matching",
        "authors": [
            "Ming Gui",
            "Johannes S. Fischer",
            "Ulrich Prestel",
            "Pingchuan Ma",
            "Dmytro Kotovenko",
            "Olga Grebenkova",
            "Stefan Andreas Baumann",
            "Vincent Tao Hu",
            "Bj\u00f6rn Ommer"
        ],
        "abstract": "Monocular depth estimation is crucial for numerous downstream vision tasks\nand applications. Current discriminative approaches to this problem are limited\ndue to blurry artifacts, while state-of-the-art generative methods suffer from\nslow sampling due to their SDE nature. Rather than starting from noise, we seek\na direct mapping from input image to depth map. We observe that this can be\neffectively framed using flow matching, since its straight trajectories through\nsolution space offer efficiency and high quality. Our study demonstrates that a\npre-trained image diffusion model can serve as an adequate prior for a flow\nmatching depth model, allowing efficient training on only synthetic data to\ngeneralize to real images. We find that an auxiliary surface normals loss\nfurther improves the depth estimates. Due to the generative nature of our\napproach, our model reliably predicts the confidence of its depth estimates. On\nstandard benchmarks of complex natural scenes, our lightweight approach\nexhibits state-of-the-art performance at favorable low computational cost\ndespite only being trained on little synthetic data.",
        "publication_date": "2024-03-20T17:51:53Z",
        "upvotes": 13
    },
    "2403.13044": {
        "url": "https://arxiv.org/abs/2403.13044",
        "title": "Magic Fixup: Streamlining Photo Editing by Watching Dynamic Videos",
        "authors": [
            "Hadi Alzayer",
            "Zhihao Xia",
            "Xuaner Zhang",
            "Eli Shechtman",
            "Jia-Bin Huang",
            "Michael Gharbi"
        ],
        "abstract": "We propose a generative model that, given a coarsely edited image,\nsynthesizes a photorealistic output that follows the prescribed layout. Our\nmethod transfers fine details from the original image and preserves the\nidentity of its parts. Yet, it adapts it to the lighting and context defined by\nthe new layout. Our key insight is that videos are a powerful source of\nsupervision for this task: objects and camera motions provide many observations\nof how the world changes with viewpoint, lighting, and physical interactions.\nWe construct an image dataset in which each sample is a pair of source and\ntarget frames extracted from the same video at randomly chosen time intervals.\nWe warp the source frame toward the target using two motion models that mimic\nthe expected test-time user edits. We supervise our model to translate the\nwarped image into the ground truth, starting from a pretrained diffusion model.\nOur model design explicitly enables fine detail transfer from the source frame\nto the generated image, while closely following the user-specified layout. We\nshow that by using simple segmentations and coarse 2D manipulations, we can\nsynthesize a photorealistic edit faithful to the user's input while addressing\nsecond-order effects like harmonizing the lighting and physical interactions\nbetween edited objects.",
        "publication_date": "2024-03-19T17:59:58Z",
        "upvotes": 13
    },
    "2403.13745": {
        "url": "https://arxiv.org/abs/2403.13745",
        "title": "Be-Your-Outpainter: Mastering Video Outpainting through Input-Specific\n  Adaptation",
        "authors": [
            "Fu-Yun Wang",
            "Xiaoshi Wu",
            "Zhaoyang Huang",
            "Xiaoyu Shi",
            "Dazhong Shen",
            "Guanglu Song",
            "Yu Liu",
            "Hongsheng Li"
        ],
        "abstract": "Video outpainting is a challenging task, aiming at generating video content\noutside the viewport of the input video while maintaining inter-frame and\nintra-frame consistency. Existing methods fall short in either generation\nquality or flexibility. We introduce MOTIA Mastering Video Outpainting Through\nInput-Specific Adaptation, a diffusion-based pipeline that leverages both the\nintrinsic data-specific patterns of the source video and the image/video\ngenerative prior for effective outpainting. MOTIA comprises two main phases:\ninput-specific adaptation and pattern-aware outpainting. The input-specific\nadaptation phase involves conducting efficient and effective pseudo outpainting\nlearning on the single-shot source video. This process encourages the model to\nidentify and learn patterns within the source video, as well as bridging the\ngap between standard generative processes and outpainting. The subsequent\nphase, pattern-aware outpainting, is dedicated to the generalization of these\nlearned patterns to generate outpainting outcomes. Additional strategies\nincluding spatial-aware insertion and noise travel are proposed to better\nleverage the diffusion model's generative prior and the acquired video patterns\nfrom source videos. Extensive evaluations underscore MOTIA's superiority,\noutperforming existing state-of-the-art methods in widely recognized\nbenchmarks. Notably, these advancements are achieved without necessitating\nextensive, task-specific tuning.",
        "publication_date": "2024-03-20T16:53:45Z",
        "upvotes": 10
    },
    "2403.13799": {
        "url": "https://arxiv.org/abs/2403.13799",
        "title": "Reverse Training to Nurse the Reversal Curse",
        "authors": [
            "Olga Golovneva",
            "Zeyuan Allen-Zhu",
            "Jason Weston",
            "Sainbayar Sukhbaatar"
        ],
        "abstract": "Large language models (LLMs) have a surprising failure: when trained on \"A\nhas a feature B\", they do not generalize to \"B is a feature of A\", which is\ntermed the Reversal Curse. Even when training with trillions of tokens this\nissue still appears due to Zipf's law - hence even if we train on the entire\ninternet. This work proposes an alternative training scheme, called reverse\ntraining, whereby all words are used twice, doubling the amount of available\ntokens. The LLM is trained in both forward and reverse directions by reversing\nthe training strings while preserving (i.e., not reversing) chosen substrings,\nsuch as entities. We show that data-matched reverse-trained models provide\nsuperior performance to standard models on standard tasks, and compute-matched\nreverse-trained models provide far superior performance on reversal tasks,\nhelping resolve the reversal curse issue.",
        "publication_date": "2024-03-20T17:55:35Z",
        "upvotes": 10
    },
    "2403.13501": {
        "url": "https://arxiv.org/abs/2403.13501",
        "title": "VSTAR: Generative Temporal Nursing for Longer Dynamic Video Synthesis",
        "authors": [
            "Yumeng Li",
            "William Beluch",
            "Margret Keuper",
            "Dan Zhang",
            "Anna Khoreva"
        ],
        "abstract": "Despite tremendous progress in the field of text-to-video (T2V) synthesis,\nopen-sourced T2V diffusion models struggle to generate longer videos with\ndynamically varying and evolving content. They tend to synthesize quasi-static\nvideos, ignoring the necessary visual change-over-time implied in the text\nprompt. At the same time, scaling these models to enable longer, more dynamic\nvideo synthesis often remains computationally intractable. To address this\nchallenge, we introduce the concept of Generative Temporal Nursing (GTN), where\nwe aim to alter the generative process on the fly during inference to improve\ncontrol over the temporal dynamics and enable generation of longer videos. We\npropose a method for GTN, dubbed VSTAR, which consists of two key ingredients:\n1) Video Synopsis Prompting (VSP) - automatic generation of a video synopsis\nbased on the original single prompt leveraging LLMs, which gives accurate\ntextual guidance to different visual states of longer videos, and 2) Temporal\nAttention Regularization (TAR) - a regularization technique to refine the\ntemporal attention units of the pre-trained T2V diffusion models, which enables\ncontrol over the video dynamics. We experimentally showcase the superiority of\nthe proposed approach in generating longer, visually appealing videos over\nexisting open-sourced T2V models. We additionally analyze the temporal\nattention maps realized with and without VSTAR, demonstrating the importance of\napplying our method to mitigate neglect of the desired visual change over time.",
        "publication_date": "2024-03-20T10:58:58Z",
        "upvotes": 9
    },
    "2401.13923": {
        "url": "https://arxiv.org/abs/2401.13923",
        "title": "Towards 3D Molecule-Text Interpretation in Language Models",
        "authors": [
            "Sihang Li",
            "Zhiyuan Liu",
            "Yanchen Luo",
            "Xiang Wang",
            "Xiangnan He",
            "Kenji Kawaguchi",
            "Tat-Seng Chua",
            "Qi Tian"
        ],
        "abstract": "Language Models (LMs) have greatly influenced diverse domains. However, their\ninherent limitation in comprehending 3D molecular structures has considerably\nconstrained their potential in the biomolecular domain. To bridge this gap, we\nfocus on 3D molecule-text interpretation, and propose 3D-MoLM: 3D-Molecular\nLanguage Modeling. Specifically, 3D-MoLM enables an LM to interpret and analyze\n3D molecules by equipping the LM with a 3D molecular encoder. This integration\nis achieved by a 3D molecule-text projector, bridging the 3D molecular\nencoder's representation space and the LM's input space. Moreover, to enhance\n3D-MoLM's ability of cross-modal molecular understanding and instruction\nfollowing, we meticulously curated a 3D molecule-centric instruction tuning\ndataset -- 3D-MoIT. Through 3D molecule-text alignment and 3D molecule-centric\ninstruction tuning, 3D-MoLM establishes an integration of 3D molecular encoder\nand LM. It significantly surpasses existing baselines on downstream tasks,\nincluding molecule-text retrieval, molecule captioning, and more challenging\nopen-text molecular QA tasks, especially focusing on 3D-dependent properties.\nWe release our codes and datasets at https://github.com/lsh0520/3D-MoLM.",
        "publication_date": "2024-01-25T03:42:00Z",
        "upvotes": 9
    },
    "2403.13524": {
        "url": "https://arxiv.org/abs/2403.13524",
        "title": "Compress3D: a Compressed Latent Space for 3D Generation from a Single\n  Image",
        "authors": [
            "Bowen Zhang",
            "Tianyu Yang",
            "Yu Li",
            "Lei Zhang",
            "Xi Zhao"
        ],
        "abstract": "3D generation has witnessed significant advancements, yet efficiently\nproducing high-quality 3D assets from a single image remains challenging. In\nthis paper, we present a triplane autoencoder, which encodes 3D models into a\ncompact triplane latent space to effectively compress both the 3D geometry and\ntexture information. Within the autoencoder framework, we introduce a 3D-aware\ncross-attention mechanism, which utilizes low-resolution latent representations\nto query features from a high-resolution 3D feature volume, thereby enhancing\nthe representation capacity of the latent space. Subsequently, we train a\ndiffusion model on this refined latent space. In contrast to solely relying on\nimage embedding for 3D generation, our proposed method advocates for the\nsimultaneous utilization of both image embedding and shape embedding as\nconditions. Specifically, the shape embedding is estimated via a diffusion\nprior model conditioned on the image embedding. Through comprehensive\nexperiments, we demonstrate that our method outperforms state-of-the-art\nalgorithms, achieving superior performance while requiring less training data\nand time. Our approach enables the generation of high-quality 3D assets in\nmerely 7 seconds on a single A100 GPU.",
        "publication_date": "2024-03-20T11:51:04Z",
        "upvotes": 8
    },
    "2403.13793": {
        "url": "https://arxiv.org/abs/2403.13793",
        "title": "Evaluating Frontier Models for Dangerous Capabilities",
        "authors": [
            "Mary Phuong",
            "Matthew Aitchison",
            "Elliot Catt",
            "Sarah Cogan",
            "Alexandre Kaskasoli",
            "Victoria Krakovna",
            "David Lindner",
            "Matthew Rahtz",
            "Yannis Assael",
            "Sarah Hodkinson",
            "Heidi Howard",
            "Tom Lieberum",
            "Ramana Kumar",
            "Maria Abi Raad",
            "Albert Webson",
            "Lewis Ho",
            "Sharon Lin",
            "Sebastian Farquhar",
            "Marcus Hutter",
            "Gregoire Deletang",
            "Anian Ruoss",
            "Seliem El-Sayed",
            "Sasha Brown",
            "Anca Dragan",
            "Rohin Shah",
            "Allan Dafoe",
            "Toby Shevlane"
        ],
        "abstract": "To understand the risks posed by a new AI system, we must understand what it\ncan and cannot do. Building on prior work, we introduce a programme of new\n\"dangerous capability\" evaluations and pilot them on Gemini 1.0 models. Our\nevaluations cover four areas: (1) persuasion and deception; (2) cyber-security;\n(3) self-proliferation; and (4) self-reasoning. We do not find evidence of\nstrong dangerous capabilities in the models we evaluated, but we flag early\nwarning signs. Our goal is to help advance a rigorous science of dangerous\ncapability evaluation, in preparation for future models.",
        "publication_date": "2024-03-20T17:54:26Z",
        "upvotes": 7
    },
    "2403.14624": {
        "url": "https://arxiv.org/abs/2403.14624",
        "title": "MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual\n  Math Problems?",
        "authors": [
            "Renrui Zhang",
            "Dongzhi Jiang",
            "Yichi Zhang",
            "Haokun Lin",
            "Ziyu Guo",
            "Pengshuo Qiu",
            "Aojun Zhou",
            "Pan Lu",
            "Kai-Wei Chang",
            "Peng Gao",
            "Hongsheng Li"
        ],
        "abstract": "The remarkable progress of Multi-modal Large Language Models (MLLMs) has\ngarnered unparalleled attention, due to their superior performance in visual\ncontexts. However, their capabilities in visual math problem-solving remain\ninsufficiently evaluated and understood. We investigate current benchmarks to\nincorporate excessive visual content within textual questions, which\npotentially assist MLLMs in deducing answers without truly interpreting the\ninput diagrams. To this end, we introduce MathVerse, an all-around visual math\nbenchmark designed for an equitable and in-depth evaluation of MLLMs. We\nmeticulously collect 2,612 high-quality, multi-subject math problems with\ndiagrams from publicly available sources. Each problem is then transformed by\nhuman annotators into six distinct versions, each offering varying degrees of\ninformation content in multi-modality, contributing to 15K test samples in\ntotal. This approach allows MathVerse to comprehensively assess whether and how\nmuch MLLMs can truly understand the visual diagrams for mathematical reasoning.\nIn addition, we propose a Chain-of-Thought (CoT) evaluation strategy for a\nfine-grained assessment of the output answers. Rather than naively judging True\nor False, we employ GPT-4(V) to adaptively extract crucial reasoning steps, and\nthen score each step with detailed error analysis, which can reveal the\nintermediate CoT reasoning quality by MLLMs. We hope the MathVerse benchmark\nmay provide unique insights to guide the future development of MLLMs. Project\npage: https://mathverse-cuhk.github.io",
        "publication_date": "2024-03-21T17:59:50Z",
        "upvotes": 45
    },
    "2403.14613": {
        "url": "https://arxiv.org/abs/2403.14613",
        "title": "DreamReward: Text-to-3D Generation with Human Preference",
        "authors": [
            "Junliang Ye",
            "Fangfu Liu",
            "Qixiu Li",
            "Zhengyi Wang",
            "Yikai Wang",
            "Xinzhou Wang",
            "Yueqi Duan",
            "Jun Zhu"
        ],
        "abstract": "3D content creation from text prompts has shown remarkable success recently.\nHowever, current text-to-3D methods often generate 3D results that do not align\nwell with human preferences. In this paper, we present a comprehensive\nframework, coined DreamReward, to learn and improve text-to-3D models from\nhuman preference feedback. To begin with, we collect 25k expert comparisons\nbased on a systematic annotation pipeline including rating and ranking. Then,\nwe build Reward3D -- the first general-purpose text-to-3D human preference\nreward model to effectively encode human preferences. Building upon the 3D\nreward model, we finally perform theoretical analysis and present the Reward3D\nFeedback Learning (DreamFL), a direct tuning algorithm to optimize the\nmulti-view diffusion models with a redefined scorer. Grounded by theoretical\nproof and extensive experiment comparisons, our DreamReward successfully\ngenerates high-fidelity and 3D consistent results with significant boosts in\nprompt alignment with human intention. Our results demonstrate the great\npotential for learning from human feedback to improve text-to-3D models.",
        "publication_date": "2024-03-21T17:58:04Z",
        "upvotes": 32
    },
    "2403.14520": {
        "url": "https://arxiv.org/abs/2403.14520",
        "title": "Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient\n  Inference",
        "authors": [
            "Han Zhao",
            "Min Zhang",
            "Wei Zhao",
            "Pengxiang Ding",
            "Siteng Huang",
            "Donglin Wang"
        ],
        "abstract": "In recent years, the application of multimodal large language models (MLLM)\nin various fields has achieved remarkable success. However, as the foundation\nmodel for many downstream tasks, current MLLMs are composed of the well-known\nTransformer network, which has a less efficient quadratic computation\ncomplexity. To improve the efficiency of such basic models, we propose Cobra, a\nlinear computational complexity MLLM. Specifically, Cobra integrates the\nefficient Mamba language model into the visual modality. Moreover, we explore\nand study various modal fusion schemes to create an effective multi-modal\nMamba. Extensive experiments demonstrate that (1) Cobra achieves extremely\ncompetitive performance with current computationally efficient state-of-the-art\nmethods, e.g., LLaVA-Phi, TinyLLaVA, and MobileVLM v2, and has faster speed due\nto Cobra's linear sequential modeling. (2) Interestingly, the results of\nclosed-set challenging prediction benchmarks show that Cobra performs well in\novercoming visual illusions and spatial relationship judgments. (3) Notably,\nCobra even achieves comparable performance to LLaVA with about 43% of the\nnumber of parameters. We will make all codes of Cobra open-source and hope that\nthe proposed method can facilitate future research on complexity problems in\nMLLM. Our project page is available at: https://sites.google.com/view/cobravlm.",
        "publication_date": "2024-03-21T16:17:57Z",
        "upvotes": 31
    },
    "2403.14468": {
        "url": "https://arxiv.org/abs/2403.14468",
        "title": "AnyV2V: A Plug-and-Play Framework For Any Video-to-Video Editing Tasks",
        "authors": [
            "Max Ku",
            "Cong Wei",
            "Weiming Ren",
            "Harry Yang",
            "Wenhu Chen"
        ],
        "abstract": "Video-to-video editing involves editing a source video along with additional\ncontrol (such as text prompts, subjects, or styles) to generate a new video\nthat aligns with the source video and the provided control. Traditional methods\nhave been constrained to certain editing types, limiting their ability to meet\nthe wide range of user demands. In this paper, we introduce AnyV2V, a novel\ntraining-free framework designed to simplify video editing into two primary\nsteps: (1) employing an off-the-shelf image editing model (e.g.\nInstructPix2Pix, InstantID, etc) to modify the first frame, (2) utilizing an\nexisting image-to-video generation model (e.g. I2VGen-XL) for DDIM inversion\nand feature injection. In the first stage, AnyV2V can plug in any existing\nimage editing tools to support an extensive array of video editing tasks.\nBeyond the traditional prompt-based editing methods, AnyV2V also can support\nnovel video editing tasks, including reference-based style transfer,\nsubject-driven editing, and identity manipulation, which were unattainable by\nprevious methods. In the second stage, AnyV2V can plug in any existing\nimage-to-video models to perform DDIM inversion and intermediate feature\ninjection to maintain the appearance and motion consistency with the source\nvideo. On the prompt-based editing, we show that AnyV2V can outperform the\nprevious best approach by 35\\% on prompt alignment, and 25\\% on human\npreference. On the three novel tasks, we show that AnyV2V also achieves a high\nsuccess rate. We believe AnyV2V will continue to thrive due to its ability to\nseamlessly integrate the fast-evolving image editing methods. Such\ncompatibility can help AnyV2V to increase its versatility to cater to diverse\nuser demands.",
        "publication_date": "2024-03-21T15:15:00Z",
        "upvotes": 18
    },
    "2403.14602": {
        "url": "https://arxiv.org/abs/2403.14602",
        "title": "ReNoise: Real Image Inversion Through Iterative Noising",
        "authors": [
            "Daniel Garibi",
            "Or Patashnik",
            "Andrey Voynov",
            "Hadar Averbuch-Elor",
            "Daniel Cohen-Or"
        ],
        "abstract": "Recent advancements in text-guided diffusion models have unlocked powerful\nimage manipulation capabilities. However, applying these methods to real images\nnecessitates the inversion of the images into the domain of the pretrained\ndiffusion model. Achieving faithful inversion remains a challenge, particularly\nfor more recent models trained to generate images with a small number of\ndenoising steps. In this work, we introduce an inversion method with a high\nquality-to-operation ratio, enhancing reconstruction accuracy without\nincreasing the number of operations. Building on reversing the diffusion\nsampling process, our method employs an iterative renoising mechanism at each\ninversion sampling step. This mechanism refines the approximation of a\npredicted point along the forward diffusion trajectory, by iteratively applying\nthe pretrained diffusion model, and averaging these predictions. We evaluate\nthe performance of our ReNoise technique using various sampling algorithms and\nmodels, including recent accelerated diffusion models. Through comprehensive\nevaluations and comparisons, we show its effectiveness in terms of both\naccuracy and speed. Furthermore, we confirm that our method preserves\neditability by demonstrating text-driven image editing on real images.",
        "publication_date": "2024-03-21T17:52:08Z",
        "upvotes": 17
    },
    "2403.14148": {
        "url": "https://arxiv.org/abs/2403.14148",
        "title": "Efficient Video Diffusion Models via Content-Frame Motion-Latent\n  Decomposition",
        "authors": [
            "Sihyun Yu",
            "Weili Nie",
            "De-An Huang",
            "Boyi Li",
            "Jinwoo Shin",
            "Anima Anandkumar"
        ],
        "abstract": "Video diffusion models have recently made great progress in generation\nquality, but are still limited by the high memory and computational\nrequirements. This is because current video diffusion models often attempt to\nprocess high-dimensional videos directly. To tackle this issue, we propose\ncontent-motion latent diffusion model (CMD), a novel efficient extension of\npretrained image diffusion models for video generation. Specifically, we\npropose an autoencoder that succinctly encodes a video as a combination of a\ncontent frame (like an image) and a low-dimensional motion latent\nrepresentation. The former represents the common content, and the latter\nrepresents the underlying motion in the video, respectively. We generate the\ncontent frame by fine-tuning a pretrained image diffusion model, and we\ngenerate the motion latent representation by training a new lightweight\ndiffusion model. A key innovation here is the design of a compact latent space\nthat can directly utilizes a pretrained image diffusion model, which has not\nbeen done in previous latent video diffusion models. This leads to considerably\nbetter quality generation and reduced computational costs. For instance, CMD\ncan sample a video 7.7$\\times$ faster than prior approaches by generating a\nvideo of 512$\\times$1024 resolution and length 16 in 3.1 seconds. Moreover, CMD\nachieves an FVD score of 212.7 on WebVid-10M, 27.3% better than the previous\nstate-of-the-art of 292.4.",
        "publication_date": "2024-03-21T05:48:48Z",
        "upvotes": 17
    },
    "2403.14621": {
        "url": "https://arxiv.org/abs/2403.14621",
        "title": "GRM: Large Gaussian Reconstruction Model for Efficient 3D Reconstruction\n  and Generation",
        "authors": [
            "Yinghao Xu",
            "Zifan Shi",
            "Wang Yifan",
            "Hansheng Chen",
            "Ceyuan Yang",
            "Sida Peng",
            "Yujun Shen",
            "Gordon Wetzstein"
        ],
        "abstract": "We introduce GRM, a large-scale reconstructor capable of recovering a 3D\nasset from sparse-view images in around 0.1s. GRM is a feed-forward\ntransformer-based model that efficiently incorporates multi-view information to\ntranslate the input pixels into pixel-aligned Gaussians, which are unprojected\nto create a set of densely distributed 3D Gaussians representing a scene.\nTogether, our transformer architecture and the use of 3D Gaussians unlock a\nscalable and efficient reconstruction framework. Extensive experimental results\ndemonstrate the superiority of our method over alternatives regarding both\nreconstruction quality and efficiency. We also showcase the potential of GRM in\ngenerative tasks, i.e., text-to-3D and image-to-3D, by integrating it with\nexisting multi-view diffusion models. Our project website is at:\nhttps://justimyhxu.github.io/projects/grm/.",
        "publication_date": "2024-03-21T17:59:34Z",
        "upvotes": 14
    },
    "2403.14599": {
        "url": "https://arxiv.org/abs/2403.14599",
        "title": "MyVLM: Personalizing VLMs for User-Specific Queries",
        "authors": [
            "Yuval Alaluf",
            "Elad Richardson",
            "Sergey Tulyakov",
            "Kfir Aberman",
            "Daniel Cohen-Or"
        ],
        "abstract": "Recent large-scale vision-language models (VLMs) have demonstrated remarkable\ncapabilities in understanding and generating textual descriptions for visual\ncontent. However, these models lack an understanding of user-specific concepts.\nIn this work, we take a first step toward the personalization of VLMs, enabling\nthem to learn and reason over user-provided concepts. For example, we explore\nwhether these models can learn to recognize you in an image and communicate\nwhat you are doing, tailoring the model to reflect your personal experiences\nand relationships. To effectively recognize a variety of user-specific\nconcepts, we augment the VLM with external concept heads that function as\ntoggles for the model, enabling the VLM to identify the presence of specific\ntarget concepts in a given image. Having recognized the concept, we learn a new\nconcept embedding in the intermediate feature space of the VLM. This embedding\nis tasked with guiding the language model to naturally integrate the target\nconcept in its generated response. We apply our technique to BLIP-2 and LLaVA\nfor personalized image captioning and further show its applicability for\npersonalized visual question-answering. Our experiments demonstrate our ability\nto generalize to unseen images of learned concepts while preserving the model\nbehavior on unrelated inputs.",
        "publication_date": "2024-03-21T17:51:01Z",
        "upvotes": 13
    },
    "2403.14554": {
        "url": "https://arxiv.org/abs/2403.14554",
        "title": "Gaussian Frosting: Editable Complex Radiance Fields with Real-Time\n  Rendering",
        "authors": [
            "Antoine Gu\u00e9don",
            "Vincent Lepetit"
        ],
        "abstract": "We propose Gaussian Frosting, a novel mesh-based representation for\nhigh-quality rendering and editing of complex 3D effects in real-time. Our\napproach builds on the recent 3D Gaussian Splatting framework, which optimizes\na set of 3D Gaussians to approximate a radiance field from images. We propose\nfirst extracting a base mesh from Gaussians during optimization, then building\nand refining an adaptive layer of Gaussians with a variable thickness around\nthe mesh to better capture the fine details and volumetric effects near the\nsurface, such as hair or grass. We call this layer Gaussian Frosting, as it\nresembles a coating of frosting on a cake. The fuzzier the material, the\nthicker the frosting. We also introduce a parameterization of the Gaussians to\nenforce them to stay inside the frosting layer and automatically adjust their\nparameters when deforming, rescaling, editing or animating the mesh. Our\nrepresentation allows for efficient rendering using Gaussian splatting, as well\nas editing and animation by modifying the base mesh. We demonstrate the\neffectiveness of our method on various synthetic and real scenes, and show that\nit outperforms existing surface-based approaches. We will release our code and\na web-based viewer as additional contributions. Our project page is the\nfollowing: https://anttwo.github.io/frosting/",
        "publication_date": "2024-03-21T16:53:03Z",
        "upvotes": 12
    },
    "2403.14611": {
        "url": "https://arxiv.org/abs/2403.14611",
        "title": "Explorative Inbetweening of Time and Space",
        "authors": [
            "Haiwen Feng",
            "Zheng Ding",
            "Zhihao Xia",
            "Simon Niklaus",
            "Victoria Abrevaya",
            "Michael J. Black",
            "Xuaner Zhang"
        ],
        "abstract": "We introduce bounded generation as a generalized task to control video\ngeneration to synthesize arbitrary camera and subject motion based only on a\ngiven start and end frame. Our objective is to fully leverage the inherent\ngeneralization capability of an image-to-video model without additional\ntraining or fine-tuning of the original model. This is achieved through the\nproposed new sampling strategy, which we call Time Reversal Fusion, that fuses\nthe temporally forward and backward denoising paths conditioned on the start\nand end frame, respectively. The fused path results in a video that smoothly\nconnects the two frames, generating inbetweening of faithful subject motion,\nnovel views of static scenes, and seamless video looping when the two bounding\nframes are identical. We curate a diverse evaluation dataset of image pairs and\ncompare against the closest existing methods. We find that Time Reversal Fusion\noutperforms related work on all subtasks, exhibiting the ability to generate\ncomplex motions and 3D-consistent views guided by bounded frames. See project\npage at https://time-reversal.github.io.",
        "publication_date": "2024-03-21T17:57:31Z",
        "upvotes": 10
    },
    "2403.14186": {
        "url": "https://arxiv.org/abs/2403.14186",
        "title": "StyleCineGAN: Landscape Cinemagraph Generation using a Pre-trained\n  StyleGAN",
        "authors": [
            "Jongwoo Choi",
            "Kwanggyoon Seo",
            "Amirsaman Ashtari",
            "Junyong Noh"
        ],
        "abstract": "We propose a method that can generate cinemagraphs automatically from a still\nlandscape image using a pre-trained StyleGAN. Inspired by the success of recent\nunconditional video generation, we leverage a powerful pre-trained image\ngenerator to synthesize high-quality cinemagraphs. Unlike previous approaches\nthat mainly utilize the latent space of a pre-trained StyleGAN, our approach\nutilizes its deep feature space for both GAN inversion and cinemagraph\ngeneration. Specifically, we propose multi-scale deep feature warping (MSDFW),\nwhich warps the intermediate features of a pre-trained StyleGAN at different\nresolutions. By using MSDFW, the generated cinemagraphs are of high resolution\nand exhibit plausible looping animation. We demonstrate the superiority of our\nmethod through user studies and quantitative comparisons with state-of-the-art\ncinemagraph generation methods and a video generation method that uses a\npre-trained StyleGAN.",
        "publication_date": "2024-03-21T07:21:51Z",
        "upvotes": 8
    },
    "2403.14467": {
        "url": "https://arxiv.org/abs/2403.14467",
        "title": "Recourse for reclamation: Chatting with generative language models",
        "authors": [
            "Jennifer Chien",
            "Kevin R. McKee",
            "Jackie Kay",
            "William Isaac"
        ],
        "abstract": "Researchers and developers increasingly rely on toxicity scoring to moderate\ngenerative language model outputs, in settings such as customer service,\ninformation retrieval, and content generation. However, toxicity scoring may\nrender pertinent information inaccessible, rigidify or \"value-lock\" cultural\nnorms, and prevent language reclamation processes, particularly for\nmarginalized people. In this work, we extend the concept of algorithmic\nrecourse to generative language models: we provide users a novel mechanism to\nachieve their desired prediction by dynamically setting thresholds for toxicity\nfiltering. Users thereby exercise increased agency relative to interactions\nwith the baseline system. A pilot study ($n = 30$) supports the potential of\nour proposed recourse mechanism, indicating improvements in usability compared\nto fixed-threshold toxicity-filtering of model outputs. Future work should\nexplore the intersection of toxicity scoring, model controllability, user\nagency, and language reclamation processes -- particularly with regard to the\nbias that many communities encounter when interacting with generative language\nmodels.",
        "publication_date": "2024-03-21T15:14:25Z",
        "upvotes": 6
    },
    "2403.15371": {
        "url": "https://arxiv.org/abs/2403.15371",
        "title": "Can large language models explore in-context?",
        "authors": [
            "Akshay Krishnamurthy",
            "Keegan Harris",
            "Dylan J. Foster",
            "Cyril Zhang",
            "Aleksandrs Slivkins"
        ],
        "abstract": "We investigate the extent to which contemporary Large Language Models (LLMs)\ncan engage in exploration, a core capability in reinforcement learning and\ndecision making. We focus on native performance of existing LLMs, without\ntraining interventions. We deploy LLMs as agents in simple multi-armed bandit\nenvironments, specifying the environment description and interaction history\nentirely in-context, i.e., within the LLM prompt. We experiment with GPT-3.5,\nGPT-4, and Llama2, using a variety of prompt designs, and find that the models\ndo not robustly engage in exploration without substantial interventions: i)\nAcross all of our experiments, only one configuration resulted in satisfactory\nexploratory behavior: GPT-4 with chain-of-thought reasoning and an externally\nsummarized interaction history, presented as sufficient statistics; ii) All\nother configurations did not result in robust exploratory behavior, including\nthose with chain-of-thought reasoning but unsummarized history. Although these\nfindings can be interpreted positively, they suggest that external\nsummarization -- which may not be possible in more complex settings -- is\nimportant for obtaining desirable behavior from LLM agents. We conclude that\nnon-trivial algorithmic interventions, such as fine-tuning or dataset curation,\nmay be required to empower LLM-based decision making agents in complex\nsettings.",
        "publication_date": "2024-03-22T17:50:43Z",
        "upvotes": 29
    },
    "2403.15042": {
        "url": "https://arxiv.org/abs/2403.15042",
        "title": "LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement",
        "authors": [
            "Nicholas Lee",
            "Thanakul Wattanawong",
            "Sehoon Kim",
            "Karttikeya Mangalam",
            "Sheng Shen",
            "Gopala Anumanchipali",
            "Michael W. Mahoney",
            "Kurt Keutzer",
            "Amir Gholami"
        ],
        "abstract": "Pretrained large language models (LLMs) are currently state-of-the-art for\nsolving the vast majority of natural language processing tasks. While many\nreal-world applications still require fine-tuning to reach satisfactory levels\nof performance, many of them are in the low-data regime, making fine-tuning\nchallenging. To address this, we propose LLM2LLM, a targeted and iterative data\naugmentation strategy that uses a teacher LLM to enhance a small seed dataset\nby augmenting additional data that can be used for fine-tuning on a specific\ntask. LLM2LLM (1) fine-tunes a baseline student LLM on the initial seed data,\n(2) evaluates and extracts data points that the model gets wrong, and (3) uses\na teacher LLM to generate synthetic data based on these incorrect data points,\nwhich are then added back into the training data. This approach amplifies the\nsignal from incorrectly predicted data points by the LLM during training and\nreintegrates them into the dataset to focus on more challenging examples for\nthe LLM. Our results show that LLM2LLM significantly enhances the performance\nof LLMs in the low-data regime, outperforming both traditional fine-tuning and\nother data augmentation baselines. LLM2LLM reduces the dependence on\nlabor-intensive data curation and paves the way for more scalable and\nperformant LLM solutions, allowing us to tackle data-constrained domains and\ntasks. We achieve improvements up to 24.2% on the GSM8K dataset, 32.6% on\nCaseHOLD, 32.0% on SNIPS, 52.6% on TREC and 39.8% on SST-2 over regular\nfine-tuning in the low-data regime using a LLaMA2-7B student model.",
        "publication_date": "2024-03-22T08:57:07Z",
        "upvotes": 23
    },
    "2403.15377": {
        "url": "https://arxiv.org/abs/2403.15377",
        "title": "InternVideo2: Scaling Video Foundation Models for Multimodal Video\n  Understanding",
        "authors": [
            "Yi Wang",
            "Kunchang Li",
            "Xinhao Li",
            "Jiashuo Yu",
            "Yinan He",
            "Guo Chen",
            "Baoqi Pei",
            "Rongkun Zheng",
            "Jilan Xu",
            "Zun Wang",
            "Yansong Shi",
            "Tianxiang Jiang",
            "Songze Li",
            "Hongjie Zhang",
            "Yifei Huang",
            "Yu Qiao",
            "Yali Wang",
            "Limin Wang"
        ],
        "abstract": "We introduce InternVideo2, a new video foundation model (ViFM) that achieves\nthe state-of-the-art performance in action recognition, video-text tasks, and\nvideo-centric dialogue. Our approach employs a progressive training paradigm\nthat unifies the different self- or weakly-supervised learning frameworks of\nmasked video token reconstruction, cross-modal contrastive learning, and next\ntoken prediction. Different training stages would guide our model to capture\ndifferent levels of structure and semantic information through different\npretext tasks. At the data level, we prioritize the spatiotemporal consistency\nby semantically segmenting videos and generating video-audio-speech captions.\nThis improves the alignment between video and text. We scale both data and\nmodel size for our InternVideo2. Through extensive experiments, we validate our\ndesigns and demonstrate the state-of-the-art performance on over 60 video and\naudio tasks. Notably, our model outperforms others on various video-related\ncaptioning, dialogue, and long video understanding benchmarks, highlighting its\nability to reason and comprehend long temporal contexts. Code and models are\navailable at https://github.com/OpenGVLab/InternVideo2/.",
        "publication_date": "2024-03-22T17:57:42Z",
        "upvotes": 14
    },
    "2403.14781": {
        "url": "https://arxiv.org/abs/2403.14781",
        "title": "Champ: Controllable and Consistent Human Image Animation with 3D\n  Parametric Guidance",
        "authors": [
            "Shenhao Zhu",
            "Junming Leo Chen",
            "Zuozhuo Dai",
            "Yinghui Xu",
            "Xun Cao",
            "Yao Yao",
            "Hao Zhu",
            "Siyu Zhu"
        ],
        "abstract": "In this study, we introduce a methodology for human image animation by\nleveraging a 3D human parametric model within a latent diffusion framework to\nenhance shape alignment and motion guidance in curernt human generative\ntechniques. The methodology utilizes the SMPL(Skinned Multi-Person Linear)\nmodel as the 3D human parametric model to establish a unified representation of\nbody shape and pose. This facilitates the accurate capture of intricate human\ngeometry and motion characteristics from source videos. Specifically, we\nincorporate rendered depth images, normal maps, and semantic maps obtained from\nSMPL sequences, alongside skeleton-based motion guidance, to enrich the\nconditions to the latent diffusion model with comprehensive 3D shape and\ndetailed pose attributes. A multi-layer motion fusion module, integrating\nself-attention mechanisms, is employed to fuse the shape and motion latent\nrepresentations in the spatial domain. By representing the 3D human parametric\nmodel as the motion guidance, we can perform parametric shape alignment of the\nhuman body between the reference image and the source video motion.\nExperimental evaluations conducted on benchmark datasets demonstrate the\nmethodology's superior ability to generate high-quality human animations that\naccurately capture both pose and shape variations. Furthermore, our approach\nalso exhibits superior generalization capabilities on the proposed wild\ndataset. Project page: https://fudan-generative-vision.github.io/champ.",
        "publication_date": "2024-03-21T18:52:58Z",
        "upvotes": 13
    },
    "2403.15383": {
        "url": "https://arxiv.org/abs/2403.15383",
        "title": "ThemeStation: Generating Theme-Aware 3D Assets from Few Exemplars",
        "authors": [
            "Zhenwei Wang",
            "Tengfei Wang",
            "Gerhard Hancke",
            "Ziwei Liu",
            "Rynson W. H. Lau"
        ],
        "abstract": "Real-world applications often require a large gallery of 3D assets that share\na consistent theme. While remarkable advances have been made in general 3D\ncontent creation from text or image, synthesizing customized 3D assets\nfollowing the shared theme of input 3D exemplars remains an open and\nchallenging problem. In this work, we present ThemeStation, a novel approach\nfor theme-aware 3D-to-3D generation. ThemeStation synthesizes customized 3D\nassets based on given few exemplars with two goals: 1) unity for generating 3D\nassets that thematically align with the given exemplars and 2) diversity for\ngenerating 3D assets with a high degree of variations. To this end, we design a\ntwo-stage framework that draws a concept image first, followed by a\nreference-informed 3D modeling stage. We propose a novel dual score\ndistillation (DSD) loss to jointly leverage priors from both the input\nexemplars and the synthesized concept image. Extensive experiments and user\nstudies confirm that ThemeStation surpasses prior works in producing diverse\ntheme-aware 3D models with impressive quality. ThemeStation also enables\nvarious applications such as controllable 3D-to-3D generation.",
        "publication_date": "2024-03-22T17:59:01Z",
        "upvotes": 11
    },
    "2403.15360": {
        "url": "https://arxiv.org/abs/2403.15360",
        "title": "SiMBA: Simplified Mamba-Based Architecture for Vision and Multivariate\n  Time series",
        "authors": [
            "Badri N. Patro",
            "Vijay S. Agneeswaran"
        ],
        "abstract": "Transformers have widely adopted attention networks for sequence mixing and\nMLPs for channel mixing, playing a pivotal role in achieving breakthroughs\nacross domains. However, recent literature highlights issues with attention\nnetworks, including low inductive bias and quadratic complexity concerning\ninput sequence length. State Space Models (SSMs) like S4 and others (Hippo,\nGlobal Convolutions, liquid S4, LRU, Mega, and Mamba), have emerged to address\nthe above issues to help handle longer sequence lengths. Mamba, while being the\nstate-of-the-art SSM, has a stability issue when scaled to large networks for\ncomputer vision datasets. We propose SiMBA, a new architecture that introduces\nEinstein FFT (EinFFT) for channel modeling by specific eigenvalue computations\nand uses the Mamba block for sequence modeling. Extensive performance studies\nacross image and time-series benchmarks demonstrate that SiMBA outperforms\nexisting SSMs, bridging the performance gap with state-of-the-art transformers.\nNotably, SiMBA establishes itself as the new state-of-the-art SSM on ImageNet\nand transfer learning benchmarks such as Stanford Car and Flower as well as\ntask learning benchmarks as well as seven time series benchmark datasets. The\nproject page is available on this website\n~\\url{https://github.com/badripatro/Simba}.",
        "publication_date": "2024-03-22T17:22:56Z",
        "upvotes": 11
    },
    "2403.15382": {
        "url": "https://arxiv.org/abs/2403.15382",
        "title": "DragAPart: Learning a Part-Level Motion Prior for Articulated Objects",
        "authors": [
            "Ruining Li",
            "Chuanxia Zheng",
            "Christian Rupprecht",
            "Andrea Vedaldi"
        ],
        "abstract": "We introduce DragAPart, a method that, given an image and a set of drags as\ninput, can generate a new image of the same object in a new state, compatible\nwith the action of the drags. Differently from prior works that focused on\nrepositioning objects, DragAPart predicts part-level interactions, such as\nopening and closing a drawer. We study this problem as a proxy for learning a\ngeneralist motion model, not restricted to a specific kinematic structure or\nobject category. To this end, we start from a pre-trained image generator and\nfine-tune it on a new synthetic dataset, Drag-a-Move, which we introduce.\nCombined with a new encoding for the drags and dataset randomization, the new\nmodel generalizes well to real images and different categories. Compared to\nprior motion-controlled generators, we demonstrate much better part-level\nmotion understanding.",
        "publication_date": "2024-03-22T17:58:59Z",
        "upvotes": 9
    },
    "2403.14870": {
        "url": "https://arxiv.org/abs/2403.14870",
        "title": "VidLA: Video-Language Alignment at Scale",
        "authors": [
            "Mamshad Nayeem Rizve",
            "Fan Fei",
            "Jayakrishnan Unnikrishnan",
            "Son Tran",
            "Benjamin Z. Yao",
            "Belinda Zeng",
            "Mubarak Shah",
            "Trishul Chilimbi"
        ],
        "abstract": "In this paper, we propose VidLA, an approach for video-language alignment at\nscale. There are two major limitations of previous video-language alignment\napproaches. First, they do not capture both short-range and long-range temporal\ndependencies and typically employ complex hierarchical deep network\narchitectures that are hard to integrate with existing pretrained image-text\nfoundation models. To effectively address this limitation, we instead keep the\nnetwork architecture simple and use a set of data tokens that operate at\ndifferent temporal resolutions in a hierarchical manner, accounting for the\ntemporally hierarchical nature of videos. By employing a simple two-tower\narchitecture, we are able to initialize our video-language model with\npretrained image-text foundation models, thereby boosting the final\nperformance. Second, existing video-language alignment works struggle due to\nthe lack of semantically aligned large-scale training data. To overcome it, we\nleverage recent LLMs to curate the largest video-language dataset to date with\nbetter visual grounding. Furthermore, unlike existing video-text datasets which\nonly contain short clips, our dataset is enriched with video clips of varying\ndurations to aid our temporally hierarchical data tokens in extracting better\nrepresentations at varying temporal scales. Overall, empirical results show\nthat our proposed approach surpasses state-of-the-art methods on multiple\nretrieval benchmarks, especially on longer videos, and performs competitively\non classification benchmarks.",
        "publication_date": "2024-03-21T22:36:24Z",
        "upvotes": 9
    },
    "2403.15246": {
        "url": "https://arxiv.org/abs/2403.15246",
        "title": "FollowIR: Evaluating and Teaching Information Retrieval Models to Follow\n  Instructions",
        "authors": [
            "Orion Weller",
            "Benjamin Chang",
            "Sean MacAvaney",
            "Kyle Lo",
            "Arman Cohan",
            "Benjamin Van Durme",
            "Dawn Lawrie",
            "Luca Soldaini"
        ],
        "abstract": "Modern Large Language Models (LLMs) are capable of following long and complex\ninstructions that enable a diverse amount of user tasks. However, despite\nInformation Retrieval (IR) models using LLMs as the backbone of their\narchitectures, nearly all of them still only take queries as input, with no\ninstructions. For the handful of recent models that do take instructions, it's\nunclear how they use them. We introduce our dataset FollowIR, which contains a\nrigorous instruction evaluation benchmark as well as a training set for helping\nIR models learn to better follow real-world instructions. FollowIR builds off\nthe long history of the TREC conferences: as TREC provides human annotators\nwith instructions (also known as narratives) to determine document relevance,\nso should IR models be able to understand and decide relevance based on these\ndetailed instructions. Our evaluation benchmark starts with three deeply judged\nTREC collections and alters the annotator instructions, re-annotating relevant\ndocuments. Through this process, we can measure how well IR models follow\ninstructions, through a new pairwise evaluation framework. Our results indicate\nthat existing retrieval models fail to correctly use instructions, using them\nfor basic keywords and struggling to understand long-form information. However,\nwe show that it is possible for IR models to learn to follow complex\ninstructions: our new FollowIR-7B model has significant improvements (over 13%)\nafter fine-tuning on our training set.",
        "publication_date": "2024-03-22T14:42:29Z",
        "upvotes": 8
    },
    "2403.14773": {
        "url": "https://arxiv.org/abs/2403.14773",
        "title": "StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation\n  from Text",
        "authors": [
            "Roberto Henschel",
            "Levon Khachatryan",
            "Daniil Hayrapetyan",
            "Hayk Poghosyan",
            "Vahram Tadevosyan",
            "Zhangyang Wang",
            "Shant Navasardyan",
            "Humphrey Shi"
        ],
        "abstract": "Text-to-video diffusion models enable the generation of high-quality videos\nthat follow text instructions, making it easy to create diverse and individual\ncontent. However, existing approaches mostly focus on high-quality short video\ngeneration (typically 16 or 24 frames), ending up with hard-cuts when naively\nextended to the case of long video synthesis. To overcome these limitations, we\nintroduce StreamingT2V, an autoregressive approach for long video generation of\n80, 240, 600, 1200 or more frames with smooth transitions. The key components\nare:(i) a short-term memory block called conditional attention module (CAM),\nwhich conditions the current generation on the features extracted from the\nprevious chunk via an attentional mechanism, leading to consistent chunk\ntransitions, (ii) a long-term memory block called appearance preservation\nmodule, which extracts high-level scene and object features from the first\nvideo chunk to prevent the model from forgetting the initial scene, and (iii) a\nrandomized blending approach that enables to apply a video enhancer\nautoregressively for infinitely long videos without inconsistencies between\nchunks. Experiments show that StreamingT2V generates high motion amount. In\ncontrast, all competing image-to-video methods are prone to video stagnation\nwhen applied naively in an autoregressive manner. Thus, we propose with\nStreamingT2V a high-quality seamless text-to-long video generator that\noutperforms competitors with consistency and motion. Our code will be available\nat: https://github.com/Picsart-AI-Research/StreamingT2V",
        "publication_date": "2024-03-21T18:27:29Z",
        "upvotes": 8
    },
    "2403.15157": {
        "url": "https://arxiv.org/abs/2403.15157",
        "title": "AllHands: Ask Me Anything on Large-scale Verbatim Feedback via Large\n  Language Models",
        "authors": [
            "Chaoyun Zhang",
            "Zicheng Ma",
            "Yuhao Wu",
            "Shilin He",
            "Si Qin",
            "Minghua Ma",
            "Xiaoting Qin",
            "Yu Kang",
            "Yuyi Liang",
            "Xiaoyu Gou",
            "Yajie Xue",
            "Qingwei Lin",
            "Saravan Rajmohan",
            "Dongmei Zhang",
            "Qi Zhang"
        ],
        "abstract": "Verbatim feedback constitutes a valuable repository of user experiences,\nopinions, and requirements essential for software development. Effectively and\nefficiently extracting valuable insights from such data poses a challenging\ntask. This paper introduces Allhands , an innovative analytic framework\ndesigned for large-scale feedback analysis through a natural language\ninterface, leveraging large language models (LLMs). Allhands adheres to a\nconventional feedback analytic workflow, initially conducting classification\nand topic modeling on the feedback to convert them into a structurally\naugmented format, incorporating LLMs to enhance accuracy, robustness,\ngeneralization, and user-friendliness. Subsequently, an LLM agent is employed\nto interpret users' diverse questions in natural language on feedback,\ntranslating them into Python code for execution, and delivering comprehensive\nmulti-modal responses, including text, code, tables, and images.\n  We evaluate Allhands across three diverse feedback datasets. The experiments\ndemonstrate that Allhands achieves superior efficacy at all stages of analysis,\nincluding classification and topic modeling, eventually providing users with an\n\"ask me anything\" experience with comprehensive, correct and human-readable\nresponse. To the best of our knowledge, Allhands stands as the first\ncomprehensive feedback analysis framework that supports diverse and customized\nrequirements for insight extraction through a natural language interface.",
        "publication_date": "2024-03-22T12:13:16Z",
        "upvotes": 6
    },
    "2403.15385": {
        "url": "https://arxiv.org/abs/2403.15385",
        "title": "LATTE3D: Large-scale Amortized Text-To-Enhanced3D Synthesis",
        "authors": [
            "Kevin Xie",
            "Jonathan Lorraine",
            "Tianshi Cao",
            "Jun Gao",
            "James Lucas",
            "Antonio Torralba",
            "Sanja Fidler",
            "Xiaohui Zeng"
        ],
        "abstract": "Recent text-to-3D generation approaches produce impressive 3D results but\nrequire time-consuming optimization that can take up to an hour per prompt.\nAmortized methods like ATT3D optimize multiple prompts simultaneously to\nimprove efficiency, enabling fast text-to-3D synthesis. However, they cannot\ncapture high-frequency geometry and texture details and struggle to scale to\nlarge prompt sets, so they generalize poorly. We introduce LATTE3D, addressing\nthese limitations to achieve fast, high-quality generation on a significantly\nlarger prompt set. Key to our method is 1) building a scalable architecture and\n2) leveraging 3D data during optimization through 3D-aware diffusion priors,\nshape regularization, and model initialization to achieve robustness to diverse\nand complex training prompts. LATTE3D amortizes both neural field and textured\nsurface generation to produce highly detailed textured meshes in a single\nforward pass. LATTE3D generates 3D objects in 400ms, and can be further\nenhanced with fast test-time optimization.",
        "publication_date": "2024-03-22T17:59:37Z",
        "upvotes": 5
    },
    "2403.14714": {
        "url": "https://arxiv.org/abs/2403.14714",
        "title": "Compiler generated feedback for Large Language Models",
        "authors": [
            "Dejan Grubisic",
            "Chris Cummins",
            "Volker Seeker",
            "Hugh Leather"
        ],
        "abstract": "We introduce a novel paradigm in compiler optimization powered by Large\nLanguage Models with compiler feedback to optimize the code size of LLVM\nassembly. The model takes unoptimized LLVM IR as input and produces optimized\nIR, the best optimization passes, and instruction counts of both unoptimized\nand optimized IRs. Then we compile the input with generated optimization passes\nand evaluate if the predicted instruction count is correct, generated IR is\ncompilable, and corresponds to compiled code. We provide this feedback back to\nLLM and give it another chance to optimize code. This approach adds an extra\n0.53% improvement over -Oz to the original model. Even though, adding more\ninformation with feedback seems intuitive, simple sampling techniques achieve\nmuch higher performance given 10 or more samples.",
        "publication_date": "2024-03-18T23:25:13Z",
        "upvotes": 4
    },
    "2403.16971": {
        "url": "https://arxiv.org/abs/2403.16971",
        "title": "AIOS: LLM Agent Operating System",
        "authors": [
            "Kai Mei",
            "Zelong Li",
            "Shuyuan Xu",
            "Ruosong Ye",
            "Yingqiang Ge",
            "Yongfeng Zhang"
        ],
        "abstract": "The integration and deployment of large language model (LLM)-based\nintelligent agents have been fraught with challenges that compromise their\nefficiency and efficacy. Among these issues are sub-optimal scheduling and\nresource allocation of agent requests over the LLM, the difficulties in\nmaintaining context during interactions between agent and LLM, and the\ncomplexities inherent in integrating heterogeneous agents with different\ncapabilities and specializations. The rapid increase of agent quantity and\ncomplexity further exacerbates these issues, often leading to bottlenecks and\nsub-optimal utilization of resources. Inspired by these challenges, this paper\npresents AIOS, an LLM agent operating system, which embeds large language model\ninto operating systems (OS) as the brain of the OS, enabling an operating\nsystem \"with soul\" -- an important step towards AGI. Specifically, AIOS is\ndesigned to optimize resource allocation, facilitate context switch across\nagents, enable concurrent execution of agents, provide tool service for agents,\nand maintain access control for agents. We present the architecture of such an\noperating system, outline the core challenges it aims to resolve, and provide\nthe basic design and implementation of the AIOS. Our experiments on concurrent\nexecution of multiple agents demonstrate the reliability and efficiency of our\nAIOS modules. Through this, we aim to not only improve the performance and\nefficiency of LLM agents but also to pioneer for better development and\ndeployment of the AIOS ecosystem in the future. The project is open-source at\nhttps://github.com/agiresearch/AIOS.",
        "publication_date": "2024-03-25T17:32:23Z",
        "upvotes": 60
    },
    "2403.16990": {
        "url": "https://arxiv.org/abs/2403.16990",
        "title": "Be Yourself: Bounded Attention for Multi-Subject Text-to-Image\n  Generation",
        "authors": [
            "Omer Dahary",
            "Or Patashnik",
            "Kfir Aberman",
            "Daniel Cohen-Or"
        ],
        "abstract": "Text-to-image diffusion models have an unprecedented ability to generate\ndiverse and high-quality images. However, they often struggle to faithfully\ncapture the intended semantics of complex input prompts that include multiple\nsubjects. Recently, numerous layout-to-image extensions have been introduced to\nimprove user control, aiming to localize subjects represented by specific\ntokens. Yet, these methods often produce semantically inaccurate images,\nespecially when dealing with multiple semantically or visually similar\nsubjects. In this work, we study and analyze the causes of these limitations.\nOur exploration reveals that the primary issue stems from inadvertent semantic\nleakage between subjects in the denoising process. This leakage is attributed\nto the diffusion model's attention layers, which tend to blend the visual\nfeatures of different subjects. To address these issues, we introduce Bounded\nAttention, a training-free method for bounding the information flow in the\nsampling process. Bounded Attention prevents detrimental leakage among subjects\nand enables guiding the generation to promote each subject's individuality,\neven with complex multi-subject conditioning. Through extensive\nexperimentation, we demonstrate that our method empowers the generation of\nmultiple subjects that better align with given prompts and layouts.",
        "publication_date": "2024-03-25T17:52:07Z",
        "upvotes": 23
    },
    "2403.16627": {
        "url": "https://arxiv.org/abs/2403.16627",
        "title": "SDXS: Real-Time One-Step Latent Diffusion Models with Image Conditions",
        "authors": [
            "Yuda Song",
            "Zehao Sun",
            "Xuanwu Yin"
        ],
        "abstract": "Recent advancements in diffusion models have positioned them at the forefront\nof image generation. Despite their superior performance, diffusion models are\nnot without drawbacks; they are characterized by complex architectures and\nsubstantial computational demands, resulting in significant latency due to\ntheir iterative sampling process. To mitigate these limitations, we introduce a\ndual approach involving model miniaturization and a reduction in sampling\nsteps, aimed at significantly decreasing model latency. Our methodology\nleverages knowledge distillation to streamline the U-Net and image decoder\narchitectures, and introduces an innovative one-step DM training technique that\nutilizes feature matching and score distillation. We present two models,\nSDXS-512 and SDXS-1024, achieving inference speeds of approximately 100 FPS\n(30x faster than SD v1.5) and 30 FP (60x faster than SDXL) on a single GPU,\nrespectively. Moreover, our training approach offers promising applications in\nimage-conditioned control, facilitating efficient image-to-image translation.",
        "publication_date": "2024-03-25T11:16:23Z",
        "upvotes": 19
    },
    "2403.17008": {
        "url": "https://arxiv.org/abs/2403.17008",
        "title": "FlashFace: Human Image Personalization with High-fidelity Identity\n  Preservation",
        "authors": [
            "Shilong Zhang",
            "Lianghua Huang",
            "Xi Chen",
            "Yifei Zhang",
            "Zhi-Fan Wu",
            "Yutong Feng",
            "Wei Wang",
            "Yujun Shen",
            "Yu Liu",
            "Ping Luo"
        ],
        "abstract": "This work presents FlashFace, a practical tool with which users can easily\npersonalize their own photos on the fly by providing one or a few reference\nface images and a text prompt. Our approach is distinguishable from existing\nhuman photo customization methods by higher-fidelity identity preservation and\nbetter instruction following, benefiting from two subtle designs. First, we\nencode the face identity into a series of feature maps instead of one image\ntoken as in prior arts, allowing the model to retain more details of the\nreference faces (e.g., scars, tattoos, and face shape ). Second, we introduce a\ndisentangled integration strategy to balance the text and image guidance during\nthe text-to-image generation process, alleviating the conflict between the\nreference faces and the text prompts (e.g., personalizing an adult into a\n\"child\" or an \"elder\"). Extensive experimental results demonstrate the\neffectiveness of our method on various applications, including human image\npersonalization, face swapping under language prompts, making virtual\ncharacters into real people, etc. Project Page:\nhttps://jshilong.github.io/flashface-page.",
        "publication_date": "2024-03-25T17:59:57Z",
        "upvotes": 17
    },
    "2403.15447": {
        "url": "https://arxiv.org/abs/2403.15447",
        "title": "Decoding Compressed Trust: Scrutinizing the Trustworthiness of Efficient\n  LLMs Under Compression",
        "authors": [
            "Junyuan Hong",
            "Jinhao Duan",
            "Chenhui Zhang",
            "Zhangheng Li",
            "Chulin Xie",
            "Kelsey Lieberman",
            "James Diffenderfer",
            "Brian Bartoldson",
            "Ajay Jaiswal",
            "Kaidi Xu",
            "Bhavya Kailkhura",
            "Dan Hendrycks",
            "Dawn Song",
            "Zhangyang Wang",
            "Bo Li"
        ],
        "abstract": "Compressing high-capability Large Language Models (LLMs) has emerged as a\nfavored strategy for resource-efficient inferences. While state-of-the-art\n(SoTA) compression methods boast impressive advancements in preserving benign\ntask performance, the potential risks of compression in terms of safety and\ntrustworthiness have been largely neglected. This study conducts the first,\nthorough evaluation of three (3) leading LLMs using five (5) SoTA compression\ntechniques across eight (8) trustworthiness dimensions. Our experiments\nhighlight the intricate interplay between compression and trustworthiness,\nrevealing some interesting patterns. We find that quantization is currently a\nmore effective approach than pruning in achieving efficiency and\ntrustworthiness simultaneously. For instance, a 4-bit quantized model retains\nthe trustworthiness of its original counterpart, but model pruning\nsignificantly degrades trustworthiness, even at 50% sparsity. Moreover,\nemploying quantization within a moderate bit range could unexpectedly improve\ncertain trustworthiness dimensions such as ethics and fairness. Conversely,\nextreme quantization to very low bit levels (3 bits) tends to significantly\nreduce trustworthiness. This increased risk cannot be uncovered by looking at\nbenign performance alone, in turn, mandating comprehensive trustworthiness\nevaluation in practice. These findings culminate in practical recommendations\nfor simultaneously achieving high utility, efficiency, and trustworthiness in\nLLMs. Models and code are available at https://decoding-comp-trust.github.io/.",
        "publication_date": "2024-03-18T01:38:19Z",
        "upvotes": 14
    },
    "2403.17005": {
        "url": "https://arxiv.org/abs/2403.17005",
        "title": "TRIP: Temporal Residual Learning with Image Noise Prior for\n  Image-to-Video Diffusion Models",
        "authors": [
            "Zhongwei Zhang",
            "Fuchen Long",
            "Yingwei Pan",
            "Zhaofan Qiu",
            "Ting Yao",
            "Yang Cao",
            "Tao Mei"
        ],
        "abstract": "Recent advances in text-to-video generation have demonstrated the utility of\npowerful diffusion models. Nevertheless, the problem is not trivial when\nshaping diffusion models to animate static image (i.e., image-to-video\ngeneration). The difficulty originates from the aspect that the diffusion\nprocess of subsequent animated frames should not only preserve the faithful\nalignment with the given image but also pursue temporal coherence among\nadjacent frames. To alleviate this, we present TRIP, a new recipe of\nimage-to-video diffusion paradigm that pivots on image noise prior derived from\nstatic image to jointly trigger inter-frame relational reasoning and ease the\ncoherent temporal modeling via temporal residual learning. Technically, the\nimage noise prior is first attained through one-step backward diffusion process\nbased on both static image and noised video latent codes. Next, TRIP executes a\nresidual-like dual-path scheme for noise prediction: 1) a shortcut path that\ndirectly takes image noise prior as the reference noise of each frame to\namplify the alignment between the first frame and subsequent frames; 2) a\nresidual path that employs 3D-UNet over noised video and static image latent\ncodes to enable inter-frame relational reasoning, thereby easing the learning\nof the residual noise for each frame. Furthermore, both reference and residual\nnoise of each frame are dynamically merged via attention mechanism for final\nvideo generation. Extensive experiments on WebVid-10M, DTDB and MSR-VTT\ndatasets demonstrate the effectiveness of our TRIP for image-to-video\ngeneration. Please see our project page at https://trip-i2v.github.io/TRIP/.",
        "publication_date": "2024-03-25T17:59:40Z",
        "upvotes": 13
    },
    "2403.15484": {
        "url": "https://arxiv.org/abs/2403.15484",
        "title": "RakutenAI-7B: Extending Large Language Models for Japanese",
        "authors": [
            "Rakuten Group",
            "Aaron Levine",
            "Connie Huang",
            "Chenguang Wang",
            "Eduardo Batista",
            "Ewa Szymanska",
            "Hongyi Ding",
            "Hou Wei Chou",
            "Jean-Fran\u00e7ois Pessiot",
            "Johanes Effendi",
            "Justin Chiu",
            "Kai Torben Ohlhus",
            "Karan Chopra",
            "Keiji Shinzato",
            "Koji Murakami",
            "Lee Xiong",
            "Lei Chen",
            "Maki Kubota",
            "Maksim Tkachenko",
            "Miroku Lee",
            "Naoki Takahashi",
            "Prathyusha Jwalapuram",
            "Ryutaro Tatsushima",
            "Saurabh Jain",
            "Sunil Kumar Yadav",
            "Ting Cai",
            "Wei-Te Chen",
            "Yandi Xia",
            "Yuki Nakayama",
            "Yutaka Higashiyama"
        ],
        "abstract": "We introduce RakutenAI-7B, a suite of Japanese-oriented large language models\nthat achieve the best performance on the Japanese LM Harness benchmarks among\nthe open 7B models. Along with the foundation model, we release instruction-\nand chat-tuned models, RakutenAI-7B-instruct and RakutenAI-7B-chat\nrespectively, under the Apache 2.0 license.",
        "publication_date": "2024-03-21T06:56:07Z",
        "upvotes": 11
    },
    "2403.17001": {
        "url": "https://arxiv.org/abs/2403.17001",
        "title": "VP3D: Unleashing 2D Visual Prompt for Text-to-3D Generation",
        "authors": [
            "Yang Chen",
            "Yingwei Pan",
            "Haibo Yang",
            "Ting Yao",
            "Tao Mei"
        ],
        "abstract": "Recent innovations on text-to-3D generation have featured Score Distillation\nSampling (SDS), which enables the zero-shot learning of implicit 3D models\n(NeRF) by directly distilling prior knowledge from 2D diffusion models.\nHowever, current SDS-based models still struggle with intricate text prompts\nand commonly result in distorted 3D models with unrealistic textures or\ncross-view inconsistency issues. In this work, we introduce a novel Visual\nPrompt-guided text-to-3D diffusion model (VP3D) that explicitly unleashes the\nvisual appearance knowledge in 2D visual prompt to boost text-to-3D generation.\nInstead of solely supervising SDS with text prompt, VP3D first capitalizes on\n2D diffusion model to generate a high-quality image from input text, which\nsubsequently acts as visual prompt to strengthen SDS optimization with explicit\nvisual appearance. Meanwhile, we couple the SDS optimization with additional\ndifferentiable reward function that encourages rendering images of 3D models to\nbetter visually align with 2D visual prompt and semantically match with text\nprompt. Through extensive experiments, we show that the 2D Visual Prompt in our\nVP3D significantly eases the learning of visual appearance of 3D models and\nthus leads to higher visual fidelity with more detailed textures. It is also\nappealing in view that when replacing the self-generating visual prompt with a\ngiven reference image, VP3D is able to trigger a new task of stylized\ntext-to-3D generation. Our project page is available at\nhttps://vp3d-cvpr24.github.io.",
        "publication_date": "2024-03-25T17:59:31Z",
        "upvotes": 6
    },
    "2403.17887": {
        "url": "https://arxiv.org/abs/2403.17887",
        "title": "The Unreasonable Ineffectiveness of the Deeper Layers",
        "authors": [
            "Andrey Gromov",
            "Kushal Tirumala",
            "Hassan Shapourian",
            "Paolo Glorioso",
            "Daniel A. Roberts"
        ],
        "abstract": "We empirically study a simple layer-pruning strategy for popular families of\nopen-weight pretrained LLMs, finding minimal degradation of performance on\ndifferent question-answering benchmarks until after a large fraction (up to\nhalf) of the layers are removed. To prune these models, we identify the optimal\nblock of layers to prune by considering similarity across layers; then, to\n\"heal\" the damage, we perform a small amount of finetuning. In particular, we\nuse parameter-efficient finetuning (PEFT) methods, specifically quantization\nand Low Rank Adapters (QLoRA), such that each of our experiments can be\nperformed on a single A100 GPU. From a practical perspective, these results\nsuggest that layer pruning methods can complement other PEFT strategies to\nfurther reduce computational resources of finetuning on the one hand, and can\nimprove the memory and latency of inference on the other hand. From a\nscientific perspective, the robustness of these LLMs to the deletion of layers\nimplies either that current pretraining methods are not properly leveraging the\nparameters in the deeper layers of the network or that the shallow layers play\na critical role in storing knowledge.",
        "publication_date": "2024-03-26T17:20:04Z",
        "upvotes": 70
    },
    "2403.17888": {
        "url": "https://arxiv.org/abs/2403.17888",
        "title": "2D Gaussian Splatting for Geometrically Accurate Radiance Fields",
        "authors": [
            "Binbin Huang",
            "Zehao Yu",
            "Anpei Chen",
            "Andreas Geiger",
            "Shenghua Gao"
        ],
        "abstract": "3D Gaussian Splatting (3DGS) has recently revolutionized radiance field\nreconstruction, achieving high quality novel view synthesis and fast rendering\nspeed without baking. However, 3DGS fails to accurately represent surfaces due\nto the multi-view inconsistent nature of 3D Gaussians. We present 2D Gaussian\nSplatting (2DGS), a novel approach to model and reconstruct geometrically\naccurate radiance fields from multi-view images. Our key idea is to collapse\nthe 3D volume into a set of 2D oriented planar Gaussian disks. Unlike 3D\nGaussians, 2D Gaussians provide view-consistent geometry while modeling\nsurfaces intrinsically. To accurately recover thin surfaces and achieve stable\noptimization, we introduce a perspective-accurate 2D splatting process\nutilizing ray-splat intersection and rasterization. Additionally, we\nincorporate depth distortion and normal consistency terms to further enhance\nthe quality of the reconstructions. We demonstrate that our differentiable\nrenderer allows for noise-free and detailed geometry reconstruction while\nmaintaining competitive appearance quality, fast training speed, and real-time\nrendering. Our code will be made publicly available.",
        "publication_date": "2024-03-26T17:21:24Z",
        "upvotes": 24
    },
    "2403.17297": {
        "url": "https://arxiv.org/abs/2403.17297",
        "title": "InternLM2 Technical Report",
        "authors": [
            "Zheng Cai",
            "Maosong Cao",
            "Haojiong Chen",
            "Kai Chen",
            "Keyu Chen",
            "Xin Chen",
            "Xun Chen",
            "Zehui Chen",
            "Zhi Chen",
            "Pei Chu",
            "Xiaoyi Dong",
            "Haodong Duan",
            "Qi Fan",
            "Zhaoye Fei",
            "Yang Gao",
            "Jiaye Ge",
            "Chenya Gu",
            "Yuzhe Gu",
            "Tao Gui",
            "Aijia Guo",
            "Qipeng Guo",
            "Conghui He",
            "Yingfan Hu",
            "Ting Huang",
            "Tao Jiang",
            "Penglong Jiao",
            "Zhenjiang Jin",
            "Zhikai Lei",
            "Jiaxing Li",
            "Jingwen Li",
            "Linyang Li",
            "Shuaibin Li",
            "Wei Li",
            "Yining Li",
            "Hongwei Liu",
            "Jiangning Liu",
            "Jiawei Hong",
            "Kaiwen Liu",
            "Kuikun Liu",
            "Xiaoran Liu",
            "Chengqi Lv",
            "Haijun Lv",
            "Kai Lv",
            "Li Ma",
            "Runyuan Ma",
            "Zerun Ma",
            "Wenchang Ning",
            "Linke Ouyang",
            "Jiantao Qiu",
            "Yuan Qu",
            "Fukai Shang",
            "Yunfan Shao",
            "Demin Song",
            "Zifan Song",
            "Zhihao Sui",
            "Peng Sun",
            "Yu Sun",
            "Huanze Tang",
            "Bin Wang",
            "Guoteng Wang",
            "Jiaqi Wang",
            "Jiayu Wang",
            "Rui Wang",
            "Yudong Wang",
            "Ziyi Wang",
            "Xingjian Wei",
            "Qizhen Weng",
            "Fan Wu",
            "Yingtong Xiong",
            "Chao Xu",
            "Ruiliang Xu",
            "Hang Yan",
            "Yirong Yan",
            "Xiaogui Yang",
            "Haochen Ye",
            "Huaiyuan Ying",
            "Jia Yu",
            "Jing Yu",
            "Yuhang Zang",
            "Chuyu Zhang",
            "Li Zhang",
            "Pan Zhang",
            "Peng Zhang",
            "Ruijie Zhang",
            "Shuo Zhang",
            "Songyang Zhang",
            "Wenjian Zhang",
            "Wenwei Zhang",
            "Xingcheng Zhang",
            "Xinyue Zhang",
            "Hui Zhao",
            "Qian Zhao",
            "Xiaomeng Zhao",
            "Fengzhe Zhou",
            "Zaida Zhou",
            "Jingming Zhuo",
            "Yicheng Zou",
            "Xipeng Qiu",
            "Yu Qiao",
            "Dahua Lin"
        ],
        "abstract": "The evolution of Large Language Models (LLMs) like ChatGPT and GPT-4 has\nsparked discussions on the advent of Artificial General Intelligence (AGI).\nHowever, replicating such advancements in open-source models has been\nchallenging. This paper introduces InternLM2, an open-source LLM that\noutperforms its predecessors in comprehensive evaluations across 6 dimensions\nand 30 benchmarks, long-context modeling, and open-ended subjective evaluations\nthrough innovative pre-training and optimization techniques. The pre-training\nprocess of InternLM2 is meticulously detailed, highlighting the preparation of\ndiverse data types including text, code, and long-context data. InternLM2\nefficiently captures long-term dependencies, initially trained on 4k tokens\nbefore advancing to 32k tokens in pre-training and fine-tuning stages,\nexhibiting remarkable performance on the 200k ``Needle-in-a-Haystack\" test.\nInternLM2 is further aligned using Supervised Fine-Tuning (SFT) and a novel\nConditional Online Reinforcement Learning from Human Feedback (COOL RLHF)\nstrategy that addresses conflicting human preferences and reward hacking. By\nreleasing InternLM2 models in different training stages and model sizes, we\nprovide the community with insights into the model's evolution.",
        "publication_date": "2024-03-26T00:53:24Z",
        "upvotes": 23
    },
    "2403.17920": {
        "url": "https://arxiv.org/abs/2403.17920",
        "title": "TC4D: Trajectory-Conditioned Text-to-4D Generation",
        "authors": [
            "Sherwin Bahmani",
            "Xian Liu",
            "Yifan Wang",
            "Ivan Skorokhodov",
            "Victor Rong",
            "Ziwei Liu",
            "Xihui Liu",
            "Jeong Joon Park",
            "Sergey Tulyakov",
            "Gordon Wetzstein",
            "Andrea Tagliasacchi",
            "David B. Lindell"
        ],
        "abstract": "Recent techniques for text-to-4D generation synthesize dynamic 3D scenes\nusing supervision from pre-trained text-to-video models. However, existing\nrepresentations for motion, such as deformation models or time-dependent neural\nrepresentations, are limited in the amount of motion they can generate-they\ncannot synthesize motion extending far beyond the bounding box used for volume\nrendering. The lack of a more flexible motion model contributes to the gap in\nrealism between 4D generation methods and recent, near-photorealistic video\ngeneration models. Here, we propose TC4D: trajectory-conditioned text-to-4D\ngeneration, which factors motion into global and local components. We represent\nthe global motion of a scene's bounding box using rigid transformation along a\ntrajectory parameterized by a spline. We learn local deformations that conform\nto the global trajectory using supervision from a text-to-video model. Our\napproach enables the synthesis of scenes animated along arbitrary trajectories,\ncompositional scene generation, and significant improvements to the realism and\namount of generated motion, which we evaluate qualitatively and through a user\nstudy. Video results can be viewed on our website:\nhttps://sherwinbahmani.github.io/tc4d.",
        "publication_date": "2024-03-26T17:55:11Z",
        "upvotes": 15
    },
    "2403.17804": {
        "url": "https://arxiv.org/abs/2403.17804",
        "title": "Improving Text-to-Image Consistency via Automatic Prompt Optimization",
        "authors": [
            "Oscar Ma\u00f1as",
            "Pietro Astolfi",
            "Melissa Hall",
            "Candace Ross",
            "Jack Urbanek",
            "Adina Williams",
            "Aishwarya Agrawal",
            "Adriana Romero-Soriano",
            "Michal Drozdzal"
        ],
        "abstract": "Impressive advances in text-to-image (T2I) generative models have yielded a\nplethora of high performing models which are able to generate aesthetically\nappealing, photorealistic images. Despite the progress, these models still\nstruggle to produce images that are consistent with the input prompt,\noftentimes failing to capture object quantities, relations and attributes\nproperly. Existing solutions to improve prompt-image consistency suffer from\nthe following challenges: (1) they oftentimes require model fine-tuning, (2)\nthey only focus on nearby prompt samples, and (3) they are affected by\nunfavorable trade-offs among image quality, representation diversity, and\nprompt-image consistency. In this paper, we address these challenges and\nintroduce a T2I optimization-by-prompting framework, OPT2I, which leverages a\nlarge language model (LLM) to improve prompt-image consistency in T2I models.\nOur framework starts from a user prompt and iteratively generates revised\nprompts with the goal of maximizing a consistency score. Our extensive\nvalidation on two datasets, MSCOCO and PartiPrompts, shows that OPT2I can boost\nthe initial consistency score by up to 24.9% in terms of DSG score while\npreserving the FID and increasing the recall between generated and real data.\nOur work paves the way toward building more reliable and robust T2I systems by\nharnessing the power of LLMs.",
        "publication_date": "2024-03-26T15:42:01Z",
        "upvotes": 14
    },
    "2403.17898": {
        "url": "https://arxiv.org/abs/2403.17898",
        "title": "Octree-GS: Towards Consistent Real-time Rendering with LOD-Structured 3D\n  Gaussians",
        "authors": [
            "Kerui Ren",
            "Lihan Jiang",
            "Tao Lu",
            "Mulin Yu",
            "Linning Xu",
            "Zhangkai Ni",
            "Bo Dai"
        ],
        "abstract": "The recent 3D Gaussian splatting (3D-GS) has shown remarkable rendering\nfidelity and efficiency compared to NeRF-based neural scene representations.\nWhile demonstrating the potential for real-time rendering, 3D-GS encounters\nrendering bottlenecks in large scenes with complex details due to an excessive\nnumber of Gaussian primitives located within the viewing frustum. This\nlimitation is particularly noticeable in zoom-out views and can lead to\ninconsistent rendering speeds in scenes with varying details. Moreover, it\noften struggles to capture the corresponding level of details at different\nscales with its heuristic density control operation. Inspired by the\nLevel-of-Detail (LOD) techniques, we introduce Octree-GS, featuring an\nLOD-structured 3D Gaussian approach supporting level-of-detail decomposition\nfor scene representation that contributes to the final rendering results. Our\nmodel dynamically selects the appropriate level from the set of\nmulti-resolution anchor points, ensuring consistent rendering performance with\nadaptive LOD adjustments while maintaining high-fidelity rendering results.",
        "publication_date": "2024-03-26T17:39:36Z",
        "upvotes": 12
    },
    "2403.17694": {
        "url": "https://arxiv.org/abs/2403.17694",
        "title": "AniPortrait: Audio-Driven Synthesis of Photorealistic Portrait Animation",
        "authors": [
            "Huawei Wei",
            "Zejun Yang",
            "Zhisheng Wang"
        ],
        "abstract": "In this study, we propose AniPortrait, a novel framework for generating\nhigh-quality animation driven by audio and a reference portrait image. Our\nmethodology is divided into two stages. Initially, we extract 3D intermediate\nrepresentations from audio and project them into a sequence of 2D facial\nlandmarks. Subsequently, we employ a robust diffusion model, coupled with a\nmotion module, to convert the landmark sequence into photorealistic and\ntemporally consistent portrait animation. Experimental results demonstrate the\nsuperiority of AniPortrait in terms of facial naturalness, pose diversity, and\nvisual quality, thereby offering an enhanced perceptual experience. Moreover,\nour methodology exhibits considerable potential in terms of flexibility and\ncontrollability, which can be effectively applied in areas such as facial\nmotion editing or face reenactment. We release code and model weights at\nhttps://github.com/scutzzj/AniPortrait",
        "publication_date": "2024-03-26T13:35:02Z",
        "upvotes": 10
    },
    "2403.17237": {
        "url": "https://arxiv.org/abs/2403.17237",
        "title": "DreamPolisher: Towards High-Quality Text-to-3D Generation via Geometric\n  Diffusion",
        "authors": [
            "Yuanze Lin",
            "Ronald Clark",
            "Philip Torr"
        ],
        "abstract": "We present DreamPolisher, a novel Gaussian Splatting based method with\ngeometric guidance, tailored to learn cross-view consistency and intricate\ndetail from textual descriptions. While recent progress on text-to-3D\ngeneration methods have been promising, prevailing methods often fail to ensure\nview-consistency and textural richness. This problem becomes particularly\nnoticeable for methods that work with text input alone. To address this, we\npropose a two-stage Gaussian Splatting based approach that enforces geometric\nconsistency among views. Initially, a coarse 3D generation undergoes refinement\nvia geometric optimization. Subsequently, we use a ControlNet driven refiner\ncoupled with the geometric consistency term to improve both texture fidelity\nand overall consistency of the generated 3D asset. Empirical evaluations across\ndiverse textual prompts spanning various object categories demonstrate the\nefficacy of DreamPolisher in generating consistent and realistic 3D objects,\naligning closely with the semantics of the textual instructions.",
        "publication_date": "2024-03-25T22:34:05Z",
        "upvotes": 7
    },
    "2403.17607": {
        "url": "https://arxiv.org/abs/2403.17607",
        "title": "Fully-fused Multi-Layer Perceptrons on Intel Data Center GPUs",
        "authors": [
            "Kai Yuan",
            "Christoph Bauinger",
            "Xiangyi Zhang",
            "Pascal Baehr",
            "Matthias Kirchhart",
            "Darius Dabert",
            "Adrien Tousnakhoff",
            "Pierre Boudier",
            "Michael Paulitsch"
        ],
        "abstract": "This paper presents a SYCL implementation of Multi-Layer Perceptrons (MLPs),\nwhich targets and is optimized for the Intel Data Center GPU Max 1550. To\nincrease the performance, our implementation minimizes the slow global memory\naccesses by maximizing the data reuse within the general register file and the\nshared local memory by fusing the operations in each layer of the MLP. We show\nwith a simple roofline model that this results in a significant increase in the\narithmetic intensity, leading to improved performance, especially for\ninference. We compare our approach to a similar CUDA implementation for MLPs\nand show that our implementation on the Intel Data Center GPU outperforms the\nCUDA implementation on Nvidia's H100 GPU by a factor up to 2.84 in inference\nand 1.75 in training. The paper also showcases the efficiency of our SYCL\nimplementation in three significant areas: Image Compression, Neural Radiance\nFields, and Physics-Informed Machine Learning. In all cases, our implementation\noutperforms the off-the-shelf Intel Extension for PyTorch (IPEX) implementation\non the same Intel GPU by up to a factor of 30 and the CUDA PyTorch version on\nNvidia's H100 GPU by up to a factor 19. The code can be found at\nhttps://github.com/intel/tiny-dpcpp-nn.",
        "publication_date": "2024-03-26T11:38:39Z",
        "upvotes": 7
    },
    "2403.18361": {
        "url": "https://arxiv.org/abs/2403.18361",
        "title": "ViTAR: Vision Transformer with Any Resolution",
        "authors": [
            "Qihang Fan",
            "Quanzeng You",
            "Xiaotian Han",
            "Yongfei Liu",
            "Yunzhe Tao",
            "Huaibo Huang",
            "Ran He",
            "Hongxia Yang"
        ],
        "abstract": "This paper tackles a significant challenge faced by Vision Transformers\n(ViTs): their constrained scalability across different image resolutions.\nTypically, ViTs experience a performance decline when processing resolutions\ndifferent from those seen during training. Our work introduces two key\ninnovations to address this issue. Firstly, we propose a novel module for\ndynamic resolution adjustment, designed with a single Transformer block,\nspecifically to achieve highly efficient incremental token integration.\nSecondly, we introduce fuzzy positional encoding in the Vision Transformer to\nprovide consistent positional awareness across multiple resolutions, thereby\npreventing overfitting to any single training resolution. Our resulting model,\nViTAR (Vision Transformer with Any Resolution), demonstrates impressive\nadaptability, achieving 83.3\\% top-1 accuracy at a 1120x1120 resolution and\n80.4\\% accuracy at a 4032x4032 resolution, all while reducing computational\ncosts. ViTAR also shows strong performance in downstream tasks such as instance\nand semantic segmentation and can easily combined with self-supervised learning\ntechniques like Masked AutoEncoder. Our work provides a cost-effective solution\nfor enhancing the resolution scalability of ViTs, paving the way for more\nversatile and efficient high-resolution image processing.",
        "publication_date": "2024-03-27T08:53:13Z",
        "upvotes": 44
    },
    "2403.18814": {
        "url": "https://arxiv.org/abs/2403.18814",
        "title": "Mini-Gemini: Mining the Potential of Multi-modality Vision Language\n  Models",
        "authors": [
            "Yanwei Li",
            "Yuechen Zhang",
            "Chengyao Wang",
            "Zhisheng Zhong",
            "Yixin Chen",
            "Ruihang Chu",
            "Shaoteng Liu",
            "Jiaya Jia"
        ],
        "abstract": "In this work, we introduce Mini-Gemini, a simple and effective framework\nenhancing multi-modality Vision Language Models (VLMs). Despite the\nadvancements in VLMs facilitating basic visual dialog and reasoning, a\nperformance gap persists compared to advanced models like GPT-4 and Gemini. We\ntry to narrow the gap by mining the potential of VLMs for better performance\nand any-to-any workflow from three aspects, i.e., high-resolution visual\ntokens, high-quality data, and VLM-guided generation. To enhance visual tokens,\nwe propose to utilize an additional visual encoder for high-resolution\nrefinement without increasing the visual token count. We further construct a\nhigh-quality dataset that promotes precise image comprehension and\nreasoning-based generation, expanding the operational scope of current VLMs. In\ngeneral, Mini-Gemini further mines the potential of VLMs and empowers current\nframeworks with image understanding, reasoning, and generation simultaneously.\nMini-Gemini supports a series of dense and MoE Large Language Models (LLMs)\nfrom 2B to 34B. It is demonstrated to achieve leading performance in several\nzero-shot benchmarks and even surpasses the developed private models. Code and\nmodels are available at https://github.com/dvlab-research/MiniGemini.",
        "publication_date": "2024-03-27T17:59:04Z",
        "upvotes": 33
    },
    "2403.18818": {
        "url": "https://arxiv.org/abs/2403.18818",
        "title": "ObjectDrop: Bootstrapping Counterfactuals for Photorealistic Object\n  Removal and Insertion",
        "authors": [
            "Daniel Winter",
            "Matan Cohen",
            "Shlomi Fruchter",
            "Yael Pritch",
            "Alex Rav-Acha",
            "Yedid Hoshen"
        ],
        "abstract": "Diffusion models have revolutionized image editing but often generate images\nthat violate physical laws, particularly the effects of objects on the scene,\ne.g., occlusions, shadows, and reflections. By analyzing the limitations of\nself-supervised approaches, we propose a practical solution centered on a\n\\q{counterfactual} dataset. Our method involves capturing a scene before and\nafter removing a single object, while minimizing other changes. By fine-tuning\na diffusion model on this dataset, we are able to not only remove objects but\nalso their effects on the scene. However, we find that applying this approach\nfor photorealistic object insertion requires an impractically large dataset. To\ntackle this challenge, we propose bootstrap supervision; leveraging our object\nremoval model trained on a small counterfactual dataset, we synthetically\nexpand this dataset considerably. Our approach significantly outperforms prior\nmethods in photorealistic object removal and insertion, particularly at\nmodeling the effects of objects on the scene.",
        "publication_date": "2024-03-27T17:59:52Z",
        "upvotes": 22
    },
    "2403.18802": {
        "url": "https://arxiv.org/abs/2403.18802",
        "title": "Long-form factuality in large language models",
        "authors": [
            "Jerry Wei",
            "Chengrun Yang",
            "Xinying Song",
            "Yifeng Lu",
            "Nathan Hu",
            "Jie Huang",
            "Dustin Tran",
            "Daiyi Peng",
            "Ruibo Liu",
            "Da Huang",
            "Cosmo Du",
            "Quoc V. Le"
        ],
        "abstract": "Large language models (LLMs) often generate content that contains factual\nerrors when responding to fact-seeking prompts on open-ended topics. To\nbenchmark a model's long-form factuality in open domains, we first use GPT-4 to\ngenerate LongFact, a prompt set comprising thousands of questions spanning 38\ntopics. We then propose that LLM agents can be used as automated evaluators for\nlong-form factuality through a method which we call Search-Augmented Factuality\nEvaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into\na set of individual facts and to evaluate the accuracy of each fact using a\nmulti-step reasoning process comprising sending search queries to Google Search\nand determining whether a fact is supported by the search results. Furthermore,\nwe propose extending F1 score as an aggregated metric for long-form factuality.\nTo do so, we balance the percentage of supported facts in a response\n(precision) with the percentage of provided facts relative to a hyperparameter\nrepresenting a user's preferred response length (recall).\n  Empirically, we demonstrate that LLM agents can outperform crowdsourced human\nannotators - on a set of ~16k individual facts, SAFE agrees with crowdsourced\nhuman annotators 72% of the time, and on a random subset of 100 disagreement\ncases, SAFE wins 76% of the time. At the same time, SAFE is more than 20 times\ncheaper than human annotators. We also benchmark thirteen language models on\nLongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding\nthat larger language models generally achieve better long-form factuality.\nLongFact, SAFE, and all experimental code are available at\nhttps://github.com/google-deepmind/long-form-factuality.",
        "publication_date": "2024-03-27T17:48:55Z",
        "upvotes": 22
    },
    "2403.18421": {
        "url": "https://arxiv.org/abs/2403.18421",
        "title": "BioMedLM: A 2.7B Parameter Language Model Trained On Biomedical Text",
        "authors": [
            "Elliot Bolton",
            "Abhinav Venigalla",
            "Michihiro Yasunaga",
            "David Hall",
            "Betty Xiong",
            "Tony Lee",
            "Roxana Daneshjou",
            "Jonathan Frankle",
            "Percy Liang",
            "Michael Carbin",
            "Christopher D. Manning"
        ],
        "abstract": "Models such as GPT-4 and Med-PaLM 2 have demonstrated impressive performance\non a wide variety of biomedical NLP tasks. However, these models have hundreds\nof billions of parameters, are computationally expensive to run, require users\nto send their input data over the internet, and are trained on unknown data\nsources. Can smaller, more targeted models compete? To address this question,\nwe build and release BioMedLM, a 2.7 billion parameter GPT-style autoregressive\nmodel trained exclusively on PubMed abstracts and full articles. When\nfine-tuned, BioMedLM can produce strong multiple-choice biomedical\nquestion-answering results competitive with much larger models, such as\nachieving a score of 57.3% on MedMCQA (dev) and 69.0% on the MMLU Medical\nGenetics exam. BioMedLM can also be fine-tuned to produce useful answers to\npatient questions on medical topics. This demonstrates that smaller models can\npotentially serve as transparent, privacy-preserving, economical and\nenvironmentally friendly foundations for particular NLP applications, such as\nin biomedicine. The model is available on the Hugging Face Hub:\nhttps://huggingface.co/stanford-crfm/BioMedLM.",
        "publication_date": "2024-03-27T10:18:21Z",
        "upvotes": 20
    },
    "2403.18795": {
        "url": "https://arxiv.org/abs/2403.18795",
        "title": "Gamba: Marry Gaussian Splatting with Mamba for single view 3D\n  reconstruction",
        "authors": [
            "Qiuhong Shen",
            "Xuanyu Yi",
            "Zike Wu",
            "Pan Zhou",
            "Hanwang Zhang",
            "Shuicheng Yan",
            "Xinchao Wang"
        ],
        "abstract": "We tackle the challenge of efficiently reconstructing a 3D asset from a\nsingle image with growing demands for automated 3D content creation pipelines.\nPrevious methods primarily rely on Score Distillation Sampling (SDS) and Neural\nRadiance Fields (NeRF). Despite their significant success, these approaches\nencounter practical limitations due to lengthy optimization and considerable\nmemory usage. In this report, we introduce Gamba, an end-to-end amortized 3D\nreconstruction model from single-view images, emphasizing two main insights:\n(1) 3D representation: leveraging a large number of 3D Gaussians for an\nefficient 3D Gaussian splatting process; (2) Backbone design: introducing a\nMamba-based sequential network that facilitates context-dependent reasoning and\nlinear scalability with the sequence (token) length, accommodating a\nsubstantial number of Gaussians. Gamba incorporates significant advancements in\ndata preprocessing, regularization design, and training methodologies. We\nassessed Gamba against existing optimization-based and feed-forward 3D\ngeneration approaches using the real-world scanned OmniObject3D dataset. Here,\nGamba demonstrates competitive generation capabilities, both qualitatively and\nquantitatively, while achieving remarkable speed, approximately 0.6 second on a\nsingle NVIDIA A100 GPU.",
        "publication_date": "2024-03-27T17:40:14Z",
        "upvotes": 16
    },
    "2403.18816": {
        "url": "https://arxiv.org/abs/2403.18816",
        "title": "Garment3DGen: 3D Garment Stylization and Texture Generation",
        "authors": [
            "Nikolaos Sarafianos",
            "Tuur Stuyck",
            "Xiaoyu Xiang",
            "Yilei Li",
            "Jovan Popovic",
            "Rakesh Ranjan"
        ],
        "abstract": "We introduce Garment3DGen a new method to synthesize 3D garment assets from a\nbase mesh given a single input image as guidance. Our proposed approach allows\nusers to generate 3D textured clothes based on both real and synthetic images,\nsuch as those generated by text prompts. The generated assets can be directly\ndraped and simulated on human bodies. First, we leverage the recent progress of\nimage to 3D diffusion methods to generate 3D garment geometries. However, since\nthese geometries cannot be utilized directly for downstream tasks, we propose\nto use them as pseudo ground-truth and set up a mesh deformation optimization\nprocedure that deforms a base template mesh to match the generated 3D target.\nSecond, we introduce carefully designed losses that allow the input base mesh\nto freely deform towards the desired target, yet preserve mesh quality and\ntopology such that they can be simulated. Finally, a texture estimation module\ngenerates high-fidelity texture maps that are globally and locally consistent\nand faithfully capture the input guidance, allowing us to render the generated\n3D assets. With Garment3DGen users can generate the textured 3D garment of\ntheir choice without the need of artist intervention. One can provide a textual\nprompt describing the garment they desire to generate a simulation-ready 3D\nasset. We present a plethora of quantitative and qualitative comparisons on\nvarious assets both real and generated and provide use-cases of how one can\ngenerate simulation-ready 3D garments.",
        "publication_date": "2024-03-27T17:59:33Z",
        "upvotes": 15
    },
    "2403.18118": {
        "url": "https://arxiv.org/abs/2403.18118",
        "title": "EgoLifter: Open-world 3D Segmentation for Egocentric Perception",
        "authors": [
            "Qiao Gu",
            "Zhaoyang Lv",
            "Duncan Frost",
            "Simon Green",
            "Julian Straub",
            "Chris Sweeney"
        ],
        "abstract": "In this paper we present EgoLifter, a novel system that can automatically\nsegment scenes captured from egocentric sensors into a complete decomposition\nof individual 3D objects. The system is specifically designed for egocentric\ndata where scenes contain hundreds of objects captured from natural\n(non-scanning) motion. EgoLifter adopts 3D Gaussians as the underlying\nrepresentation of 3D scenes and objects and uses segmentation masks from the\nSegment Anything Model (SAM) as weak supervision to learn flexible and\npromptable definitions of object instances free of any specific object\ntaxonomy. To handle the challenge of dynamic objects in ego-centric videos, we\ndesign a transient prediction module that learns to filter out dynamic objects\nin the 3D reconstruction. The result is a fully automatic pipeline that is able\nto reconstruct 3D object instances as collections of 3D Gaussians that\ncollectively compose the entire scene. We created a new benchmark on the Aria\nDigital Twin dataset that quantitatively demonstrates its state-of-the-art\nperformance in open-world 3D segmentation from natural egocentric input. We run\nEgoLifter on various egocentric activity datasets which shows the promise of\nthe method for 3D egocentric perception at scale.",
        "publication_date": "2024-03-26T21:48:27Z",
        "upvotes": 7
    },
    "2403.18605": {
        "url": "https://arxiv.org/abs/2403.18605",
        "title": "FlexEdit: Flexible and Controllable Diffusion-based Object-centric Image\n  Editing",
        "authors": [
            "Trong-Tung Nguyen",
            "Duc-Anh Nguyen",
            "Anh Tran",
            "Cuong Pham"
        ],
        "abstract": "Our work addresses limitations seen in previous approaches for object-centric\nediting problems, such as unrealistic results due to shape discrepancies and\nlimited control in object replacement or insertion. To this end, we introduce\nFlexEdit, a flexible and controllable editing framework for objects where we\niteratively adjust latents at each denoising step using our FlexEdit block.\nInitially, we optimize latents at test time to align with specified object\nconstraints. Then, our framework employs an adaptive mask, automatically\nextracted during denoising, to protect the background while seamlessly blending\nnew content into the target image. We demonstrate the versatility of FlexEdit\nin various object editing tasks and curate an evaluation test suite with\nsamples from both real and synthetic images, along with novel evaluation\nmetrics designed for object-centric editing. We conduct extensive experiments\non different editing scenarios, demonstrating the superiority of our editing\nframework over recent advanced text-guided image editing methods. Our project\npage is published at https://flex-edit.github.io/.",
        "publication_date": "2024-03-27T14:24:30Z",
        "upvotes": 5
    },
    "2403.18783": {
        "url": "https://arxiv.org/abs/2403.18783",
        "title": "Towards a World-English Language Model for On-Device Virtual Assistants",
        "authors": [
            "Rricha Jalota",
            "Lyan Verwimp",
            "Markus Nussbaum-Thom",
            "Amr Mousa",
            "Arturo Argueta",
            "Youssef Oualil"
        ],
        "abstract": "Neural Network Language Models (NNLMs) for Virtual Assistants (VAs) are\ngenerally language-, region-, and in some cases, device-dependent, which\nincreases the effort to scale and maintain them. Combining NNLMs for one or\nmore of the categories is one way to improve scalability. In this work, we\ncombine regional variants of English to build a ``World English'' NNLM for\non-device VAs. In particular, we investigate the application of adapter\nbottlenecks to model dialect-specific characteristics in our existing\nproduction NNLMs {and enhance the multi-dialect baselines}. We find that\nadapter modules are more effective in modeling dialects than specializing\nentire sub-networks. Based on this insight and leveraging the design of our\nproduction models, we introduce a new architecture for World English NNLM that\nmeets the accuracy, latency, and memory constraints of our single-dialect\nmodels.",
        "publication_date": "2024-03-27T17:31:39Z",
        "upvotes": 4
    },
    "2403.19270": {
        "url": "https://arxiv.org/abs/2403.19270",
        "title": "sDPO: Don't Use Your Data All at Once",
        "authors": [
            "Dahyun Kim",
            "Yungi Kim",
            "Wonho Song",
            "Hyeonwoo Kim",
            "Yunsu Kim",
            "Sanghoon Kim",
            "Chanjun Park"
        ],
        "abstract": "As development of large language models (LLM) progresses, aligning them with\nhuman preferences has become increasingly important. We propose stepwise DPO\n(sDPO), an extension of the recently popularized direct preference optimization\n(DPO) for alignment tuning. This approach involves dividing the available\npreference datasets and utilizing them in a stepwise manner, rather than\nemploying it all at once. We demonstrate that this method facilitates the use\nof more precisely aligned reference models within the DPO training framework.\nFurthermore, sDPO trains the final model to be more performant, even\noutperforming other popular LLMs with more parameters.",
        "publication_date": "2024-03-28T09:56:04Z",
        "upvotes": 27
    },
    "2403.19046": {
        "url": "https://arxiv.org/abs/2403.19046",
        "title": "LITA: Language Instructed Temporal-Localization Assistant",
        "authors": [
            "De-An Huang",
            "Shijia Liao",
            "Subhashree Radhakrishnan",
            "Hongxu Yin",
            "Pavlo Molchanov",
            "Zhiding Yu",
            "Jan Kautz"
        ],
        "abstract": "There has been tremendous progress in multimodal Large Language Models\n(LLMs). Recent works have extended these models to video input with promising\ninstruction following capabilities. However, an important missing piece is\ntemporal localization. These models cannot accurately answer the \"When?\"\nquestions. We identify three key aspects that limit their temporal localization\ncapabilities: (i) time representation, (ii) architecture, and (iii) data. We\naddress these shortcomings by proposing Language Instructed\nTemporal-Localization Assistant (LITA) with the following features: (1) We\nintroduce time tokens that encode timestamps relative to the video length to\nbetter represent time in videos. (2) We introduce SlowFast tokens in the\narchitecture to capture temporal information at fine temporal resolution. (3)\nWe emphasize temporal localization data for LITA. In addition to leveraging\nexisting video datasets with timestamps, we propose a new task, Reasoning\nTemporal Localization (RTL), along with the dataset, ActivityNet-RTL, for\nlearning and evaluating this task. Reasoning temporal localization requires\nboth the reasoning and temporal localization of Video LLMs. LITA demonstrates\nstrong performance on this challenging task, nearly doubling the temporal mean\nintersection-over-union (mIoU) of baselines. In addition, we show that our\nemphasis on temporal localization also substantially improves video-based text\ngeneration compared to existing Video LLMs, including a 36% relative\nimprovement of Temporal Understanding. Code is available at:\nhttps://github.com/NVlabs/LITA",
        "publication_date": "2024-03-27T22:50:48Z",
        "upvotes": 16
    },
    "2403.19655": {
        "url": "https://arxiv.org/abs/2403.19655",
        "title": "GaussianCube: Structuring Gaussian Splatting using Optimal Transport for\n  3D Generative Modeling",
        "authors": [
            "Bowen Zhang",
            "Yiji Cheng",
            "Jiaolong Yang",
            "Chunyu Wang",
            "Feng Zhao",
            "Yansong Tang",
            "Dong Chen",
            "Baining Guo"
        ],
        "abstract": "3D Gaussian Splatting (GS) have achieved considerable improvement over Neural\nRadiance Fields in terms of 3D fitting fidelity and rendering speed. However,\nthis unstructured representation with scattered Gaussians poses a significant\nchallenge for generative modeling. To address the problem, we introduce\nGaussianCube, a structured GS representation that is both powerful and\nefficient for generative modeling. We achieve this by first proposing a\nmodified densification-constrained GS fitting algorithm which can yield\nhigh-quality fitting results using a fixed number of free Gaussians, and then\nre-arranging the Gaussians into a predefined voxel grid via Optimal Transport.\nThe structured grid representation allows us to use standard 3D U-Net as our\nbackbone in diffusion generative modeling without elaborate designs. Extensive\nexperiments conducted on ShapeNet and OmniObject3D show that our model achieves\nstate-of-the-art generation results both qualitatively and quantitatively,\nunderscoring the potential of GaussianCube as a powerful and versatile 3D\nrepresentation.",
        "publication_date": "2024-03-28T17:59:50Z",
        "upvotes": 14
    },
    "2403.18978": {
        "url": "https://arxiv.org/abs/2403.18978",
        "title": "TextCraftor: Your Text Encoder Can be Image Quality Controller",
        "authors": [
            "Yanyu Li",
            "Xian Liu",
            "Anil Kag",
            "Ju Hu",
            "Yerlan Idelbayev",
            "Dhritiman Sagar",
            "Yanzhi Wang",
            "Sergey Tulyakov",
            "Jian Ren"
        ],
        "abstract": "Diffusion-based text-to-image generative models, e.g., Stable Diffusion, have\nrevolutionized the field of content generation, enabling significant\nadvancements in areas like image editing and video synthesis. Despite their\nformidable capabilities, these models are not without their limitations. It is\nstill challenging to synthesize an image that aligns well with the input text,\nand multiple runs with carefully crafted prompts are required to achieve\nsatisfactory results. To mitigate these limitations, numerous studies have\nendeavored to fine-tune the pre-trained diffusion models, i.e., UNet, utilizing\nvarious technologies. Yet, amidst these efforts, a pivotal question of\ntext-to-image diffusion model training has remained largely unexplored: Is it\npossible and feasible to fine-tune the text encoder to improve the performance\nof text-to-image diffusion models? Our findings reveal that, instead of\nreplacing the CLIP text encoder used in Stable Diffusion with other large\nlanguage models, we can enhance it through our proposed fine-tuning approach,\nTextCraftor, leading to substantial improvements in quantitative benchmarks and\nhuman assessments. Interestingly, our technique also empowers controllable\nimage generation through the interpolation of different text encoders\nfine-tuned with various rewards. We also demonstrate that TextCraftor is\northogonal to UNet finetuning, and can be combined to further improve\ngenerative quality.",
        "publication_date": "2024-03-27T19:52:55Z",
        "upvotes": 12
    },
    "2403.19319": {
        "url": "https://arxiv.org/abs/2403.19319",
        "title": "Mesh2NeRF: Direct Mesh Supervision for Neural Radiance Field\n  Representation and Generation",
        "authors": [
            "Yujin Chen",
            "Yinyu Nie",
            "Benjamin Ummenhofer",
            "Reiner Birkl",
            "Michael Paulitsch",
            "Matthias M\u00fcller",
            "Matthias Nie\u00dfner"
        ],
        "abstract": "We present Mesh2NeRF, an approach to derive ground-truth radiance fields from\ntextured meshes for 3D generation tasks. Many 3D generative approaches\nrepresent 3D scenes as radiance fields for training. Their ground-truth\nradiance fields are usually fitted from multi-view renderings from a\nlarge-scale synthetic 3D dataset, which often results in artifacts due to\nocclusions or under-fitting issues. In Mesh2NeRF, we propose an analytic\nsolution to directly obtain ground-truth radiance fields from 3D meshes,\ncharacterizing the density field with an occupancy function featuring a defined\nsurface thickness, and determining view-dependent color through a reflection\nfunction considering both the mesh and environment lighting. Mesh2NeRF extracts\naccurate radiance fields which provides direct supervision for training\ngenerative NeRFs and single scene representation. We validate the effectiveness\nof Mesh2NeRF across various tasks, achieving a noteworthy 3.12dB improvement in\nPSNR for view synthesis in single scene representation on the ABO dataset, a\n0.69 PSNR enhancement in the single-view conditional generation of ShapeNet\nCars, and notably improved mesh extraction from NeRF in the unconditional\ngeneration of Objaverse Mugs.",
        "publication_date": "2024-03-28T11:22:53Z",
        "upvotes": 6
    },
    "2403.19887": {
        "url": "https://arxiv.org/abs/2403.19887",
        "title": "Jamba: A Hybrid Transformer-Mamba Language Model",
        "authors": [
            "Opher Lieber",
            "Barak Lenz",
            "Hofit Bata",
            "Gal Cohen",
            "Jhonathan Osin",
            "Itay Dalmedigos",
            "Erez Safahi",
            "Shaked Meirom",
            "Yonatan Belinkov",
            "Shai Shalev-Shwartz",
            "Omri Abend",
            "Raz Alon",
            "Tomer Asida",
            "Amir Bergman",
            "Roman Glozman",
            "Michael Gokhman",
            "Avashalom Manevich",
            "Nir Ratner",
            "Noam Rozen",
            "Erez Shwartz",
            "Mor Zusman",
            "Yoav Shoham"
        ],
        "abstract": "We present Jamba, a new base large language model based on a novel hybrid\nTransformer-Mamba mixture-of-experts (MoE) architecture. Specifically, Jamba\ninterleaves blocks of Transformer and Mamba layers, enjoying the benefits of\nboth model families. MoE is added in some of these layers to increase model\ncapacity while keeping active parameter usage manageable. This flexible\narchitecture allows resource- and objective-specific configurations. In the\nparticular configuration we have implemented, we end up with a powerful model\nthat fits in a single 80GB GPU. Built at large scale, Jamba provides high\nthroughput and small memory footprint compared to vanilla Transformers, and at\nthe same time state-of-the-art performance on standard language model\nbenchmarks and long-context evaluations. Remarkably, the model presents strong\nresults for up to 256K tokens context length. We study various architectural\ndecisions, such as how to combine Transformer and Mamba layers, and how to mix\nexperts, and show that some of them are crucial in large scale modeling. We\nalso describe several interesting properties of these architectures which the\ntraining and evaluation of Jamba have revealed, and plan to release checkpoints\nfrom various ablation runs, to encourage further exploration of this novel\narchitecture. We make the weights of our implementation of Jamba publicly\navailable under a permissive license.",
        "publication_date": "2024-03-28T23:55:06Z",
        "upvotes": 85
    },
    "2403.20327": {
        "url": "https://arxiv.org/abs/2403.20327",
        "title": "Gecko: Versatile Text Embeddings Distilled from Large Language Models",
        "authors": [
            "Jinhyuk Lee",
            "Zhuyun Dai",
            "Xiaoqi Ren",
            "Blair Chen",
            "Daniel Cer",
            "Jeremy R. Cole",
            "Kai Hui",
            "Michael Boratko",
            "Rajvi Kapadia",
            "Wen Ding",
            "Yi Luan",
            "Sai Meher Karthik Duddu",
            "Gustavo Hernandez Abrego",
            "Weiqiang Shi",
            "Nithi Gupta",
            "Aditya Kusupati",
            "Prateek Jain",
            "Siddhartha Reddy Jonnalagadda",
            "Ming-Wei Chang",
            "Iftekhar Naim"
        ],
        "abstract": "We present Gecko, a compact and versatile text embedding model. Gecko\nachieves strong retrieval performance by leveraging a key idea: distilling\nknowledge from large language models (LLMs) into a retriever. Our two-step\ndistillation process begins with generating diverse, synthetic paired data\nusing an LLM. Next, we further refine the data quality by retrieving a set of\ncandidate passages for each query, and relabeling the positive and hard\nnegative passages using the same LLM. The effectiveness of our approach is\ndemonstrated by the compactness of the Gecko. On the Massive Text Embedding\nBenchmark (MTEB), Gecko with 256 embedding dimensions outperforms all existing\nentries with 768 embedding size. Gecko with 768 embedding dimensions achieves\nan average score of 66.31, competing with 7x larger models and 5x higher\ndimensional embeddings.",
        "publication_date": "2024-03-29T17:56:40Z",
        "upvotes": 38
    },
    "2403.20041": {
        "url": "https://arxiv.org/abs/2403.20041",
        "title": "Transformer-Lite: High-efficiency Deployment of Large Language Models on\n  Mobile Phone GPUs",
        "authors": [
            "Luchang Li",
            "Sheng Qian",
            "Jie Lu",
            "Lunxi Yuan",
            "Rui Wang",
            "Qin Xie"
        ],
        "abstract": "The Large Language Model (LLM) is widely employed for tasks such as\nintelligent assistants, text summarization, translation, and multi-modality on\nmobile phones. However, the current methods for on-device LLM deployment\nmaintain slow inference speed, which causes poor user experience. To facilitate\nhigh-efficiency LLM deployment on device GPUs, we propose four optimization\ntechniques: (a) a symbolic expression-based approach to support dynamic shape\nmodel inference; (b) operator optimizations and execution priority setting to\nenhance inference speed and reduce phone lagging; (c) an FP4 quantization\nmethod termed M0E4 to reduce dequantization overhead; (d) a sub-tensor-based\ntechnique to eliminate the need for copying KV cache after LLM inference.\nFurthermore, we implement these methods in our mobile inference engine,\nTransformer-Lite, which is compatible with both Qualcomm and MTK processors. We\nevaluated Transformer-Lite's performance using LLMs with varied architectures\nand parameters ranging from 2B to 14B. Specifically, we achieved prefill and\ndecoding speeds of 121 token/s and 14 token/s for ChatGLM2 6B, and 330 token/s\nand 30 token/s for smaller Gemma 2B, respectively. Compared with CPU-based\nFastLLM and GPU-based MLC-LLM, our engine attains over 10x speedup for the\nprefill speed and 2~3x speedup for the decoding speed.",
        "publication_date": "2024-03-29T08:26:53Z",
        "upvotes": 33
    },
    "2403.20329": {
        "url": "https://arxiv.org/abs/2403.20329",
        "title": "ReALM: Reference Resolution As Language Modeling",
        "authors": [
            "Joel Ruben Antony Moniz",
            "Soundarya Krishnan",
            "Melis Ozyildirim",
            "Prathamesh Saraf",
            "Halim Cagri Ates",
            "Yuan Zhang",
            "Hong Yu",
            "Nidhi Rajshree"
        ],
        "abstract": "Reference resolution is an important problem, one that is essential to\nunderstand and successfully handle context of different kinds. This context\nincludes both previous turns and context that pertains to non-conversational\nentities, such as entities on the user's screen or those running in the\nbackground. While LLMs have been shown to be extremely powerful for a variety\nof tasks, their use in reference resolution, particularly for\nnon-conversational entities, remains underutilized. This paper demonstrates how\nLLMs can be used to create an extremely effective system to resolve references\nof various types, by showing how reference resolution can be converted into a\nlanguage modeling problem, despite involving forms of entities like those on\nscreen that are not traditionally conducive to being reduced to a text-only\nmodality. We demonstrate large improvements over an existing system with\nsimilar functionality across different types of references, with our smallest\nmodel obtaining absolute gains of over 5% for on-screen references. We also\nbenchmark against GPT-3.5 and GPT-4, with our smallest model achieving\nperformance comparable to that of GPT-4, and our larger models substantially\noutperforming it.",
        "publication_date": "2024-03-29T17:59:06Z",
        "upvotes": 18
    },
    "2403.20309": {
        "url": "https://arxiv.org/abs/2403.20309",
        "title": "InstantSplat: Unbounded Sparse-view Pose-free Gaussian Splatting in 40\n  Seconds",
        "authors": [
            "Zhiwen Fan",
            "Wenyan Cong",
            "Kairun Wen",
            "Kevin Wang",
            "Jian Zhang",
            "Xinghao Ding",
            "Danfei Xu",
            "Boris Ivanovic",
            "Marco Pavone",
            "Georgios Pavlakos",
            "Zhangyang Wang",
            "Yue Wang"
        ],
        "abstract": "While novel view synthesis (NVS) has made substantial progress in 3D computer\nvision, it typically requires an initial estimation of camera intrinsics and\nextrinsics from dense viewpoints. This pre-processing is usually conducted via\na Structure-from-Motion (SfM) pipeline, a procedure that can be slow and\nunreliable, particularly in sparse-view scenarios with insufficient matched\nfeatures for accurate reconstruction. In this work, we integrate the strengths\nof point-based representations (e.g., 3D Gaussian Splatting, 3D-GS) with\nend-to-end dense stereo models (DUSt3R) to tackle the complex yet unresolved\nissues in NVS under unconstrained settings, which encompasses pose-free and\nsparse view challenges. Our framework, InstantSplat, unifies dense stereo\npriors with 3D-GS to build 3D Gaussians of large-scale scenes from sparseview &\npose-free images in less than 1 minute. Specifically, InstantSplat comprises a\nCoarse Geometric Initialization (CGI) module that swiftly establishes a\npreliminary scene structure and camera parameters across all training views,\nutilizing globally-aligned 3D point maps derived from a pre-trained dense\nstereo pipeline. This is followed by the Fast 3D-Gaussian Optimization (F-3DGO)\nmodule, which jointly optimizes the 3D Gaussian attributes and the initialized\nposes with pose regularization. Experiments conducted on the large-scale\noutdoor Tanks & Temples datasets demonstrate that InstantSplat significantly\nimproves SSIM (by 32%) while concurrently reducing Absolute Trajectory Error\n(ATE) by 80%. These establish InstantSplat as a viable solution for scenarios\ninvolving posefree and sparse-view conditions. Project page:\ninstantsplat.github.io.",
        "publication_date": "2024-03-29T17:29:58Z",
        "upvotes": 14
    },
    "2403.20331": {
        "url": "https://arxiv.org/abs/2403.20331",
        "title": "Unsolvable Problem Detection: Evaluating Trustworthiness of Vision\n  Language Models",
        "authors": [
            "Atsuyuki Miyai",
            "Jingkang Yang",
            "Jingyang Zhang",
            "Yifei Ming",
            "Qing Yu",
            "Go Irie",
            "Yixuan Li",
            "Hai Li",
            "Ziwei Liu",
            "Kiyoharu Aizawa"
        ],
        "abstract": "This paper introduces a novel and significant challenge for Vision Language\nModels (VLMs), termed Unsolvable Problem Detection (UPD). UPD examines the\nVLM's ability to withhold answers when faced with unsolvable problems in the\ncontext of Visual Question Answering (VQA) tasks. UPD encompasses three\ndistinct settings: Absent Answer Detection (AAD), Incompatible Answer Set\nDetection (IASD), and Incompatible Visual Question Detection (IVQD). To deeply\ninvestigate the UPD problem, extensive experiments indicate that most VLMs,\nincluding GPT-4V and LLaVA-Next-34B, struggle with our benchmarks to varying\nextents, highlighting significant room for the improvements. To address UPD, we\nexplore both training-free and training-based solutions, offering new insights\ninto their effectiveness and limitations. We hope our insights, together with\nfuture efforts within the proposed UPD settings, will enhance the broader\nunderstanding and development of more practical and reliable VLMs.",
        "publication_date": "2024-03-29T17:59:53Z",
        "upvotes": 14
    },
    "2403.19851": {
        "url": "https://arxiv.org/abs/2403.19851",
        "title": "Localizing Paragraph Memorization in Language Models",
        "authors": [
            "Niklas Stoehr",
            "Mitchell Gordon",
            "Chiyuan Zhang",
            "Owen Lewis"
        ],
        "abstract": "Can we localize the weights and mechanisms used by a language model to\nmemorize and recite entire paragraphs of its training data? In this paper, we\nshow that while memorization is spread across multiple layers and model\ncomponents, gradients of memorized paragraphs have a distinguishable spatial\npattern, being larger in lower model layers than gradients of non-memorized\nexamples. Moreover, the memorized examples can be unlearned by fine-tuning only\nthe high-gradient weights. We localize a low-layer attention head that appears\nto be especially involved in paragraph memorization. This head is predominantly\nfocusing its attention on distinctive, rare tokens that are least frequent in a\ncorpus-level unigram distribution. Next, we study how localized memorization is\nacross the tokens in the prefix by perturbing tokens and measuring the caused\nchange in the decoding. A few distinctive tokens early in a prefix can often\ncorrupt the entire continuation. Overall, memorized continuations are not only\nharder to unlearn, but also to corrupt than non-memorized ones.",
        "publication_date": "2024-03-28T21:53:24Z",
        "upvotes": 13
    },
    "2403.19928": {
        "url": "https://arxiv.org/abs/2403.19928",
        "title": "DiJiang: Efficient Large Language Models through Compact Kernelization",
        "authors": [
            "Hanting Chen",
            "Zhicheng Liu",
            "Xutao Wang",
            "Yuchuan Tian",
            "Yunhe Wang"
        ],
        "abstract": "In an effort to reduce the computational load of Transformers, research on\nlinear attention has gained significant momentum. However, the improvement\nstrategies for attention mechanisms typically necessitate extensive retraining,\nwhich is impractical for large language models with a vast array of parameters.\nIn this paper, we present DiJiang, a novel Frequency Domain Kernelization\napproach that enables the transformation of a pre-trained vanilla Transformer\ninto a linear complexity model with little training costs. By employing a\nweighted Quasi-Monte Carlo method for sampling, the proposed approach\ntheoretically offers superior approximation efficiency. To further reduce the\ntraining computational complexity, our kernelization is based on Discrete\nCosine Transform (DCT) operations. Extensive experiments demonstrate that the\nproposed method achieves comparable performance to the original Transformer,\nbut with significantly reduced training costs and much faster inference speeds.\nOur DiJiang-7B achieves comparable performance with LLaMA2-7B on various\nbenchmark while requires only about 1/50 training cost. Code is available at\nhttps://github.com/YuchuanTian/DiJiang.",
        "publication_date": "2024-03-29T02:32:15Z",
        "upvotes": 9
    },
    "2403.19888": {
        "url": "https://arxiv.org/abs/2403.19888",
        "title": "MambaMixer: Efficient Selective State Space Models with Dual Token and\n  Channel Selection",
        "authors": [
            "Ali Behrouz",
            "Michele Santacatterina",
            "Ramin Zabih"
        ],
        "abstract": "Recent advances in deep learning have mainly relied on Transformers due to\ntheir data dependency and ability to learn at scale. The attention module in\nthese architectures, however, exhibits quadratic time and space in input size,\nlimiting their scalability for long-sequence modeling. Despite recent attempts\nto design efficient and effective architecture backbone for multi-dimensional\ndata, such as images and multivariate time series, existing models are either\ndata independent, or fail to allow inter- and intra-dimension communication.\nRecently, State Space Models (SSMs), and more specifically Selective State\nSpace Models, with efficient hardware-aware implementation, have shown\npromising potential for long sequence modeling. Motivated by the success of\nSSMs, we present MambaMixer, a new architecture with data-dependent weights\nthat uses a dual selection mechanism across tokens and channels, called\nSelective Token and Channel Mixer. MambaMixer connects selective mixers using a\nweighted averaging mechanism, allowing layers to have direct access to early\nfeatures. As a proof of concept, we design Vision MambaMixer (ViM2) and Time\nSeries MambaMixer (TSM2) architectures based on the MambaMixer block and\nexplore their performance in various vision and time series forecasting tasks.\nOur results underline the importance of selective mixing across both tokens and\nchannels. In ImageNet classification, object detection, and semantic\nsegmentation tasks, ViM2 achieves competitive performance with well-established\nvision models and outperforms SSM-based vision models. In time series\nforecasting, TSM2 achieves outstanding performance compared to state-of-the-art\nmethods while demonstrating significantly improved computational cost. These\nresults show that while Transformers, cross-channel attention, and MLPs are\nsufficient for good performance in time series forecasting, neither is\nnecessary.",
        "publication_date": "2024-03-29T00:05:13Z",
        "upvotes": 8
    },
    "2403.20275": {
        "url": "https://arxiv.org/abs/2403.20275",
        "title": "Snap-it, Tap-it, Splat-it: Tactile-Informed 3D Gaussian Splatting for\n  Reconstructing Challenging Surfaces",
        "authors": [
            "Mauro Comi",
            "Alessio Tonioni",
            "Max Yang",
            "Jonathan Tremblay",
            "Valts Blukis",
            "Yijiong Lin",
            "Nathan F. Lepora",
            "Laurence Aitchison"
        ],
        "abstract": "Touch and vision go hand in hand, mutually enhancing our ability to\nunderstand the world. From a research perspective, the problem of mixing touch\nand vision is underexplored and presents interesting challenges. To this end,\nwe propose Tactile-Informed 3DGS, a novel approach that incorporates touch data\n(local depth maps) with multi-view vision data to achieve surface\nreconstruction and novel view synthesis. Our method optimises 3D Gaussian\nprimitives to accurately model the object's geometry at points of contact. By\ncreating a framework that decreases the transmittance at touch locations, we\nachieve a refined surface reconstruction, ensuring a uniformly smooth depth\nmap. Touch is particularly useful when considering non-Lambertian objects (e.g.\nshiny or reflective surfaces) since contemporary methods tend to fail to\nreconstruct with fidelity specular highlights. By combining vision and tactile\nsensing, we achieve more accurate geometry reconstructions with fewer images\nthan prior methods. We conduct evaluation on objects with glossy and reflective\nsurfaces and demonstrate the effectiveness of our approach, offering\nsignificant improvements in reconstruction quality.",
        "publication_date": "2024-03-29T16:30:17Z",
        "upvotes": 8
    },
    "2404.00399": {
        "url": "https://arxiv.org/abs/2404.00399",
        "title": "Aurora-M: The First Open Source Multilingual Language Model Red-teamed\n  according to the U.S. Executive Order",
        "authors": [
            "Taishi Nakamura",
            "Mayank Mishra",
            "Simone Tedeschi",
            "Yekun Chai",
            "Jason T Stillerman",
            "Felix Friedrich",
            "Prateek Yadav",
            "Tanmay Laud",
            "Vu Minh Chien",
            "Terry Yue Zhuo",
            "Diganta Misra",
            "Ben Bogin",
            "Xuan-Son Vu",
            "Marzena Karpinska",
            "Arnav Varma Dantuluri",
            "Wojciech Kusa",
            "Tommaso Furlanello",
            "Rio Yokota",
            "Niklas Muennighoff",
            "Suhas Pai",
            "Tosin Adewumi",
            "Veronika Laippala",
            "Xiaozhe Yao",
            "Adalberto Junior",
            "Alpay Ariyak",
            "Aleksandr Drozd",
            "Jordan Clive",
            "Kshitij Gupta",
            "Liangyu Chen",
            "Qi Sun",
            "Ken Tsui",
            "Noah Persaud",
            "Nour Fahmy",
            "Tianlong Chen",
            "Mohit Bansal",
            "Nicolo Monti",
            "Tai Dang",
            "Ziyang Luo",
            "Tien-Tung Bui",
            "Roberto Navigli",
            "Virendra Mehta",
            "Matthew Blumberg",
            "Victor May",
            "Huu Nguyen",
            "Sampo Pyysalo"
        ],
        "abstract": "Pretrained language models underpin several AI applications, but their high\ncomputational cost for training limits accessibility. Initiatives such as BLOOM\nand StarCoder aim to democratize access to pretrained models for collaborative\ncommunity development. However, such existing models face challenges: limited\nmultilingual capabilities, continual pretraining causing catastrophic\nforgetting, whereas pretraining from scratch is computationally expensive, and\ncompliance with AI safety and development laws. This paper presents Aurora-M, a\n15B parameter multilingual open-source model trained on English, Finnish,\nHindi, Japanese, Vietnamese, and code. Continually pretrained from\nStarCoderPlus on 435 billion additional tokens, Aurora-M surpasses 2 trillion\ntokens in total training token count. It is the first open-source multilingual\nmodel fine-tuned on human-reviewed safety instructions, thus aligning its\ndevelopment not only with conventional red-teaming considerations, but also\nwith the specific concerns articulated in the Biden-Harris Executive Order on\nthe Safe, Secure, and Trustworthy Development and Use of Artificial\nIntelligence. Aurora-M is rigorously evaluated across various tasks and\nlanguages, demonstrating robustness against catastrophic forgetting and\noutperforming alternatives in multilingual settings, particularly in safety\nevaluations. To promote responsible open-source LLM development, Aurora-M and\nits variants are released at\nhttps://huggingface.co/collections/aurora-m/aurora-m-models-65fdfdff62471e09812f5407 .",
        "publication_date": "2024-03-30T15:38:54Z",
        "upvotes": 37
    },
    "2404.01197": {
        "url": "https://arxiv.org/abs/2404.01197",
        "title": "Getting it Right: Improving Spatial Consistency in Text-to-Image Models",
        "authors": [
            "Agneet Chatterjee",
            "Gabriela Ben Melech Stan",
            "Estelle Aflalo",
            "Sayak Paul",
            "Dhruba Ghosh",
            "Tejas Gokhale",
            "Ludwig Schmidt",
            "Hannaneh Hajishirzi",
            "Vasudev Lal",
            "Chitta Baral",
            "Yezhou Yang"
        ],
        "abstract": "One of the key shortcomings in current text-to-image (T2I) models is their\ninability to consistently generate images which faithfully follow the spatial\nrelationships specified in the text prompt. In this paper, we offer a\ncomprehensive investigation of this limitation, while also developing datasets\nand methods that achieve state-of-the-art performance. First, we find that\ncurrent vision-language datasets do not represent spatial relationships well\nenough; to alleviate this bottleneck, we create SPRIGHT, the first\nspatially-focused, large scale dataset, by re-captioning 6 million images from\n4 widely used vision datasets. Through a 3-fold evaluation and analysis\npipeline, we find that SPRIGHT largely improves upon existing datasets in\ncapturing spatial relationships. To demonstrate its efficacy, we leverage only\n~0.25% of SPRIGHT and achieve a 22% improvement in generating spatially\naccurate images while also improving the FID and CMMD scores. Secondly, we find\nthat training on images containing a large number of objects results in\nsubstantial improvements in spatial consistency. Notably, we attain\nstate-of-the-art on T2I-CompBench with a spatial score of 0.2133, by\nfine-tuning on <500 images. Finally, through a set of controlled experiments\nand ablations, we document multiple findings that we believe will enhance the\nunderstanding of factors that affect spatial consistency in text-to-image\nmodels. We publicly release our dataset and model to foster further research in\nthis area.",
        "publication_date": "2024-04-01T15:55:25Z",
        "upvotes": 28
    },
    "2404.00987": {
        "url": "https://arxiv.org/abs/2404.00987",
        "title": "FlexiDreamer: Single Image-to-3D Generation with FlexiCubes",
        "authors": [
            "Ruowen Zhao",
            "Zhengyi Wang",
            "Yikai Wang",
            "Zihan Zhou",
            "Jun Zhu"
        ],
        "abstract": "3D content generation from text prompts or single images has made remarkable\nprogress in quality and speed recently. One of its dominant paradigms involves\ngenerating consistent multi-view images followed by a sparse-view\nreconstruction. However, due to the challenge of directly deforming the mesh\nrepresentation to approach the target topology, most methodologies learn an\nimplicit representation (such as NeRF) during the sparse-view reconstruction\nand acquire the target mesh by a post-processing extraction. Although the\nimplicit representation can effectively model rich 3D information, its training\ntypically entails a long convergence time. In addition, the post-extraction\noperation from the implicit field also leads to undesirable visual artifacts.\nIn this paper, we propose FlexiDreamer, a novel single image-to-3d generation\nframework that reconstructs the target mesh in an end-to-end manner. By\nleveraging a flexible gradient-based extraction known as FlexiCubes, our method\ncircumvents the defects brought by the post-processing and facilitates a direct\nacquisition of the target mesh. Furthermore, we incorporate a multi-resolution\nhash grid encoding scheme that progressively activates the encoding levels into\nthe implicit field in FlexiCubes to help capture geometric details for per-step\noptimization. Notably, FlexiDreamer recovers a dense 3D structure from a\nsingle-view image in approximately 1 minute on a single NVIDIA A100 GPU,\noutperforming previous methodologies by a large margin.",
        "publication_date": "2024-04-01T08:20:18Z",
        "upvotes": 19
    },
    "2404.01294": {
        "url": "https://arxiv.org/abs/2404.01294",
        "title": "CosmicMan: A Text-to-Image Foundation Model for Humans",
        "authors": [
            "Shikai Li",
            "Jianglin Fu",
            "Kaiyuan Liu",
            "Wentao Wang",
            "Kwan-Yee Lin",
            "Wayne Wu"
        ],
        "abstract": "We present CosmicMan, a text-to-image foundation model specialized for\ngenerating high-fidelity human images. Unlike current general-purpose\nfoundation models that are stuck in the dilemma of inferior quality and\ntext-image misalignment for humans, CosmicMan enables generating\nphoto-realistic human images with meticulous appearance, reasonable structure,\nand precise text-image alignment with detailed dense descriptions. At the heart\nof CosmicMan's success are the new reflections and perspectives on data and\nmodels: (1) We found that data quality and a scalable data production flow are\nessential for the final results from trained models. Hence, we propose a new\ndata production paradigm, Annotate Anyone, which serves as a perpetual data\nflywheel to produce high-quality data with accurate yet cost-effective\nannotations over time. Based on this, we constructed a large-scale dataset,\nCosmicMan-HQ 1.0, with 6 Million high-quality real-world human images in a mean\nresolution of 1488x1255, and attached with precise text annotations deriving\nfrom 115 Million attributes in diverse granularities. (2) We argue that a\ntext-to-image foundation model specialized for humans must be pragmatic -- easy\nto integrate into down-streaming tasks while effective in producing\nhigh-quality human images. Hence, we propose to model the relationship between\ndense text descriptions and image pixels in a decomposed manner, and present\nDecomposed-Attention-Refocusing (Daring) training framework. It seamlessly\ndecomposes the cross-attention features in existing text-to-image diffusion\nmodel, and enforces attention refocusing without adding extra modules. Through\nDaring, we show that explicitly discretizing continuous text space into several\nbasic groups that align with human body structure is the key to tackling the\nmisalignment problem in a breeze.",
        "publication_date": "2024-04-01T17:59:05Z",
        "upvotes": 14
    },
    "2404.01292": {
        "url": "https://arxiv.org/abs/2404.01292",
        "title": "Measuring Style Similarity in Diffusion Models",
        "authors": [
            "Gowthami Somepalli",
            "Anubhav Gupta",
            "Kamal Gupta",
            "Shramay Palta",
            "Micah Goldblum",
            "Jonas Geiping",
            "Abhinav Shrivastava",
            "Tom Goldstein"
        ],
        "abstract": "Generative models are now widely used by graphic designers and artists. Prior\nworks have shown that these models remember and often replicate content from\ntheir training data during generation. Hence as their proliferation increases,\nit has become important to perform a database search to determine whether the\nproperties of the image are attributable to specific training data, every time\nbefore a generated image is used for professional purposes. Existing tools for\nthis purpose focus on retrieving images of similar semantic content. Meanwhile,\nmany artists are concerned with style replication in text-to-image models. We\npresent a framework for understanding and extracting style descriptors from\nimages. Our framework comprises a new dataset curated using the insight that\nstyle is a subjective property of an image that captures complex yet meaningful\ninteractions of factors including but not limited to colors, textures, shapes,\netc. We also propose a method to extract style descriptors that can be used to\nattribute style of a generated image to the images used in the training dataset\nof a text-to-image model. We showcase promising results in various style\nretrieval tasks. We also quantitatively and qualitatively analyze style\nattribution and matching in the Stable Diffusion model. Code and artifacts are\navailable at https://github.com/learn2phoenix/CSD.",
        "publication_date": "2024-04-01T17:58:30Z",
        "upvotes": 13
    },
    "2404.00345": {
        "url": "https://arxiv.org/abs/2404.00345",
        "title": "MaGRITTe: Manipulative and Generative 3D Realization from Image, Topview\n  and Text",
        "authors": [
            "Takayuki Hara",
            "Tatsuya Harada"
        ],
        "abstract": "The generation of 3D scenes from user-specified conditions offers a promising\navenue for alleviating the production burden in 3D applications. Previous\nstudies required significant effort to realize the desired scene, owing to\nlimited control conditions. We propose a method for controlling and generating\n3D scenes under multimodal conditions using partial images, layout information\nrepresented in the top view, and text prompts. Combining these conditions to\ngenerate a 3D scene involves the following significant difficulties: (1) the\ncreation of large datasets, (2) reflection on the interaction of multimodal\nconditions, and (3) domain dependence of the layout conditions. We decompose\nthe process of 3D scene generation into 2D image generation from the given\nconditions and 3D scene generation from 2D images. 2D image generation is\nachieved by fine-tuning a pretrained text-to-image model with a small\nartificial dataset of partial images and layouts, and 3D scene generation is\nachieved by layout-conditioned depth estimation and neural radiance fields\n(NeRF), thereby avoiding the creation of large datasets. The use of a common\nrepresentation of spatial information using 360-degree images allows for the\nconsideration of multimodal condition interactions and reduces the domain\ndependence of the layout control. The experimental results qualitatively and\nquantitatively demonstrated that the proposed method can generate 3D scenes in\ndiverse domains, from indoor to outdoor, according to multimodal conditions.",
        "publication_date": "2024-03-30T12:50:25Z",
        "upvotes": 13
    },
    "2404.01143": {
        "url": "https://arxiv.org/abs/2404.01143",
        "title": "Condition-Aware Neural Network for Controlled Image Generation",
        "authors": [
            "Han Cai",
            "Muyang Li",
            "Zhuoyang Zhang",
            "Qinsheng Zhang",
            "Ming-Yu Liu",
            "Song Han"
        ],
        "abstract": "We present Condition-Aware Neural Network (CAN), a new method for adding\ncontrol to image generative models. In parallel to prior conditional control\nmethods, CAN controls the image generation process by dynamically manipulating\nthe weight of the neural network. This is achieved by introducing a\ncondition-aware weight generation module that generates conditional weight for\nconvolution/linear layers based on the input condition. We test CAN on\nclass-conditional image generation on ImageNet and text-to-image generation on\nCOCO. CAN consistently delivers significant improvements for diffusion\ntransformer models, including DiT and UViT. In particular, CAN combined with\nEfficientViT (CaT) achieves 2.78 FID on ImageNet 512x512, surpassing DiT-XL/2\nwhile requiring 52x fewer MACs per sampling step.",
        "publication_date": "2024-04-01T14:42:57Z",
        "upvotes": 10
    },
    "2404.01297": {
        "url": "https://arxiv.org/abs/2404.01297",
        "title": "Streaming Dense Video Captioning",
        "authors": [
            "Xingyi Zhou",
            "Anurag Arnab",
            "Shyamal Buch",
            "Shen Yan",
            "Austin Myers",
            "Xuehan Xiong",
            "Arsha Nagrani",
            "Cordelia Schmid"
        ],
        "abstract": "An ideal model for dense video captioning -- predicting captions localized\ntemporally in a video -- should be able to handle long input videos, predict\nrich, detailed textual descriptions, and be able to produce outputs before\nprocessing the entire video. Current state-of-the-art models, however, process\na fixed number of downsampled frames, and make a single full prediction after\nseeing the whole video. We propose a streaming dense video captioning model\nthat consists of two novel components: First, we propose a new memory module,\nbased on clustering incoming tokens, which can handle arbitrarily long videos\nas the memory is of a fixed size. Second, we develop a streaming decoding\nalgorithm that enables our model to make predictions before the entire video\nhas been processed. Our model achieves this streaming ability, and\nsignificantly improves the state-of-the-art on three dense video captioning\nbenchmarks: ActivityNet, YouCook2 and ViTT. Our code is released at\nhttps://github.com/google-research/scenic.",
        "publication_date": "2024-04-01T17:59:15Z",
        "upvotes": 9
    },
    "2404.01258": {
        "url": "https://arxiv.org/abs/2404.01258",
        "title": "Direct Preference Optimization of Video Large Multimodal Models from\n  Language Model Reward",
        "authors": [
            "Ruohong Zhang",
            "Liangke Gui",
            "Zhiqing Sun",
            "Yihao Feng",
            "Keyang Xu",
            "Yuanhan Zhang",
            "Di Fu",
            "Chunyuan Li",
            "Alexander Hauptmann",
            "Yonatan Bisk",
            "Yiming Yang"
        ],
        "abstract": "Preference modeling techniques, such as direct preference optimization (DPO),\nhas shown effective in enhancing the generalization abilities of large language\nmodel (LLM). However, in tasks involving video instruction-following, providing\ninformative feedback, especially for detecting hallucinations in generated\nresponses, remains a significant challenge. Previous studies have explored\nusing large large multimodal models (LMMs) as reward models to guide preference\nmodeling, but their ability to accurately assess the factuality of generated\nresponses compared to corresponding videos has not been conclusively\nestablished. This paper introduces a novel framework that utilizes detailed\nvideo captions as a proxy of video content, enabling language models to\nincorporate this information as supporting evidence for scoring video Question\nAnswering (QA) predictions. Our approach demonstrates robust alignment with\nOpenAI GPT-4V model's reward mechanism, which directly takes video frames as\ninput. Furthermore, we show that applying this tailored reward through DPO\nsignificantly improves the performance of video LMMs on video QA tasks.",
        "publication_date": "2024-04-01T17:28:16Z",
        "upvotes": 9
    },
    "2404.00488": {
        "url": "https://arxiv.org/abs/2404.00488",
        "title": "Noise-Aware Training of Layout-Aware Language Models",
        "authors": [
            "Ritesh Sarkhel",
            "Xiaoqi Ren",
            "Lauro Beltrao Costa",
            "Guolong Su",
            "Vincent Perot",
            "Yanan Xie",
            "Emmanouil Koukoumidis",
            "Arnab Nandi"
        ],
        "abstract": "A visually rich document (VRD) utilizes visual features along with linguistic\ncues to disseminate information. Training a custom extractor that identifies\nnamed entities from a document requires a large number of instances of the\ntarget document type annotated at textual and visual modalities. This is an\nexpensive bottleneck in enterprise scenarios, where we want to train custom\nextractors for thousands of different document types in a scalable way.\nPre-training an extractor model on unlabeled instances of the target document\ntype, followed by a fine-tuning step on human-labeled instances does not work\nin these scenarios, as it surpasses the maximum allowable training time\nallocated for the extractor. We address this scenario by proposing a\nNoise-Aware Training method or NAT in this paper. Instead of acquiring\nexpensive human-labeled documents, NAT utilizes weakly labeled documents to\ntrain an extractor in a scalable way. To avoid degradation in the model's\nquality due to noisy, weakly labeled samples, NAT estimates the confidence of\neach training sample and incorporates it as uncertainty measure during\ntraining. We train multiple state-of-the-art extractor models using NAT.\nExperiments on a number of publicly available and in-house datasets show that\nNAT-trained models are not only robust in performance -- it outperforms a\ntransfer-learning baseline by up to 6% in terms of macro-F1 score, but it is\nalso more label-efficient -- it reduces the amount of human-effort required to\nobtain comparable performance by up to 73%.",
        "publication_date": "2024-03-30T23:06:34Z",
        "upvotes": 6
    },
    "2404.00656": {
        "url": "https://arxiv.org/abs/2404.00656",
        "title": "WavLLM: Towards Robust and Adaptive Speech Large Language Model",
        "authors": [
            "Shujie Hu",
            "Long Zhou",
            "Shujie Liu",
            "Sanyuan Chen",
            "Hongkun Hao",
            "Jing Pan",
            "Xunying Liu",
            "Jinyu Li",
            "Sunit Sivasankaran",
            "Linquan Liu",
            "Furu Wei"
        ],
        "abstract": "The recent advancements in large language models (LLMs) have revolutionized\nthe field of natural language processing, progressively broadening their scope\nto multimodal perception and generation. However, effectively integrating\nlistening capabilities into LLMs poses significant challenges, particularly\nwith respect to generalizing across varied contexts and executing complex\nauditory tasks. In this work, we introduce WavLLM, a robust and adaptive speech\nlarge language model with dual encoders, and a prompt-aware LoRA weight\nadapter, optimized by a two-stage curriculum learning approach. Leveraging dual\nencoders, we decouple different types of speech information, utilizing a\nWhisper encoder to process the semantic content of speech, and a WavLM encoder\nto capture the unique characteristics of the speaker's identity. Within the\ncurriculum learning framework, WavLLM first builds its foundational\ncapabilities by optimizing on mixed elementary single tasks, followed by\nadvanced multi-task training on more complex tasks such as combinations of the\nelementary tasks. To enhance the flexibility and adherence to different tasks\nand instructions, a prompt-aware LoRA weight adapter is introduced in the\nsecond advanced multi-task training stage. We validate the proposed model on\nuniversal speech benchmarks including tasks such as ASR, ST, SV, ER, and also\napply it to specialized datasets like Gaokao English listening comprehension\nset for SQA, and speech Chain-of-Thought (CoT) evaluation set. Experiments\ndemonstrate that the proposed model achieves state-of-the-art performance\nacross a range of speech tasks on the same model size, exhibiting robust\ngeneralization capabilities in executing complex tasks using CoT approach.\nFurthermore, our model successfully completes Gaokao tasks without specialized\ntraining. The codes, models, audio, and Gaokao evaluation set can be accessed\nat \\url{aka.ms/wavllm}.",
        "publication_date": "2024-03-31T12:01:32Z",
        "upvotes": 5
    },
    "2404.00308": {
        "url": "https://arxiv.org/abs/2404.00308",
        "title": "ST-LLM: Large Language Models Are Effective Temporal Learners",
        "authors": [
            "Ruyang Liu",
            "Chen Li",
            "Haoran Tang",
            "Yixiao Ge",
            "Ying Shan",
            "Ge Li"
        ],
        "abstract": "Large Language Models (LLMs) have showcased impressive capabilities in text\ncomprehension and generation, prompting research efforts towards video LLMs to\nfacilitate human-AI interaction at the video level. However, how to effectively\nencode and understand videos in video-based dialogue systems remains to be\nsolved. In this paper, we investigate a straightforward yet unexplored\nquestion: Can we feed all spatial-temporal tokens into the LLM, thus delegating\nthe task of video sequence modeling to the LLMs? Surprisingly, this simple\napproach yields significant improvements in video understanding. Based upon\nthis, we propose ST-LLM, an effective video-LLM baseline with Spatial-Temporal\nsequence modeling inside LLM. Furthermore, to address the overhead and\nstability issues introduced by uncompressed video tokens within LLMs, we\ndevelop a dynamic masking strategy with tailor-made training objectives. For\nparticularly long videos, we have also designed a global-local input module to\nbalance efficiency and effectiveness. Consequently, we harness LLM for\nproficient spatial-temporal modeling, while upholding efficiency and stability.\nExtensive experimental results attest to the effectiveness of our method.\nThrough a more concise model and training pipeline, ST-LLM establishes a new\nstate-of-the-art result on VideoChatGPT-Bench and MVBench. Codes have been\navailable at https://github.com/TencentARC/ST-LLM.",
        "publication_date": "2024-03-30T10:11:26Z",
        "upvotes": 4
    },
    "2404.01744": {
        "url": "https://arxiv.org/abs/2404.01744",
        "title": "Octopus v2: On-device language model for super agent",
        "authors": [
            "Wei Chen",
            "Zhiyuan Li"
        ],
        "abstract": "Language models have shown effectiveness in a variety of software\napplications, particularly in tasks related to automatic workflow. These models\npossess the crucial ability to call functions, which is essential in creating\nAI agents. Despite the high performance of large-scale language models in cloud\nenvironments, they are often associated with concerns over privacy and cost.\nCurrent on-device models for function calling face issues with latency and\naccuracy. Our research presents a new method that empowers an on-device model\nwith 2 billion parameters to surpass the performance of GPT-4 in both accuracy\nand latency, and decrease the context length by 95\\%. When compared to Llama-7B\nwith a RAG-based function calling mechanism, our method enhances latency by\n35-fold. This method reduces the latency to levels deemed suitable for\ndeployment across a variety of edge devices in production environments,\naligning with the performance requisites for real-world applications.",
        "publication_date": "2024-04-02T09:01:32Z",
        "upvotes": 45
    },
    "2404.02078": {
        "url": "https://arxiv.org/abs/2404.02078",
        "title": "Advancing LLM Reasoning Generalists with Preference Trees",
        "authors": [
            "Lifan Yuan",
            "Ganqu Cui",
            "Hanbin Wang",
            "Ning Ding",
            "Xingyao Wang",
            "Jia Deng",
            "Boji Shan",
            "Huimin Chen",
            "Ruobing Xie",
            "Yankai Lin",
            "Zhenghao Liu",
            "Bowen Zhou",
            "Hao Peng",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "abstract": "We introduce Eurus, a suite of large language models (LLMs) optimized for\nreasoning. Finetuned from Mistral-7B and CodeLlama-70B, Eurus models achieve\nstate-of-the-art results among open-source models on a diverse set of\nbenchmarks covering mathematics, code generation, and logical reasoning\nproblems. Notably, Eurus-70B beats GPT-3.5 Turbo in reasoning through a\ncomprehensive benchmarking across 12 tests covering five tasks, and achieves a\n33.3% pass@1 accuracy on LeetCode and 32.6% on TheoremQA, two challenging\nbenchmarks, substantially outperforming existing open-source models by margins\nmore than 13.3%. The strong performance of Eurus can be primarily attributed to\nUltraInteract, our newly-curated large-scale, high-quality alignment dataset\nspecifically designed for complex reasoning tasks. UltraInteract can be used in\nboth supervised fine-tuning and preference learning. For each instruction, it\nincludes a preference tree consisting of (1) reasoning chains with diverse\nplanning strategies in a unified format, (2) multi-turn interaction\ntrajectories with the environment and the critique, and (3) pairwise data to\nfacilitate preference learning. UltraInteract allows us to conduct an in-depth\nexploration of preference learning for reasoning tasks. Our investigation\nreveals that some well-established preference learning algorithms may be less\nsuitable for reasoning tasks compared to their effectiveness in general\nconversations. Inspired by this, we derive a novel reward modeling objective\nwhich, together with UltraInteract, leads to a strong reward model.",
        "publication_date": "2024-04-02T16:25:30Z",
        "upvotes": 35
    },
    "2404.02060": {
        "url": "https://arxiv.org/abs/2404.02060",
        "title": "Long-context LLMs Struggle with Long In-context Learning",
        "authors": [
            "Tianle Li",
            "Ge Zhang",
            "Quy Duc Do",
            "Xiang Yue",
            "Wenhu Chen"
        ],
        "abstract": "Large Language Models (LLMs) have made significant strides in handling long\nsequences exceeding 32K tokens. However, their performance evaluation has\nlargely been confined to metrics like perplexity and synthetic tasks, which may\nnot fully capture their abilities in more nuanced, real-world scenarios. This\nstudy introduces a specialized benchmark (LongICLBench) focusing on long\nin-context learning within the realm of extreme-label classification. We\nmeticulously selected six datasets with a label range spanning 28 to 174\nclasses covering different input (few-shot demonstration) lengths from 2K to\n50K tokens. Our benchmark requires LLMs to comprehend the entire input to\nrecognize the massive label spaces to make correct predictions. We evaluate 13\nlong-context LLMs on our benchmarks. We find that the long-context LLMs perform\nrelatively well on less challenging tasks with shorter demonstration lengths by\neffectively utilizing the long context window. However, on the most challenging\ntask Discovery with 174 labels, all the LLMs struggle to understand the task\ndefinition, thus reaching a performance close to zero. This suggests a notable\ngap in current LLM capabilities for processing and understanding long,\ncontext-rich sequences. Further analysis revealed a tendency among models to\nfavor predictions for labels presented toward the end of the sequence. Their\nability to reason over multiple pieces in the long sequence is yet to be\nimproved. Our study reveals that long context understanding and reasoning is\nstill a challenging task for the existing LLMs. We believe LongICLBench could\nserve as a more realistic evaluation for the future long-context LLMs.",
        "publication_date": "2024-04-02T15:59:11Z",
        "upvotes": 27
    },
    "2404.01331": {
        "url": "https://arxiv.org/abs/2404.01331",
        "title": "LLaVA-Gemma: Accelerating Multimodal Foundation Models with a Compact\n  Language Model",
        "authors": [
            "Musashi Hinck",
            "Matthew L. Olson",
            "David Cobbley",
            "Shao-Yen Tseng",
            "Vasudev Lal"
        ],
        "abstract": "We train a suite of multimodal foundation models (MMFM) using the popular\nLLaVA framework with the recently released Gemma family of large language\nmodels (LLMs). Of particular interest is the 2B parameter Gemma model, which\nprovides opportunities to construct capable small-scale MMFMs. In line with\nfindings from other papers in this space, we test the effect of ablating three\ndesign features: pretraining the connector, utilizing a more powerful image\nbackbone, and increasing the size of the language backbone. The resulting\nmodels, which we call LLaVA-Gemma, exhibit moderate performance on an array of\nevaluations, but fail to improve past the current comparably sized SOTA models.\nCloser analysis of performance shows mixed effects; skipping pretraining tends\nto reduce performance, larger vision models sometimes improve performance, and\nincreasing language model size has inconsistent effects. We publicly release\ntraining recipes, code and weights for our models for the LLaVA-Gemma models.",
        "publication_date": "2024-03-29T21:32:50Z",
        "upvotes": 22
    },
    "2404.01367": {
        "url": "https://arxiv.org/abs/2404.01367",
        "title": "Bigger is not Always Better: Scaling Properties of Latent Diffusion\n  Models",
        "authors": [
            "Kangfu Mei",
            "Zhengzhong Tu",
            "Mauricio Delbracio",
            "Hossein Talebi",
            "Vishal M. Patel",
            "Peyman Milanfar"
        ],
        "abstract": "We study the scaling properties of latent diffusion models (LDMs) with an\nemphasis on their sampling efficiency. While improved network architecture and\ninference algorithms have shown to effectively boost sampling efficiency of\ndiffusion models, the role of model size -- a critical determinant of sampling\nefficiency -- has not been thoroughly examined. Through empirical analysis of\nestablished text-to-image diffusion models, we conduct an in-depth\ninvestigation into how model size influences sampling efficiency across varying\nsampling steps. Our findings unveil a surprising trend: when operating under a\ngiven inference budget, smaller models frequently outperform their larger\nequivalents in generating high-quality results. Moreover, we extend our study\nto demonstrate the generalizability of the these findings by applying various\ndiffusion samplers, exploring diverse downstream tasks, evaluating\npost-distilled models, as well as comparing performance relative to training\ncompute. These findings open up new pathways for the development of LDM scaling\nstrategies which can be employed to enhance generative capabilities within\nlimited inference budgets.",
        "publication_date": "2024-04-01T17:59:48Z",
        "upvotes": 17
    },
    "2404.02101": {
        "url": "https://arxiv.org/abs/2404.02101",
        "title": "CameraCtrl: Enabling Camera Control for Text-to-Video Generation",
        "authors": [
            "Hao He",
            "Yinghao Xu",
            "Yuwei Guo",
            "Gordon Wetzstein",
            "Bo Dai",
            "Hongsheng Li",
            "Ceyuan Yang"
        ],
        "abstract": "Controllability plays a crucial role in video generation since it allows\nusers to create desired content. However, existing models largely overlooked\nthe precise control of camera pose that serves as a cinematic language to\nexpress deeper narrative nuances. To alleviate this issue, we introduce\nCameraCtrl, enabling accurate camera pose control for text-to-video(T2V)\nmodels. After precisely parameterizing the camera trajectory, a plug-and-play\ncamera module is then trained on a T2V model, leaving others untouched.\nAdditionally, a comprehensive study on the effect of various datasets is also\nconducted, suggesting that videos with diverse camera distribution and similar\nappearances indeed enhance controllability and generalization. Experimental\nresults demonstrate the effectiveness of CameraCtrl in achieving precise and\ndomain-adaptive camera control, marking a step forward in the pursuit of\ndynamic and customized video storytelling from textual and camera pose inputs.\nOur project website is at: https://hehao13.github.io/projects-CameraCtrl/.",
        "publication_date": "2024-04-02T16:52:41Z",
        "upvotes": 15
    },
    "2404.01954": {
        "url": "https://arxiv.org/abs/2404.01954",
        "title": "HyperCLOVA X Technical Report",
        "authors": [
            "Kang Min Yoo",
            "Jaegeun Han",
            "Sookyo In",
            "Heewon Jeon",
            "Jisu Jeong",
            "Jaewook Kang",
            "Hyunwook Kim",
            "Kyung-Min Kim",
            "Munhyong Kim",
            "Sungju Kim",
            "Donghyun Kwak",
            "Hanock Kwak",
            "Se Jung Kwon",
            "Bado Lee",
            "Dongsoo Lee",
            "Gichang Lee",
            "Jooho Lee",
            "Baeseong Park",
            "Seongjin Shin",
            "Joonsang Yu",
            "Seolki Baek",
            "Sumin Byeon",
            "Eungsup Cho",
            "Dooseok Choe",
            "Jeesung Han",
            "Youngkyun Jin",
            "Hyein Jun",
            "Jaeseung Jung",
            "Chanwoong Kim",
            "Jinhong Kim",
            "Jinuk Kim",
            "Dokyeong Lee",
            "Dongwook Park",
            "Jeong Min Sohn",
            "Sujung Han",
            "Jiae Heo",
            "Sungju Hong",
            "Mina Jeon",
            "Hyunhoon Jung",
            "Jungeun Jung",
            "Wangkyo Jung",
            "Chungjoon Kim",
            "Hyeri Kim",
            "Jonghyun Kim",
            "Min Young Kim",
            "Soeun Lee",
            "Joonhee Park",
            "Jieun Shin",
            "Sojin Yang",
            "Jungsoon Yoon",
            "Hwaran Lee",
            "Sanghwan Bae",
            "Jeehwan Cha",
            "Donghoon Ham",
            "Youngki Hong",
            "Yunki Hong",
            "Myunggeun Ji",
            "Yeguk Jin",
            "Chansong Jo",
            "Shinyoung Joo",
            "Seunghwan Jung",
            "Hyomin Kim",
            "Jungwhan Kim",
            "Minkyoung Kim",
            "Minseung Kim",
            "Sungdong Kim",
            "Yonghee Kim",
            "Youngjun Kim",
            "Donghyeon Ko",
            "Dughyun Lee",
            "Jaehong Lee",
            "Jieun Lee",
            "Jongjin Lee",
            "Min Young Lee",
            "Yehbin Lee",
            "Taehong Min",
            "Kiyoon Moon",
            "Jaesun Park",
            "Kyuyon Park",
            "Seunghyun Seo",
            "Gyubin Son",
            "Wonjoon Yoo",
            "Myungin You",
            "Doheon Ahn",
            "Homin Ahn",
            "Joohee Ahn",
            "Seongmin Ahn",
            "Chanwoo An",
            "Hyeryun An",
            "Junho An",
            "Sang-Min An",
            "Boram Byun",
            "Jongho Cha",
            "Minji Chang",
            "Seunggyu Chang",
            "Haesong Cho",
            "Youngdo Cho",
            "Dalnim Choi",
            "Daseul Choi",
            "Hyoseok Choi",
            "Minseong Choi",
            "Sangho Choi",
            "Seongjae Choi",
            "Wooyong Choi",
            "Sewhan Chun",
            "Dong Young Go",
            "Chiheon Ham",
            "Danbi Han",
            "Jaemin Han",
            "Mihak Hong",
            "Moonyoung Hong",
            "Sung Bum Hong",
            "Seongchan Hwang",
            "Eunbin Hyun",
            "Jinbae Im",
            "Jaehyung Jang",
            "Jaeni Jang",
            "Sihyeon Jang",
            "Sungwon Jang",
            "Joonha Jeon",
            "Yujin Jeon",
            "Daun Jeong",
            "Joonhyun Jeong",
            "Kyeongseok Jeong",
            "Mini Jeong",
            "Yeji Jeong",
            "Sol Jin",
            "Hanbyeol Jo",
            "Hanju Jo",
            "Minjung Jo",
            "Lee Jonghyun",
            "Chaeyoon Jung",
            "Hyungsik Jung",
            "Jaeuk Jung",
            "Ju Hwan Jung",
            "Kwangsun Jung",
            "Seungjae Jung",
            "Soonwon Ka",
            "Donghan Kang",
            "Soyoung Kang",
            "Taeho Kil",
            "Areum Kim",
            "Beomyoung Kim",
            "Byeongwook Kim",
            "Daehee Kim",
            "Dong-Gyun Kim",
            "Donggook Kim",
            "Donghyun Kim",
            "Euna Kim",
            "Eunchul Kim",
            "Geewook Kim",
            "Gyu Ri Kim",
            "Hanbyul Kim",
            "Heesu Kim",
            "Isaac Kim",
            "Jeonghoon Kim",
            "Jihye Kim",
            "Joonghoon Kim",
            "Minjae Kim",
            "Minsub Kim",
            "Pil Hwan Kim",
            "Sammy Kim",
            "Seokhun Kim",
            "Seonghyeon Kim",
            "Soojin Kim",
            "Soong Kim",
            "Soyoon Kim",
            "Sunyoung Kim",
            "Taeho Kim",
            "Wonho Kim",
            "Yoonsik Kim",
            "You Jin Kim",
            "Yuri Kim",
            "Beomseok Kwon",
            "Ohsung Kwon",
            "Yoo-Hwan Kwon",
            "Anna Lee",
            "Byungwook Lee",
            "Changho Lee",
            "Daun Lee",
            "Dongjae Lee",
            "Ha-Ram Lee",
            "Hodong Lee",
            "Hwiyeong Lee",
            "Hyunmi Lee",
            "Injae Lee",
            "Jaeung Lee",
            "Jeongsang Lee",
            "Jisoo Lee",
            "Joongjae Lee",
            "Juhan Lee",
            "Jung Hyun Lee",
            "Junghoon Lee",
            "Junwoo Lee",
            "Se Yun Lee",
            "Sujin Lee",
            "Sungjae Lee",
            "Sungwoo Lee",
            "Wonjae Lee",
            "Zoo Hyun Lee",
            "Jong Kun Lim",
            "Kun Lim",
            "Taemin Lim",
            "Yuri Min",
            "Nuri Na",
            "Jeongyeon Nam",
            "Kyeong-Min Nam",
            "Yeonseog Noh",
            "Biro Oh",
            "Hyangnam Oh",
            "Jung-Sik Oh",
            "Solgil Oh",
            "Yeontaek Oh",
            "Boyoun Park",
            "Cheonbok Park",
            "Dongju Park",
            "Hyeonjin Park",
            "Hyun Tae Park",
            "Hyunjung Park",
            "Jihye Park",
            "Jooseok Park",
            "Junghwan Park",
            "Jungsoo Park",
            "Miru Park",
            "Sang Hee Park",
            "Seunghyun Park",
            "Taerim Park",
            "Wonkyeong Park",
            "Hyunjoon Ryu",
            "Jeonghun Ryu",
            "Nahyeon Ryu",
            "Soonshin Seo",
            "Suk Min Seo",
            "Yoonjeong Shim",
            "Kyuyong Shin",
            "Wonkwang Shin",
            "Hyun Sim",
            "Mihyun Sim",
            "Woongseob Sim",
            "Hyejin Soh",
            "Bokyoung Son",
            "Hyunjun Son",
            "Seulah Son",
            "Chi-Yun Song",
            "Chiyoung Song",
            "Ka Yeon Song",
            "Minchul Song",
            "Seungmin Song",
            "Jisung Wang",
            "Matt Yeo",
            "Yonggoo Yeo",
            "Myeong Yeon Yi",
            "Moon Bin Yim",
            "Taehwan Yoo",
            "Youngjoon Yoo",
            "Sungmin Yoon",
            "Young Jin Yoon",
            "Hangyeol Yu",
            "Ui Seon Yu",
            "Xingdong Zuo",
            "Jeongin Bae",
            "Joungeun Bae",
            "Hyunsoo Cho",
            "Seonghyun Cho",
            "Yongjin Cho",
            "Taekyoon Choi",
            "Yera Choi",
            "Jiwan Chung",
            "Zhenghui Han",
            "Byeongho Heo",
            "Euisuk Hong",
            "Taebaek Hwang",
            "Seonyeol Im",
            "Sumin Jegal",
            "Sumin Jeon",
            "Yelim Jeong",
            "Yonghyun Jeong",
            "Can Jiang",
            "Juyong Jiang",
            "Jiho Jin",
            "Ara Jo",
            "Younghyun Jo",
            "Hoyoun Jung",
            "Juyoung Jung",
            "Dae Hee Kim",
            "Ginam Kim",
            "Hangyeol Kim",
            "Heeseung Kim",
            "Hyojin Kim",
            "Hyojun Kim",
            "Hyun-Ah Kim",
            "Jeehye Kim",
            "Jin-Hwa Kim",
            "Jiseon Kim",
            "Jonghak Kim",
            "Jung Yoon Kim",
            "Rak Yeong Kim",
            "Seoyoon Kim",
            "Sewon Kim",
            "Sooyoung Kim",
            "Sukyoung Kim",
            "Taeyong Kim",
            "Naeun Ko",
            "Bonseung Koo",
            "Heeyoung Kwak",
            "Haena Kwon",
            "Youngjin Kwon",
            "Boram Lee",
            "Bruce W. Lee",
            "Dagyeong Lee",
            "Erin Lee",
            "Euijin Lee",
            "Ha Gyeong Lee",
            "Hyojin Lee",
            "Hyunjeong Lee",
            "Jeeyoon Lee",
            "Jeonghyun Lee",
            "Jongheok Lee",
            "Joonhyung Lee",
            "Junhyuk Lee",
            "Mingu Lee",
            "Nayeon Lee",
            "Sangkyu Lee",
            "Se Young Lee",
            "Seulgi Lee",
            "Seung Jin Lee",
            "Suhyeon Lee",
            "Yeonjae Lee",
            "Yesol Lee",
            "Youngbeom Lee",
            "Yujin Lee",
            "Shaodong Li",
            "Tianyu Liu",
            "Seong-Eun Moon",
            "Taehong Moon",
            "Max-Lasse Nihlenramstroem",
            "Wonseok Oh",
            "Yuri Oh",
            "Hongbeen Park",
            "Hyekyung Park",
            "Nohil Park",
            "Sangjin Park",
            "Jiwon Ryu",
            "Miru Ryu",
            "Simo Ryu",
            "Ahreum Seo",
            "Hee Seo",
            "Kangdeok Seo",
            "Jamin Shin",
            "Seungyoun Shin",
            "Heetae Sin",
            "Jiangping Wang",
            "Lei Wang",
            "Ning Xiang",
            "Longxiang Xiao",
            "Jing Xu",
            "Seonyeong Yi",
            "Haanju Yoo",
            "Haneul Yoo",
            "Hwanhee Yoo",
            "Liang Yu",
            "Youngjae Yu",
            "Weijie Yuan",
            "Bo Zeng",
            "Qian Zhou",
            "Kyunghyun Cho",
            "Jung-Woo Ha",
            "Joonsuk Park",
            "Jihyun Hwang",
            "Hyoung Jo Kwon",
            "Soonyong Kwon",
            "Jungyeon Lee",
            "Seungho Lee",
            "Seungho Choi",
            "Sang-Woo Lee",
            "Jung Hwa Lim",
            "Nako Sung"
        ],
        "abstract": "We introduce HyperCLOVA X, a family of large language models (LLMs) tailored\nto the Korean language and culture, along with competitive capabilities in\nEnglish, math, and coding. HyperCLOVA X was trained on a balanced mix of\nKorean, English, and code data, followed by instruction-tuning with\nhigh-quality human-annotated datasets while abiding by strict safety guidelines\nreflecting our commitment to responsible AI. The model is evaluated across\nvarious benchmarks, including comprehensive reasoning, knowledge, commonsense,\nfactuality, coding, math, chatting, instruction-following, and harmlessness, in\nboth Korean and English. HyperCLOVA X exhibits strong reasoning capabilities in\nKorean backed by a deep understanding of the language and cultural nuances.\nFurther analysis of the inherent bilingual nature and its extension to\nmultilingualism highlights the model's cross-lingual proficiency and strong\ngeneralization ability to untargeted languages, including machine translation\nbetween several language pairs and cross-lingual inference tasks. We believe\nthat HyperCLOVA X can provide helpful guidance for regions or countries in\ndeveloping their sovereign LLMs.",
        "publication_date": "2024-04-02T13:48:49Z",
        "upvotes": 11
    },
    "2404.01856": {
        "url": "https://arxiv.org/abs/2404.01856",
        "title": "Poro 34B and the Blessing of Multilinguality",
        "authors": [
            "Risto Luukkonen",
            "Jonathan Burdge",
            "Elaine Zosa",
            "Aarne Talman",
            "Ville Komulainen",
            "V\u00e4in\u00f6 Hatanp\u00e4\u00e4",
            "Peter Sarlin",
            "Sampo Pyysalo"
        ],
        "abstract": "The pretraining of state-of-the-art large language models now requires\ntrillions of words of text, which is orders of magnitude more than available\nfor the vast majority of languages. While including text in more than one\nlanguage is an obvious way to acquire more pretraining data, multilinguality is\noften seen as a curse, and most model training efforts continue to focus\nnear-exclusively on individual large languages. We believe that multilinguality\ncan be a blessing and that it should be possible to substantially improve over\nthe capabilities of monolingual models for small languages through multilingual\ntraining. In this study, we introduce Poro 34B, a 34 billion parameter model\ntrained for 1 trillion tokens of Finnish, English, and programming languages,\nand demonstrate that a multilingual training approach can produce a model that\nnot only substantially advances over the capabilities of existing models for\nFinnish, but also excels in translation and is competitive in its class in\ngenerating English and programming languages. We release the model parameters,\nscripts, and data under open licenses at\nhttps://huggingface.co/LumiOpen/Poro-34B.",
        "publication_date": "2024-04-02T11:34:12Z",
        "upvotes": 11
    },
    "2404.01475": {
        "url": "https://arxiv.org/abs/2404.01475",
        "title": "Are large language models superhuman chemists?",
        "authors": [
            "Adrian Mirza",
            "Nawaf Alampara",
            "Sreekanth Kunchapu",
            "Benedict Emoekabu",
            "Aswanth Krishnan",
            "Mara Wilhelmi",
            "Macjonathan Okereke",
            "Juliane Eberhardt",
            "Amir Mohammad Elahi",
            "Maximilian Greiner",
            "Caroline T. Holick",
            "Tanya Gupta",
            "Mehrdad Asgari",
            "Christina Glaubitz",
            "Lea C. Klepsch",
            "Yannik K\u00f6ster",
            "Jakob Meyer",
            "Santiago Miret",
            "Tim Hoffmann",
            "Fabian Alexander Kreth",
            "Michael Ringleb",
            "Nicole Roesner",
            "Ulrich S. Schubert",
            "Leanne M. Stafast",
            "Dinga Wonanke",
            "Michael Pieler",
            "Philippe Schwaller",
            "Kevin Maik Jablonka"
        ],
        "abstract": "Large language models (LLMs) have gained widespread interest due to their\nability to process human language and perform tasks on which they have not been\nexplicitly trained. This is relevant for the chemical sciences, which face the\nproblem of small and diverse datasets that are frequently in the form of text.\nLLMs have shown promise in addressing these issues and are increasingly being\nharnessed to predict chemical properties, optimize reactions, and even design\nand conduct experiments autonomously. However, we still have only a very\nlimited systematic understanding of the chemical reasoning capabilities of\nLLMs, which would be required to improve models and mitigate potential harms.\nHere, we introduce \"ChemBench,\" an automated framework designed to rigorously\nevaluate the chemical knowledge and reasoning abilities of state-of-the-art\nLLMs against the expertise of human chemists. We curated more than 7,000\nquestion-answer pairs for a wide array of subfields of the chemical sciences,\nevaluated leading open and closed-source LLMs, and found that the best models\noutperformed the best human chemists in our study on average. The models,\nhowever, struggle with some chemical reasoning tasks that are easy for human\nexperts and provide overconfident, misleading predictions, such as about\nchemicals' safety profiles. These findings underscore the dual reality that,\nalthough LLMs demonstrate remarkable proficiency in chemical tasks, further\nresearch is critical to enhancing their safety and utility in chemical\nsciences. Our findings also indicate a need for adaptations to chemistry\ncurricula and highlight the importance of continuing to develop evaluation\nframeworks to improve safe and useful LLMs.",
        "publication_date": "2024-04-01T20:56:25Z",
        "upvotes": 8
    },
    "2404.02125": {
        "url": "https://arxiv.org/abs/2404.02125",
        "title": "3D Congealing: 3D-Aware Image Alignment in the Wild",
        "authors": [
            "Yunzhi Zhang",
            "Zizhang Li",
            "Amit Raj",
            "Andreas Engelhardt",
            "Yuanzhen Li",
            "Tingbo Hou",
            "Jiajun Wu",
            "Varun Jampani"
        ],
        "abstract": "We propose 3D Congealing, a novel problem of 3D-aware alignment for 2D images\ncapturing semantically similar objects. Given a collection of unlabeled\nInternet images, our goal is to associate the shared semantic parts from the\ninputs and aggregate the knowledge from 2D images to a shared 3D canonical\nspace. We introduce a general framework that tackles the task without assuming\nshape templates, poses, or any camera parameters. At its core is a canonical 3D\nrepresentation that encapsulates geometric and semantic information. The\nframework optimizes for the canonical representation together with the pose for\neach input image, and a per-image coordinate map that warps 2D pixel\ncoordinates to the 3D canonical frame to account for the shape matching. The\noptimization procedure fuses prior knowledge from a pre-trained image\ngenerative model and semantic information from input images. The former\nprovides strong knowledge guidance for this under-constraint task, while the\nlatter provides the necessary information to mitigate the training data bias\nfrom the pre-trained model. Our framework can be used for various tasks such as\ncorrespondence matching, pose estimation, and image editing, achieving strong\nresults on real-world image datasets under challenging illumination conditions\nand on in-the-wild online image collections.",
        "publication_date": "2024-04-02T17:32:12Z",
        "upvotes": 6
    },
    "2404.01617": {
        "url": "https://arxiv.org/abs/2404.01617",
        "title": "LLM-ABR: Designing Adaptive Bitrate Algorithms via Large Language Models",
        "authors": [
            "Zhiyuan He",
            "Aashish Gottipati",
            "Lili Qiu",
            "Francis Y. Yan",
            "Xufang Luo",
            "Kenuo Xu",
            "Yuqing Yang"
        ],
        "abstract": "We present LLM-ABR, the first system that utilizes the generative\ncapabilities of large language models (LLMs) to autonomously design adaptive\nbitrate (ABR) algorithms tailored for diverse network characteristics.\nOperating within a reinforcement learning framework, LLM-ABR empowers LLMs to\ndesign key components such as states and neural network architectures. We\nevaluate LLM-ABR across diverse network settings, including broadband,\nsatellite, 4G, and 5G. LLM-ABR consistently outperforms default ABR algorithms.",
        "publication_date": "2024-04-02T03:43:55Z",
        "upvotes": 6
    },
    "2404.02258": {
        "url": "https://arxiv.org/abs/2404.02258",
        "title": "Mixture-of-Depths: Dynamically allocating compute in transformer-based\n  language models",
        "authors": [
            "David Raposo",
            "Sam Ritter",
            "Blake Richards",
            "Timothy Lillicrap",
            "Peter Conway Humphreys",
            "Adam Santoro"
        ],
        "abstract": "Transformer-based language models spread FLOPs uniformly across input\nsequences. In this work we demonstrate that transformers can instead learn to\ndynamically allocate FLOPs (or compute) to specific positions in a sequence,\noptimising the allocation along the sequence for different layers across the\nmodel depth. Our method enforces a total compute budget by capping the number\nof tokens ($k$) that can participate in the self-attention and MLP computations\nat a given layer. The tokens to be processed are determined by the network\nusing a top-$k$ routing mechanism. Since $k$ is defined a priori, this simple\nprocedure uses a static computation graph with known tensor sizes, unlike other\nconditional computation techniques. Nevertheless, since the identities of the\n$k$ tokens are fluid, this method can expend FLOPs non-uniformly across the\ntime and model depth dimensions. Thus, compute expenditure is entirely\npredictable in sum total, but dynamic and context-sensitive at the token-level.\nNot only do models trained in this way learn to dynamically allocate compute,\nthey do so efficiently. These models match baseline performance for equivalent\nFLOPS and wall-clock times to train, but require a fraction of the FLOPs per\nforward pass, and can be upwards of 50\\% faster to step during post-training\nsampling.",
        "publication_date": "2024-04-02T19:28:11Z",
        "upvotes": 79
    },
    "2404.02905": {
        "url": "https://arxiv.org/abs/2404.02905",
        "title": "Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale\n  Prediction",
        "authors": [
            "Keyu Tian",
            "Yi Jiang",
            "Zehuan Yuan",
            "Bingyue Peng",
            "Liwei Wang"
        ],
        "abstract": "We present Visual AutoRegressive modeling (VAR), a new generation paradigm\nthat redefines the autoregressive learning on images as coarse-to-fine\n\"next-scale prediction\" or \"next-resolution prediction\", diverging from the\nstandard raster-scan \"next-token prediction\". This simple, intuitive\nmethodology allows autoregressive (AR) transformers to learn visual\ndistributions fast and generalize well: VAR, for the first time, makes AR\nmodels surpass diffusion transformers in image generation. On ImageNet 256x256\nbenchmark, VAR significantly improve AR baseline by improving Frechet inception\ndistance (FID) from 18.65 to 1.80, inception score (IS) from 80.4 to 356.4,\nwith around 20x faster inference speed. It is also empirically verified that\nVAR outperforms the Diffusion Transformer (DiT) in multiple dimensions\nincluding image quality, inference speed, data efficiency, and scalability.\nScaling up VAR models exhibits clear power-law scaling laws similar to those\nobserved in LLMs, with linear correlation coefficients near -0.998 as solid\nevidence. VAR further showcases zero-shot generalization ability in downstream\ntasks including image in-painting, out-painting, and editing. These results\nsuggest VAR has initially emulated the two important properties of LLMs:\nScaling Laws and zero-shot task generalization. We have released all models and\ncodes to promote the exploration of AR/VAR models for visual generation and\nunified learning.",
        "publication_date": "2024-04-03T17:59:53Z",
        "upvotes": 53
    },
    "2404.02575": {
        "url": "https://arxiv.org/abs/2404.02575",
        "title": "Language Models as Compilers: Simulating Pseudocode Execution Improves\n  Algorithmic Reasoning in Language Models",
        "authors": [
            "Hyungjoo Chae",
            "Yeonghyeon Kim",
            "Seungone Kim",
            "Kai Tzu-iunn Ong",
            "Beong-woo Kwak",
            "Moohyeon Kim",
            "Seonghwan Kim",
            "Taeyoon Kwon",
            "Jiwan Chung",
            "Youngjae Yu",
            "Jinyoung Yeo"
        ],
        "abstract": "Algorithmic reasoning refers to the ability to understand the complex\npatterns behind the problem and decompose them into a sequence of reasoning\nsteps towards the solution. Such nature of algorithmic reasoning makes it a\nchallenge for large language models (LLMs), even though they have demonstrated\npromising performance in other reasoning tasks. Within this context, some\nrecent studies use programming languages (e.g., Python) to express the\nnecessary logic for solving a given instance/question (e.g.,\nProgram-of-Thought) as inspired by their strict and precise syntaxes. However,\nit is non-trivial to write an executable code that expresses the correct logic\non the fly within a single inference call. Also, the code generated\nspecifically for an instance cannot be reused for others, even if they are from\nthe same task and might require identical logic to solve. This paper presents\nThink-and-Execute, a novel framework that decomposes the reasoning process of\nlanguage models into two steps. (1) In Think, we discover a task-level logic\nthat is shared across all instances for solving a given task and then express\nthe logic with pseudocode; (2) In Execute, we further tailor the generated\npseudocode to each instance and simulate the execution of the code. With\nextensive experiments on seven algorithmic reasoning tasks, we demonstrate the\neffectiveness of Think-and-Execute. Our approach better improves LMs' reasoning\ncompared to several strong baselines performing instance-specific reasoning\n(e.g., CoT and PoT), suggesting the helpfulness of discovering task-level\nlogic. Also, we show that compared to natural language, pseudocode can better\nguide the reasoning of LMs, even though they are trained to follow natural\nlanguage instructions.",
        "publication_date": "2024-04-03T08:49:11Z",
        "upvotes": 43
    },
    "2404.02893": {
        "url": "https://arxiv.org/abs/2404.02893",
        "title": "ChatGLM-Math: Improving Math Problem-Solving in Large Language Models\n  with a Self-Critique Pipeline",
        "authors": [
            "Yifan Xu",
            "Xiao Liu",
            "Xinghan Liu",
            "Zhenyu Hou",
            "Yueyan Li",
            "Xiaohan Zhang",
            "Zihan Wang",
            "Aohan Zeng",
            "Zhengxiao Du",
            "Wenyi Zhao",
            "Jie Tang",
            "Yuxiao Dong"
        ],
        "abstract": "Large language models (LLMs) have shown excellent mastering of human\nlanguage, but still struggle in real-world applications that require\nmathematical problem-solving. While many strategies and datasets to enhance\nLLMs' mathematics are developed, it remains a challenge to simultaneously\nmaintain and improve both language and mathematical capabilities in deployed\nLLM systems.In this work, we tailor the Self-Critique pipeline, which addresses\nthe challenge in the feedback learning stage of LLM alignment. We first train a\ngeneral Math-Critique model from the LLM itself to provide feedback signals.\nThen, we sequentially employ rejective fine-tuning and direct preference\noptimization over the LLM's own generations for data collection. Based on\nChatGLM3-32B, we conduct a series of experiments on both academic and our newly\ncreated challenging dataset, MathUserEval. Results show that our pipeline\nsignificantly enhances the LLM's mathematical problem-solving while still\nimproving its language ability, outperforming LLMs that could be two times\nlarger. Related techniques have been deployed to\nChatGLM\\footnote{\\url{https://chatglm.cn}}, an online serving LLM. Related\nevaluation dataset and scripts are released at\n\\url{https://github.com/THUDM/ChatGLM-Math}.",
        "publication_date": "2024-04-03T17:51:18Z",
        "upvotes": 16
    },
    "2404.02883": {
        "url": "https://arxiv.org/abs/2404.02883",
        "title": "On the Scalability of Diffusion-based Text-to-Image Generation",
        "authors": [
            "Hao Li",
            "Yang Zou",
            "Ying Wang",
            "Orchid Majumder",
            "Yusheng Xie",
            "R. Manmatha",
            "Ashwin Swaminathan",
            "Zhuowen Tu",
            "Stefano Ermon",
            "Stefano Soatto"
        ],
        "abstract": "Scaling up model and data size has been quite successful for the evolution of\nLLMs. However, the scaling law for the diffusion based text-to-image (T2I)\nmodels is not fully explored. It is also unclear how to efficiently scale the\nmodel for better performance at reduced cost. The different training settings\nand expensive training cost make a fair model comparison extremely difficult.\nIn this work, we empirically study the scaling properties of diffusion based\nT2I models by performing extensive and rigours ablations on scaling both\ndenoising backbones and training set, including training scaled UNet and\nTransformer variants ranging from 0.4B to 4B parameters on datasets upto 600M\nimages. For model scaling, we find the location and amount of cross attention\ndistinguishes the performance of existing UNet designs. And increasing the\ntransformer blocks is more parameter-efficient for improving text-image\nalignment than increasing channel numbers. We then identify an efficient UNet\nvariant, which is 45% smaller and 28% faster than SDXL's UNet. On the data\nscaling side, we show the quality and diversity of the training set matters\nmore than simply dataset size. Increasing caption density and diversity\nimproves text-image alignment performance and the learning efficiency. Finally,\nwe provide scaling functions to predict the text-image alignment performance as\nfunctions of the scale of model size, compute and dataset size.",
        "publication_date": "2024-04-03T17:34:28Z",
        "upvotes": 16
    },
    "2404.02733": {
        "url": "https://arxiv.org/abs/2404.02733",
        "title": "InstantStyle: Free Lunch towards Style-Preserving in Text-to-Image\n  Generation",
        "authors": [
            "Haofan Wang",
            "Matteo Spinelli",
            "Qixun Wang",
            "Xu Bai",
            "Zekui Qin",
            "Anthony Chen"
        ],
        "abstract": "Tuning-free diffusion-based models have demonstrated significant potential in\nthe realm of image personalization and customization. However, despite this\nnotable progress, current models continue to grapple with several complex\nchallenges in producing style-consistent image generation. Firstly, the concept\nof style is inherently underdetermined, encompassing a multitude of elements\nsuch as color, material, atmosphere, design, and structure, among others.\nSecondly, inversion-based methods are prone to style degradation, often\nresulting in the loss of fine-grained details. Lastly, adapter-based approaches\nfrequently require meticulous weight tuning for each reference image to achieve\na balance between style intensity and text controllability. In this paper, we\ncommence by examining several compelling yet frequently overlooked\nobservations. We then proceed to introduce InstantStyle, a framework designed\nto address these issues through the implementation of two key strategies: 1) A\nstraightforward mechanism that decouples style and content from reference\nimages within the feature space, predicated on the assumption that features\nwithin the same space can be either added to or subtracted from one another. 2)\nThe injection of reference image features exclusively into style-specific\nblocks, thereby preventing style leaks and eschewing the need for cumbersome\nweight tuning, which often characterizes more parameter-heavy designs.Our work\ndemonstrates superior visual stylization outcomes, striking an optimal balance\nbetween the intensity of style and the controllability of textual elements. Our\ncodes will be available at https://github.com/InstantStyle/InstantStyle.",
        "publication_date": "2024-04-03T13:34:09Z",
        "upvotes": 16
    },
    "2404.02747": {
        "url": "https://arxiv.org/abs/2404.02747",
        "title": "Cross-Attention Makes Inference Cumbersome in Text-to-Image Diffusion\n  Models",
        "authors": [
            "Wentian Zhang",
            "Haozhe Liu",
            "Jinheng Xie",
            "Francesco Faccio",
            "Mike Zheng Shou",
            "J\u00fcrgen Schmidhuber"
        ],
        "abstract": "This study explores the role of cross-attention during inference in\ntext-conditional diffusion models. We find that cross-attention outputs\nconverge to a fixed point after few inference steps. Accordingly, the time\npoint of convergence naturally divides the entire inference process into two\nstages: an initial semantics-planning stage, during which, the model relies on\ncross-attention to plan text-oriented visual semantics, and a subsequent\nfidelity-improving stage, during which the model tries to generate images from\npreviously planned semantics. Surprisingly, ignoring text conditions in the\nfidelity-improving stage not only reduces computation complexity, but also\nmaintains model performance. This yields a simple and training-free method\ncalled TGATE for efficient generation, which caches the cross-attention output\nonce it converges and keeps it fixed during the remaining inference steps. Our\nempirical study on the MS-COCO validation set confirms its effectiveness. The\nsource code of TGATE is available at https://github.com/HaozheLiu-ST/T-GATE.",
        "publication_date": "2024-04-03T13:44:41Z",
        "upvotes": 11
    },
    "2404.02514": {
        "url": "https://arxiv.org/abs/2404.02514",
        "title": "Freditor: High-Fidelity and Transferable NeRF Editing by Frequency\n  Decomposition",
        "authors": [
            "Yisheng He",
            "Weihao Yuan",
            "Siyu Zhu",
            "Zilong Dong",
            "Liefeng Bo",
            "Qixing Huang"
        ],
        "abstract": "This paper enables high-fidelity, transferable NeRF editing by frequency\ndecomposition. Recent NeRF editing pipelines lift 2D stylization results to 3D\nscenes while suffering from blurry results, and fail to capture detailed\nstructures caused by the inconsistency between 2D editings. Our critical\ninsight is that low-frequency components of images are more\nmultiview-consistent after editing compared with their high-frequency parts.\nMoreover, the appearance style is mainly exhibited on the low-frequency\ncomponents, and the content details especially reside in high-frequency parts.\nThis motivates us to perform editing on low-frequency components, which results\nin high-fidelity edited scenes. In addition, the editing is performed in the\nlow-frequency feature space, enabling stable intensity control and novel scene\ntransfer. Comprehensive experiments conducted on photorealistic datasets\ndemonstrate the superior performance of high-fidelity and transferable NeRF\nediting. The project page is at \\url{https://aigc3d.github.io/freditor}.",
        "publication_date": "2024-04-03T07:07:02Z",
        "upvotes": 9
    },
    "2404.03592": {
        "url": "https://arxiv.org/abs/2404.03592",
        "title": "ReFT: Representation Finetuning for Language Models",
        "authors": [
            "Zhengxuan Wu",
            "Aryaman Arora",
            "Zheng Wang",
            "Atticus Geiger",
            "Dan Jurafsky",
            "Christopher D. Manning",
            "Christopher Potts"
        ],
        "abstract": "Parameter-efficient fine-tuning (PEFT) methods seek to adapt large models via\nupdates to a small number of weights. However, much prior interpretability work\nhas shown that representations encode rich semantic information, suggesting\nthat editing representations might be a more powerful alternative. Here, we\npursue this hypothesis by developing a family of $\\textbf{Representation\nFinetuning (ReFT)}$ methods. ReFT methods operate on a frozen base model and\nlearn task-specific interventions on hidden representations. We define a strong\ninstance of the ReFT family, Low-rank Linear Subspace ReFT (LoReFT). LoReFT is\na drop-in replacement for existing PEFTs and learns interventions that are\n10x-50x more parameter-efficient than prior state-of-the-art PEFTs. We showcase\nLoReFT on eight commonsense reasoning tasks, four arithmetic reasoning tasks,\nAlpaca-Eval v1.0, and GLUE. In all these evaluations, LoReFT delivers the best\nbalance of efficiency and performance, and almost always outperforms\nstate-of-the-art PEFTs. We release a generic ReFT training library publicly at\nhttps://github.com/stanfordnlp/pyreft.",
        "publication_date": "2024-04-04T17:00:37Z",
        "upvotes": 49
    },
    "2404.03653": {
        "url": "https://arxiv.org/abs/2404.03653",
        "title": "CoMat: Aligning Text-to-Image Diffusion Model with Image-to-Text Concept\n  Matching",
        "authors": [
            "Dongzhi Jiang",
            "Guanglu Song",
            "Xiaoshi Wu",
            "Renrui Zhang",
            "Dazhong Shen",
            "Zhuofan Zong",
            "Yu Liu",
            "Hongsheng Li"
        ],
        "abstract": "Diffusion models have demonstrated great success in the field of\ntext-to-image generation. However, alleviating the misalignment between the\ntext prompts and images is still challenging. The root reason behind the\nmisalignment has not been extensively investigated. We observe that the\nmisalignment is caused by inadequate token attention activation. We further\nattribute this phenomenon to the diffusion model's insufficient condition\nutilization, which is caused by its training paradigm. To address the issue, we\npropose CoMat, an end-to-end diffusion model fine-tuning strategy with an\nimage-to-text concept matching mechanism. We leverage an image captioning model\nto measure image-to-text alignment and guide the diffusion model to revisit\nignored tokens. A novel attribute concentration module is also proposed to\naddress the attribute binding problem. Without any image or human preference\ndata, we use only 20K text prompts to fine-tune SDXL to obtain CoMat-SDXL.\nExtensive experiments show that CoMat-SDXL significantly outperforms the\nbaseline model SDXL in two text-to-image alignment benchmarks and achieves\nstart-of-the-art performance.",
        "publication_date": "2024-04-04T17:59:46Z",
        "upvotes": 25
    },
    "2404.03413": {
        "url": "https://arxiv.org/abs/2404.03413",
        "title": "MiniGPT4-Video: Advancing Multimodal LLMs for Video Understanding with\n  Interleaved Visual-Textual Tokens",
        "authors": [
            "Kirolos Ataallah",
            "Xiaoqian Shen",
            "Eslam Abdelrahman",
            "Essam Sleiman",
            "Deyao Zhu",
            "Jian Ding",
            "Mohamed Elhoseiny"
        ],
        "abstract": "This paper introduces MiniGPT4-Video, a multimodal Large Language Model (LLM)\ndesigned specifically for video understanding. The model is capable of\nprocessing both temporal visual and textual data, making it adept at\nunderstanding the complexities of videos. Building upon the success of\nMiniGPT-v2, which excelled in translating visual features into the LLM space\nfor single images and achieved impressive results on various image-text\nbenchmarks, this paper extends the model's capabilities to process a sequence\nof frames, enabling it to comprehend videos. MiniGPT4-video does not only\nconsider visual content but also incorporates textual conversations, allowing\nthe model to effectively answer queries involving both visual and text\ncomponents. The proposed model outperforms existing state-of-the-art methods,\nregistering gains of 4.22%, 1.13%, 20.82%, and 13.1% on the MSVD, MSRVTT, TGIF,\nand TVQA benchmarks respectively. Our models and code have been made publicly\navailable here https://vision-cair.github.io/MiniGPT4-video/",
        "publication_date": "2024-04-04T12:46:01Z",
        "upvotes": 20
    },
    "2404.03626": {
        "url": "https://arxiv.org/abs/2404.03626",
        "title": "Training LLMs over Neurally Compressed Text",
        "authors": [
            "Brian Lester",
            "Jaehoon Lee",
            "Alex Alemi",
            "Jeffrey Pennington",
            "Adam Roberts",
            "Jascha Sohl-Dickstein",
            "Noah Constant"
        ],
        "abstract": "In this paper, we explore the idea of training large language models (LLMs)\nover highly compressed text. While standard subword tokenizers compress text by\na small factor, neural text compressors can achieve much higher rates of\ncompression. If it were possible to train LLMs directly over neurally\ncompressed text, this would confer advantages in training and serving\nefficiency, as well as easier handling of long text spans. The main obstacle to\nthis goal is that strong compression tends to produce opaque outputs that are\nnot well-suited for learning. In particular, we find that text na\\\"ively\ncompressed via Arithmetic Coding is not readily learnable by LLMs. To overcome\nthis, we propose Equal-Info Windows, a novel compression technique whereby text\nis segmented into blocks that each compress to the same bit length. Using this\nmethod, we demonstrate effective learning over neurally compressed text that\nimproves with scale, and outperforms byte-level baselines by a wide margin on\nperplexity and inference speed benchmarks. While our method delivers worse\nperplexity than subword tokenizers for models trained with the same parameter\ncount, it has the benefit of shorter sequence lengths. Shorter sequence lengths\nrequire fewer autoregressive generation steps, and reduce latency. Finally, we\nprovide extensive analysis of the properties that contribute to learnability,\nand offer concrete suggestions for how to further improve the performance of\nhigh-compression tokenizers.",
        "publication_date": "2024-04-04T17:48:28Z",
        "upvotes": 19
    },
    "2404.03118": {
        "url": "https://arxiv.org/abs/2404.03118",
        "title": "LVLM-Intrepret: An Interpretability Tool for Large Vision-Language\n  Models",
        "authors": [
            "Gabriela Ben Melech Stan",
            "Raanan Yehezkel Rohekar",
            "Yaniv Gurwicz",
            "Matthew Lyle Olson",
            "Anahita Bhiwandiwalla",
            "Estelle Aflalo",
            "Chenfei Wu",
            "Nan Duan",
            "Shao-Yen Tseng",
            "Vasudev Lal"
        ],
        "abstract": "In the rapidly evolving landscape of artificial intelligence, multi-modal\nlarge language models are emerging as a significant area of interest. These\nmodels, which combine various forms of data input, are becoming increasingly\npopular. However, understanding their internal mechanisms remains a complex\ntask. Numerous advancements have been made in the field of explainability tools\nand mechanisms, yet there is still much to explore. In this work, we present a\nnovel interactive application aimed towards understanding the internal\nmechanisms of large vision-language models. Our interface is designed to\nenhance the interpretability of the image patches, which are instrumental in\ngenerating an answer, and assess the efficacy of the language model in\ngrounding its output in the image. With our application, a user can\nsystematically investigate the model and uncover system limitations, paving the\nway for enhancements in system capabilities. Finally, we present a case study\nof how our application can aid in understanding failure mechanisms in a popular\nlarge multi-modal model: LLaVA.",
        "publication_date": "2024-04-03T23:57:34Z",
        "upvotes": 15
    },
    "2404.03648": {
        "url": "https://arxiv.org/abs/2404.03648",
        "title": "AutoWebGLM: Bootstrap And Reinforce A Large Language Model-based Web\n  Navigating Agent",
        "authors": [
            "Hanyu Lai",
            "Xiao Liu",
            "Iat Long Iong",
            "Shuntian Yao",
            "Yuxuan Chen",
            "Pengbo Shen",
            "Hao Yu",
            "Hanchen Zhang",
            "Xiaohan Zhang",
            "Yuxiao Dong",
            "Jie Tang"
        ],
        "abstract": "Large language models (LLMs) have fueled many intelligent agent tasks, such\nas web navigation -- but most existing agents perform far from satisfying in\nreal-world webpages due to three factors: (1) the versatility of actions on\nwebpages, (2) HTML text exceeding model processing capacity, and (3) the\ncomplexity of decision-making due to the open-domain nature of web. In light of\nthe challenge, we develop AutoWebGLM, a GPT-4-outperforming automated web\nnavigation agent built upon ChatGLM3-6B. Inspired by human browsing patterns,\nwe design an HTML simplification algorithm to represent webpages, preserving\nvital information succinctly. We employ a hybrid human-AI method to build web\nbrowsing data for curriculum training. Then, we bootstrap the model by\nreinforcement learning and rejection sampling to further facilitate webpage\ncomprehension, browser operations, and efficient task decomposition by itself.\nFor testing, we establish a bilingual benchmark -- AutoWebBench -- for\nreal-world web browsing tasks. We evaluate AutoWebGLM across diverse web\nnavigation benchmarks, revealing its improvements but also underlying\nchallenges to tackle real environments. Related code, model, and data will be\nreleased at \\url{https://github.com/THUDM/AutoWebGLM}.",
        "publication_date": "2024-04-04T17:58:40Z",
        "upvotes": 15
    },
    "2404.03543": {
        "url": "https://arxiv.org/abs/2404.03543",
        "title": "CodeEditorBench: Evaluating Code Editing Capability of Large Language\n  Models",
        "authors": [
            "Jiawei Guo",
            "Ziming Li",
            "Xueling Liu",
            "Kaijing Ma",
            "Tianyu Zheng",
            "Zhouliang Yu",
            "Ding Pan",
            "Yizhi LI",
            "Ruibo Liu",
            "Yue Wang",
            "Shuyue Guo",
            "Xingwei Qu",
            "Xiang Yue",
            "Ge Zhang",
            "Wenhu Chen",
            "Jie Fu"
        ],
        "abstract": "Large Language Models (LLMs) for code are rapidly evolving, with code editing\nemerging as a critical capability. We introduce CodeEditorBench, an evaluation\nframework designed to rigorously assess the performance of LLMs in code editing\ntasks, including debugging, translating, polishing, and requirement switching.\nUnlike existing benchmarks focusing solely on code generation, CodeEditorBench\nemphasizes real-world scenarios and practical aspects of software development.\nWe curate diverse coding challenges and scenarios from five sources, covering\nvarious programming languages, complexity levels, and editing tasks. Evaluation\nof 19 LLMs reveals that closed-source models (particularly Gemini-Ultra and\nGPT-4), outperform open-source models in CodeEditorBench, highlighting\ndifferences in model performance based on problem types and prompt\nsensitivities. CodeEditorBench aims to catalyze advancements in LLMs by\nproviding a robust platform for assessing code editing capabilities. We will\nrelease all prompts and datasets to enable the community to expand the dataset\nand benchmark emerging LLMs. By introducing CodeEditorBench, we contribute to\nthe advancement of LLMs in code editing and provide a valuable resource for\nresearchers and practitioners.",
        "publication_date": "2024-04-04T15:49:49Z",
        "upvotes": 14
    },
    "2404.03566": {
        "url": "https://arxiv.org/abs/2404.03566",
        "title": "PointInfinity: Resolution-Invariant Point Diffusion Models",
        "authors": [
            "Zixuan Huang",
            "Justin Johnson",
            "Shoubhik Debnath",
            "James M. Rehg",
            "Chao-Yuan Wu"
        ],
        "abstract": "We present PointInfinity, an efficient family of point cloud diffusion\nmodels. Our core idea is to use a transformer-based architecture with a\nfixed-size, resolution-invariant latent representation. This enables efficient\ntraining with low-resolution point clouds, while allowing high-resolution point\nclouds to be generated during inference. More importantly, we show that scaling\nthe test-time resolution beyond the training resolution improves the fidelity\nof generated point clouds and surfaces. We analyze this phenomenon and draw a\nlink to classifier-free guidance commonly used in diffusion models,\ndemonstrating that both allow trading off fidelity and variability during\ninference. Experiments on CO3D show that PointInfinity can efficiently generate\nhigh-resolution point clouds (up to 131k points, 31 times more than Point-E)\nwith state-of-the-art quality.",
        "publication_date": "2024-04-04T16:24:32Z",
        "upvotes": 12
    },
    "2404.03411": {
        "url": "https://arxiv.org/abs/2404.03411",
        "title": "Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak\n  Attacks?",
        "authors": [
            "Shuo Chen",
            "Zhen Han",
            "Bailan He",
            "Zifeng Ding",
            "Wenqian Yu",
            "Philip Torr",
            "Volker Tresp",
            "Jindong Gu"
        ],
        "abstract": "Various jailbreak attacks have been proposed to red-team Large Language\nModels (LLMs) and revealed the vulnerable safeguards of LLMs. Besides, some\nmethods are not limited to the textual modality and extend the jailbreak attack\nto Multimodal Large Language Models (MLLMs) by perturbing the visual input.\nHowever, the absence of a universal evaluation benchmark complicates the\nperformance reproduction and fair comparison. Besides, there is a lack of\ncomprehensive evaluation of closed-source state-of-the-art (SOTA) models,\nespecially MLLMs, such as GPT-4V. To address these issues, this work first\nbuilds a comprehensive jailbreak evaluation dataset with 1445 harmful questions\ncovering 11 different safety policies. Based on this dataset, extensive\nred-teaming experiments are conducted on 11 different LLMs and MLLMs, including\nboth SOTA proprietary models and open-source models. We then conduct a deep\nanalysis of the evaluated results and find that (1) GPT4 and GPT-4V demonstrate\nbetter robustness against jailbreak attacks compared to open-source LLMs and\nMLLMs. (2) Llama2 and Qwen-VL-Chat are more robust compared to other\nopen-source models. (3) The transferability of visual jailbreak methods is\nrelatively limited compared to textual jailbreak methods. The dataset and code\ncan be found here\nhttps://anonymous.4open.science/r/red_teaming_gpt4-C1CE/README.md .",
        "publication_date": "2024-04-04T12:38:14Z",
        "upvotes": 8
    },
    "2404.03204": {
        "url": "https://arxiv.org/abs/2404.03204",
        "title": "RALL-E: Robust Codec Language Modeling with Chain-of-Thought Prompting\n  for Text-to-Speech Synthesis",
        "authors": [
            "Detai Xin",
            "Xu Tan",
            "Kai Shen",
            "Zeqian Ju",
            "Dongchao Yang",
            "Yuancheng Wang",
            "Shinnosuke Takamichi",
            "Hiroshi Saruwatari",
            "Shujie Liu",
            "Jinyu Li",
            "Sheng Zhao"
        ],
        "abstract": "We present RALL-E, a robust language modeling method for text-to-speech (TTS)\nsynthesis. While previous work based on large language models (LLMs) shows\nimpressive performance on zero-shot TTS, such methods often suffer from poor\nrobustness, such as unstable prosody (weird pitch and rhythm/duration) and a\nhigh word error rate (WER), due to the autoregressive prediction style of\nlanguage models. The core idea behind RALL-E is chain-of-thought (CoT)\nprompting, which decomposes the task into simpler steps to enhance the\nrobustness of LLM-based TTS. To accomplish this idea, RALL-E first predicts\nprosody features (pitch and duration) of the input text and uses them as\nintermediate conditions to predict speech tokens in a CoT style. Second, RALL-E\nutilizes the predicted duration prompt to guide the computing of self-attention\nweights in Transformer to enforce the model to focus on the corresponding\nphonemes and prosody features when predicting speech tokens. Results of\ncomprehensive objective and subjective evaluations demonstrate that, compared\nto a powerful baseline method VALL-E, RALL-E significantly improves the WER of\nzero-shot TTS from $6.3\\%$ (without reranking) and $2.1\\%$ (with reranking) to\n$2.8\\%$ and $1.0\\%$, respectively. Furthermore, we demonstrate that RALL-E\ncorrectly synthesizes sentences that are hard for VALL-E and reduces the error\nrate from $68\\%$ to $4\\%$.",
        "publication_date": "2024-04-04T05:15:07Z",
        "upvotes": 7
    },
    "2404.04125": {
        "url": "https://arxiv.org/abs/2404.04125",
        "title": "No \"Zero-Shot\" Without Exponential Data: Pretraining Concept Frequency\n  Determines Multimodal Model Performance",
        "authors": [
            "Vishaal Udandarao",
            "Ameya Prabhu",
            "Adhiraj Ghosh",
            "Yash Sharma",
            "Philip H. S. Torr",
            "Adel Bibi",
            "Samuel Albanie",
            "Matthias Bethge"
        ],
        "abstract": "Web-crawled pretraining datasets underlie the impressive \"zero-shot\"\nevaluation performance of multimodal models, such as CLIP for\nclassification/retrieval and Stable-Diffusion for image generation. However, it\nis unclear how meaningful the notion of \"zero-shot\" generalization is for such\nmultimodal models, as it is not known to what extent their pretraining datasets\nencompass the downstream concepts targeted for during \"zero-shot\" evaluation.\nIn this work, we ask: How is the performance of multimodal models on downstream\nconcepts influenced by the frequency of these concepts in their pretraining\ndatasets? We comprehensively investigate this question across 34 models and\nfive standard pretraining datasets (CC-3M, CC-12M, YFCC-15M, LAION-400M,\nLAION-Aesthetics), generating over 300GB of data artifacts. We consistently\nfind that, far from exhibiting \"zero-shot\" generalization, multimodal models\nrequire exponentially more data to achieve linear improvements in downstream\n\"zero-shot\" performance, following a sample inefficient log-linear scaling\ntrend. This trend persists even when controlling for sample-level similarity\nbetween pretraining and downstream datasets, and testing on purely synthetic\ndata distributions. Furthermore, upon benchmarking models on long-tailed data\nsampled based on our analysis, we demonstrate that multimodal models across the\nboard perform poorly. We contribute this long-tail test set as the \"Let it\nWag!\" benchmark to further research in this direction. Taken together, our\nstudy reveals an exponential need for training data which implies that the key\nto \"zero-shot\" generalization capabilities under large-scale training paradigms\nremains to be found.",
        "publication_date": "2024-04-04T17:58:02Z",
        "upvotes": 10
    },
    "2404.03715": {
        "url": "https://arxiv.org/abs/2404.03715",
        "title": "Direct Nash Optimization: Teaching Language Models to Self-Improve with\n  General Preferences",
        "authors": [
            "Corby Rosset",
            "Ching-An Cheng",
            "Arindam Mitra",
            "Michael Santacroce",
            "Ahmed Awadallah",
            "Tengyang Xie"
        ],
        "abstract": "This paper studies post-training large language models (LLMs) using\npreference feedback from a powerful oracle to help a model iteratively improve\nover itself. The typical approach for post-training LLMs involves Reinforcement\nLearning from Human Feedback (RLHF), which traditionally separates reward\nlearning and subsequent policy optimization. However, such a reward\nmaximization approach is limited by the nature of \"point-wise\" rewards (such as\nBradley-Terry model), which fails to express complex intransitive or cyclic\npreference relations. While advances on RLHF show reward learning and policy\noptimization can be merged into a single contrastive objective for stability,\nthey yet still remain tethered to the reward maximization framework. Recently,\na new wave of research sidesteps the reward maximization presumptions in favor\nof directly optimizing over \"pair-wise\" or general preferences. In this paper,\nwe introduce Direct Nash Optimization (DNO), a provable and scalable algorithm\nthat marries the simplicity and stability of contrastive learning with\ntheoretical generality from optimizing general preferences. Because DNO is a\nbatched on-policy algorithm using a regression-based objective, its\nimplementation is straightforward and efficient. Moreover, DNO enjoys monotonic\nimprovement across iterations that help it improve even over a strong teacher\n(such as GPT-4). In our experiments, a resulting 7B parameter Orca-2.5 model\naligned by DNO achieves the state-of-the-art win-rate against GPT-4-Turbo of\n33% on AlpacaEval 2.0 (even after controlling for response length), an absolute\ngain of 26% (7% to 33%) over the initializing model. It outperforms models with\nfar more parameters, including Mistral Large, Self-Rewarding LM (70B\nparameters), and older versions of GPT-4.",
        "publication_date": "2024-04-04T17:56:41Z",
        "upvotes": 9
    },
    "2404.03683": {
        "url": "https://arxiv.org/abs/2404.03683",
        "title": "Stream of Search (SoS): Learning to Search in Language",
        "authors": [
            "Kanishk Gandhi",
            "Denise Lee",
            "Gabriel Grand",
            "Muxin Liu",
            "Winson Cheng",
            "Archit Sharma",
            "Noah D. Goodman"
        ],
        "abstract": "Language models are rarely shown fruitful mistakes while training. They then\nstruggle to look beyond the next token, suffering from a snowballing of errors\nand struggling to predict the consequence of their actions several steps ahead.\nIn this paper, we show how language models can be taught to search by\nrepresenting the process of search in language, as a flattened string -- a\nstream of search (SoS). We propose a unified language for search that captures\nan array of different symbolic search strategies. We demonstrate our approach\nusing the simple yet difficult game of Countdown, where the goal is to combine\ninput numbers with arithmetic operations to reach a target number. We pretrain\na transformer-based language model from scratch on a dataset of streams of\nsearch generated by heuristic solvers. We find that SoS pretraining increases\nsearch accuracy by 25% over models trained to predict only the optimal search\ntrajectory. We further finetune this model with two policy improvement methods:\nAdvantage-Induced Policy Alignment (APA) and Self-Taught Reasoner (STaR). The\nfinetuned SoS models solve 36% of previously unsolved problems, including\nproblems that cannot be solved by any of the heuristic solvers. Our results\nindicate that language models can learn to solve problems via search,\nself-improve to flexibly use different search strategies, and potentially\ndiscover new ones.",
        "publication_date": "2024-04-01T06:50:52Z",
        "upvotes": 7
    },
    "2404.03673": {
        "url": "https://arxiv.org/abs/2404.03673",
        "title": "RL for Consistency Models: Faster Reward Guided Text-to-Image Generation",
        "authors": [
            "Owen Oertell",
            "Jonathan D. Chang",
            "Yiyi Zhang",
            "Kiant\u00e9 Brantley",
            "Wen Sun"
        ],
        "abstract": "Reinforcement learning (RL) has improved guided image generation with\ndiffusion models by directly optimizing rewards that capture image quality,\naesthetics, and instruction following capabilities. However, the resulting\ngenerative policies inherit the same iterative sampling process of diffusion\nmodels that causes slow generation. To overcome this limitation, consistency\nmodels proposed learning a new class of generative models that directly map\nnoise to data, resulting in a model that can generate an image in as few as one\nsampling iteration. In this work, to optimize text-to-image generative models\nfor task specific rewards and enable fast training and inference, we propose a\nframework for fine-tuning consistency models via RL. Our framework, called\nReinforcement Learning for Consistency Model (RLCM), frames the iterative\ninference process of a consistency model as an RL procedure. RLCM improves upon\nRL fine-tuned diffusion models on text-to-image generation capabilities and\ntrades computation during inference time for sample quality. Experimentally, we\nshow that RLCM can adapt text-to-image consistency models to objectives that\nare challenging to express with prompting, such as image compressibility, and\nthose derived from human feedback, such as aesthetic quality. Comparing to RL\nfinetuned diffusion models, RLCM trains significantly faster, improves the\nquality of the generation measured under the reward objectives, and speeds up\nthe inference procedure by generating high quality images with as few as two\ninference steps. Our code is available at https://rlcm.owenoertell.com",
        "publication_date": "2024-03-25T15:40:22Z",
        "upvotes": 6
    },
    "2404.03820": {
        "url": "https://arxiv.org/abs/2404.03820",
        "title": "CantTalkAboutThis: Aligning Language Models to Stay on Topic in\n  Dialogues",
        "authors": [
            "Makesh Narsimhan Sreedhar",
            "Traian Rebedea",
            "Shaona Ghosh",
            "Christopher Parisien"
        ],
        "abstract": "Recent advancements in instruction-tuning datasets have predominantly focused\non specific tasks like mathematical or logical reasoning. There has been a\nnotable gap in data designed for aligning language models to maintain topic\nrelevance in conversations - a critical aspect for deploying chatbots to\nproduction. We introduce the CantTalkAboutThis dataset to help language models\nremain focused on the subject at hand during task-oriented interactions. It\nconsists of synthetic dialogues on a wide range of conversation topics from\ndifferent domains. These dialogues are interspersed with distractor turns that\nintentionally divert the chatbot from the predefined topic. Fine-tuning\nlanguage models on this dataset helps make them resilient to deviating from the\nrole assigned and improves their ability to maintain topical coherence compared\nto general-purpose instruction-tuned LLMs like GPT-4-turbo and\nMixtral-Instruct. Additionally, preliminary observations suggest that training\nmodels on this dataset also enhance their performance on fine-grained\ninstruction following tasks.",
        "publication_date": "2024-04-04T22:31:58Z",
        "upvotes": 5
    },
    "2404.04167": {
        "url": "https://arxiv.org/abs/2404.04167",
        "title": "Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model",
        "authors": [
            "Xinrun Du",
            "Zhouliang Yu",
            "Songyang Gao",
            "Ding Pan",
            "Yuyang Cheng",
            "Ziyang Ma",
            "Ruibin Yuan",
            "Xingwei Qu",
            "Jiaheng Liu",
            "Tianyu Zheng",
            "Xinchen Luo",
            "Guorui Zhou",
            "Binhang Yuan",
            "Wenhu Chen",
            "Jie Fu",
            "Ge Zhang"
        ],
        "abstract": "In this study, we introduce CT-LLM, a 2B large language model (LLM) that\nillustrates a pivotal shift towards prioritizing the Chinese language in\ndeveloping LLMs. Uniquely initiated from scratch, CT-LLM diverges from the\nconventional methodology by primarily incorporating Chinese textual data,\nutilizing an extensive corpus of 1,200 billion tokens, including 800 billion\nChinese tokens, 300 billion English tokens, and 100 billion code tokens. This\nstrategic composition facilitates the model's exceptional proficiency in\nunderstanding and processing Chinese, a capability further enhanced through\nalignment techniques. Demonstrating remarkable performance on the CHC-Bench,\nCT-LLM excels in Chinese language tasks, and showcases its adeptness in English\nthrough SFT. This research challenges the prevailing paradigm of training LLMs\npredominantly on English corpora and then adapting them to other languages,\nbroadening the horizons for LLM training methodologies. By open-sourcing the\nfull process of training a Chinese LLM, including a detailed data processing\nprocedure with the obtained Massive Appropriate Pretraining Chinese Corpus\n(MAP-CC), a well-chosen multidisciplinary Chinese Hard Case Benchmark\n(CHC-Bench), and the 2B-size Chinese Tiny LLM (CT-LLM), we aim to foster\nfurther exploration and innovation in both academia and industry, paving the\nway for more inclusive and versatile language models.",
        "publication_date": "2024-04-05T15:20:02Z",
        "upvotes": 5
    },
    "2404.04204": {
        "url": "https://arxiv.org/abs/2404.04204",
        "title": "Social Skill Training with Large Language Models",
        "authors": [
            "Diyi Yang",
            "Caleb Ziems",
            "William Held",
            "Omar Shaikh",
            "Michael S. Bernstein",
            "John Mitchell"
        ],
        "abstract": "People rely on social skills like conflict resolution to communicate\neffectively and to thrive in both work and personal life. However, practice\nenvironments for social skills are typically out of reach for most people. How\ncan we make social skill training more available, accessible, and inviting?\nDrawing upon interdisciplinary research from communication and psychology, this\nperspective paper identifies social skill barriers to enter specialized fields.\nThen we present a solution that leverages large language models for social\nskill training via a generic framework. Our AI Partner, AI Mentor framework\nmerges experiential learning with realistic practice and tailored feedback.\nThis work ultimately calls for cross-disciplinary innovation to address the\nbroader implications for workforce development and social equality.",
        "publication_date": "2024-04-05T16:29:58Z",
        "upvotes": 4
    },
    "2404.04211": {
        "url": "https://arxiv.org/abs/2404.04211",
        "title": "Robust Gaussian Splatting",
        "authors": [
            "Fran\u00e7ois Darmon",
            "Lorenzo Porzi",
            "Samuel Rota-Bul\u00f2",
            "Peter Kontschieder"
        ],
        "abstract": "In this paper, we address common error sources for 3D Gaussian Splatting\n(3DGS) including blur, imperfect camera poses, and color inconsistencies, with\nthe goal of improving its robustness for practical applications like\nreconstructions from handheld phone captures. Our main contribution involves\nmodeling motion blur as a Gaussian distribution over camera poses, allowing us\nto address both camera pose refinement and motion blur correction in a unified\nway. Additionally, we propose mechanisms for defocus blur compensation and for\naddressing color in-consistencies caused by ambient light, shadows, or due to\ncamera-related factors like varying white balancing settings. Our proposed\nsolutions integrate in a seamless way with the 3DGS formulation while\nmaintaining its benefits in terms of training efficiency and rendering speed.\nWe experimentally validate our contributions on relevant benchmark datasets\nincluding Scannet++ and Deblur-NeRF, obtaining state-of-the-art results and\nthus consistent improvements over relevant baselines.",
        "publication_date": "2024-04-05T16:42:16Z",
        "upvotes": 3
    },
    "2404.04256": {
        "url": "https://arxiv.org/abs/2404.04256",
        "title": "Sigma: Siamese Mamba Network for Multi-Modal Semantic Segmentation",
        "authors": [
            "Zifu Wan",
            "Yuhao Wang",
            "Silong Yong",
            "Pingping Zhang",
            "Simon Stepputtis",
            "Katia Sycara",
            "Yaqi Xie"
        ],
        "abstract": "Multi-modal semantic segmentation significantly enhances AI agents'\nperception and scene understanding, especially under adverse conditions like\nlow-light or overexposed environments. Leveraging additional modalities\n(X-modality) like thermal and depth alongside traditional RGB provides\ncomplementary information, enabling more robust and reliable segmentation. In\nthis work, we introduce Sigma, a Siamese Mamba network for multi-modal semantic\nsegmentation, utilizing the Selective Structured State Space Model, Mamba.\nUnlike conventional methods that rely on CNNs, with their limited local\nreceptive fields, or Vision Transformers (ViTs), which offer global receptive\nfields at the cost of quadratic complexity, our model achieves global receptive\nfields coverage with linear complexity. By employing a Siamese encoder and\ninnovating a Mamba fusion mechanism, we effectively select essential\ninformation from different modalities. A decoder is then developed to enhance\nthe channel-wise modeling ability of the model. Our method, Sigma, is\nrigorously evaluated on both RGB-Thermal and RGB-Depth segmentation tasks,\ndemonstrating its superiority and marking the first successful application of\nState Space Models (SSMs) in multi-modal perception tasks. Code is available at\nhttps://github.com/zifuwan/Sigma.",
        "publication_date": "2024-04-05T17:59:44Z",
        "upvotes": 2
    }
}